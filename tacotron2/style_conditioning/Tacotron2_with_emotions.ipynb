{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZEOSm9D5YHR8",
        "outputId": "f7b46618-0d25-4908-8314-45a27282d67c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate librosa soundfile\n"
      ],
      "metadata": {
        "id": "xviH_JJdVBM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base = \"/content/drive/MyDrive/266 Group Project\"\n",
        "required = [\"dataset.py\", \"model.py\", \"train_taco.py\"]\n",
        "\n",
        "print(\"Checking required files:\\n\")\n",
        "for f in required:\n",
        "    path = os.path.join(base, f)\n",
        "    print(f, \"✓ FOUND\" if os.path.exists(path) else \"✗ MISSING\")\n"
      ],
      "metadata": {
        "id": "yUkhCnlrVBtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ljs_base = \"/content/drive/MyDrive/266 Group Project/LJSpeech-1.1/wavs\"\n",
        "print(\"LJSpeech wavs folder:\", \"✓ FOUND\" if os.path.exists(ljs_base) else \"✗ MISSING\")\n"
      ],
      "metadata": {
        "id": "334O2lg8VEoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch \"/content/drive/MyDrive/266 Group Project/train_taco.py\" \\\n",
        "  --experiment_name tacotron2_emotion \\\n",
        "  --run_name tacotron2_emotion_run \\\n",
        "  --working_directory \"/content/drive/MyDrive/266 Group Project/tacotron2_emotion\" \\\n",
        "  --save_audio_gen \"/content/drive/MyDrive/266 Group Project/inference_outputs\" \\\n",
        "  --path_to_train_manifest \"/content/drive/MyDrive/266 Group Project/datasplit/train_metadata_emotion.csv\" \\\n",
        "  --path_to_val_manifest \"/content/drive/MyDrive/266 Group Project/datasplit/test_metadata_emotion.csv\" \\\n",
        "  --training_epochs 40 \\\n",
        "  --batch_size 32 \\\n",
        "  --learning_rate 0.001 \\\n",
        "  --console_out_iters 50 \\\n",
        "  --checkpoint_epochs 1 \\\n",
        "  --num_workers 0 \\\n",
        "  --no-log_wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBSOHTQfaWb5",
        "outputId": "a9802b79-c253-49aa-8c71-87648191cb38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Trainable Params: 29227777\n",
            "\n",
            "Epoch 0\n",
            "[Epoch 0] Batch 0/405 Loss=4.3865 Mel=1.3483 Rmel=2.6287 Stop=0.4096\n",
            "[Epoch 0] Batch 10/405 Loss=1.9110 Mel=0.7837 Rmel=1.1237 Stop=0.0036\n",
            "[Epoch 0] Batch 20/405 Loss=1.2440 Mel=0.4447 Rmel=0.7982 Stop=0.0012\n",
            "[Epoch 0] Batch 30/405 Loss=1.0364 Mel=0.4091 Rmel=0.6265 Stop=0.0008\n",
            "[Epoch 0] Batch 40/405 Loss=1.1499 Mel=0.4892 Rmel=0.6591 Stop=0.0015\n",
            "[Epoch 0] Batch 50/405 Loss=1.0273 Mel=0.4304 Rmel=0.5955 Stop=0.0014\n",
            "[Epoch 0] Batch 60/405 Loss=0.8488 Mel=0.4185 Rmel=0.4290 Stop=0.0013\n",
            "[Epoch 0] Batch 70/405 Loss=0.7609 Mel=0.3749 Rmel=0.3849 Stop=0.0011\n",
            "[Epoch 0] Batch 80/405 Loss=0.7997 Mel=0.3917 Rmel=0.4070 Stop=0.0010\n",
            "[Epoch 0] Batch 90/405 Loss=0.7600 Mel=0.3692 Rmel=0.3900 Stop=0.0007\n",
            "[Epoch 0] Batch 100/405 Loss=0.7268 Mel=0.3600 Rmel=0.3661 Stop=0.0006\n",
            "[Epoch 0] Batch 110/405 Loss=0.7342 Mel=0.3571 Rmel=0.3765 Stop=0.0006\n",
            "[Epoch 0] Batch 120/405 Loss=0.6457 Mel=0.3141 Rmel=0.3310 Stop=0.0005\n",
            "[Epoch 0] Batch 130/405 Loss=0.6206 Mel=0.3097 Rmel=0.3103 Stop=0.0005\n",
            "[Epoch 0] Batch 140/405 Loss=0.6911 Mel=0.3430 Rmel=0.3476 Stop=0.0005\n",
            "[Epoch 0] Batch 150/405 Loss=0.6130 Mel=0.2937 Rmel=0.3189 Stop=0.0004\n",
            "[Epoch 0] Batch 160/405 Loss=0.6774 Mel=0.3372 Rmel=0.3398 Stop=0.0004\n",
            "[Epoch 0] Batch 170/405 Loss=0.5572 Mel=0.2782 Rmel=0.2787 Stop=0.0004\n",
            "[Epoch 0] Batch 180/405 Loss=0.5999 Mel=0.2991 Rmel=0.3005 Stop=0.0004\n",
            "[Epoch 0] Batch 190/405 Loss=0.5265 Mel=0.2630 Rmel=0.2632 Stop=0.0003\n",
            "[Epoch 0] Batch 200/405 Loss=0.5874 Mel=0.2903 Rmel=0.2968 Stop=0.0003\n",
            "[Epoch 0] Batch 210/405 Loss=0.5810 Mel=0.2920 Rmel=0.2887 Stop=0.0003\n",
            "[Epoch 0] Batch 220/405 Loss=0.6061 Mel=0.2969 Rmel=0.3090 Stop=0.0003\n",
            "[Epoch 0] Batch 230/405 Loss=0.5596 Mel=0.2805 Rmel=0.2788 Stop=0.0003\n",
            "[Epoch 0] Batch 240/405 Loss=0.6215 Mel=0.3029 Rmel=0.3183 Stop=0.0003\n",
            "[Epoch 0] Batch 250/405 Loss=0.5288 Mel=0.2656 Rmel=0.2630 Stop=0.0003\n",
            "[Epoch 0] Batch 260/405 Loss=0.5625 Mel=0.2803 Rmel=0.2819 Stop=0.0003\n",
            "[Epoch 0] Batch 270/405 Loss=0.5224 Mel=0.2625 Rmel=0.2596 Stop=0.0003\n",
            "[Epoch 0] Batch 280/405 Loss=0.5338 Mel=0.2684 Rmel=0.2651 Stop=0.0003\n",
            "[Epoch 0] Batch 290/405 Loss=0.5067 Mel=0.2532 Rmel=0.2533 Stop=0.0003\n",
            "[Epoch 0] Batch 300/405 Loss=0.5070 Mel=0.2554 Rmel=0.2513 Stop=0.0003\n",
            "[Epoch 0] Batch 310/405 Loss=0.5072 Mel=0.2527 Rmel=0.2543 Stop=0.0003\n",
            "[Epoch 0] Batch 320/405 Loss=0.4639 Mel=0.2325 Rmel=0.2312 Stop=0.0002\n",
            "[Epoch 0] Batch 330/405 Loss=0.5412 Mel=0.2676 Rmel=0.2733 Stop=0.0003\n",
            "[Epoch 0] Batch 340/405 Loss=0.4943 Mel=0.2477 Rmel=0.2465 Stop=0.0002\n",
            "[Epoch 0] Batch 350/405 Loss=0.4928 Mel=0.2375 Rmel=0.2551 Stop=0.0002\n",
            "[Epoch 0] Batch 360/405 Loss=0.5368 Mel=0.2673 Rmel=0.2693 Stop=0.0002\n",
            "[Epoch 0] Batch 370/405 Loss=0.4815 Mel=0.2411 Rmel=0.2402 Stop=0.0003\n",
            "[Epoch 0] Batch 380/405 Loss=0.4747 Mel=0.2396 Rmel=0.2349 Stop=0.0003\n",
            "[Epoch 0] Batch 390/405 Loss=0.4326 Mel=0.2149 Rmel=0.2175 Stop=0.0002\n",
            "[Epoch 0] Batch 400/405 Loss=0.4548 Mel=0.2275 Rmel=0.2271 Stop=0.0003\n",
            "Validation Loss: 0.3995\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_0\n",
            "\n",
            "Epoch 1\n",
            "[Epoch 1] Batch 0/405 Loss=0.4531 Mel=0.2283 Rmel=0.2246 Stop=0.0002\n",
            "[Epoch 1] Batch 10/405 Loss=0.4817 Mel=0.2423 Rmel=0.2391 Stop=0.0002\n",
            "[Epoch 1] Batch 20/405 Loss=0.4198 Mel=0.1955 Rmel=0.2241 Stop=0.0002\n",
            "[Epoch 1] Batch 30/405 Loss=0.4556 Mel=0.2301 Rmel=0.2253 Stop=0.0002\n",
            "[Epoch 1] Batch 40/405 Loss=0.4412 Mel=0.2226 Rmel=0.2184 Stop=0.0002\n",
            "[Epoch 1] Batch 50/405 Loss=0.4394 Mel=0.2226 Rmel=0.2166 Stop=0.0002\n",
            "[Epoch 1] Batch 60/405 Loss=0.4688 Mel=0.2362 Rmel=0.2324 Stop=0.0002\n",
            "[Epoch 1] Batch 70/405 Loss=0.4134 Mel=0.2078 Rmel=0.2055 Stop=0.0002\n",
            "[Epoch 1] Batch 80/405 Loss=0.4405 Mel=0.2223 Rmel=0.2180 Stop=0.0002\n",
            "[Epoch 1] Batch 90/405 Loss=0.4057 Mel=0.2057 Rmel=0.1998 Stop=0.0002\n",
            "[Epoch 1] Batch 100/405 Loss=0.4192 Mel=0.2121 Rmel=0.2069 Stop=0.0002\n",
            "[Epoch 1] Batch 110/405 Loss=0.3945 Mel=0.1964 Rmel=0.1979 Stop=0.0001\n",
            "[Epoch 1] Batch 120/405 Loss=0.4335 Mel=0.2200 Rmel=0.2134 Stop=0.0002\n",
            "[Epoch 1] Batch 130/405 Loss=0.4187 Mel=0.2121 Rmel=0.2064 Stop=0.0002\n",
            "[Epoch 1] Batch 140/405 Loss=0.4162 Mel=0.2043 Rmel=0.2118 Stop=0.0001\n",
            "[Epoch 1] Batch 150/405 Loss=0.4376 Mel=0.2101 Rmel=0.2274 Stop=0.0002\n",
            "[Epoch 1] Batch 160/405 Loss=0.4251 Mel=0.2153 Rmel=0.2096 Stop=0.0002\n",
            "[Epoch 1] Batch 170/405 Loss=0.4127 Mel=0.2089 Rmel=0.2036 Stop=0.0002\n",
            "[Epoch 1] Batch 180/405 Loss=0.3725 Mel=0.1899 Rmel=0.1824 Stop=0.0002\n",
            "[Epoch 1] Batch 190/405 Loss=0.3950 Mel=0.2012 Rmel=0.1937 Stop=0.0001\n",
            "[Epoch 1] Batch 200/405 Loss=0.3386 Mel=0.1699 Rmel=0.1686 Stop=0.0001\n",
            "[Epoch 1] Batch 210/405 Loss=0.4328 Mel=0.2189 Rmel=0.2137 Stop=0.0002\n",
            "[Epoch 1] Batch 220/405 Loss=0.3858 Mel=0.1971 Rmel=0.1886 Stop=0.0001\n",
            "[Epoch 1] Batch 230/405 Loss=0.3975 Mel=0.2035 Rmel=0.1939 Stop=0.0001\n",
            "[Epoch 1] Batch 240/405 Loss=0.3843 Mel=0.1964 Rmel=0.1878 Stop=0.0001\n",
            "[Epoch 1] Batch 250/405 Loss=0.3688 Mel=0.1884 Rmel=0.1802 Stop=0.0001\n",
            "[Epoch 1] Batch 260/405 Loss=0.3818 Mel=0.1930 Rmel=0.1887 Stop=0.0001\n",
            "[Epoch 1] Batch 270/405 Loss=0.3833 Mel=0.1946 Rmel=0.1885 Stop=0.0001\n",
            "[Epoch 1] Batch 280/405 Loss=0.3926 Mel=0.1986 Rmel=0.1939 Stop=0.0001\n",
            "[Epoch 1] Batch 290/405 Loss=0.3904 Mel=0.1980 Rmel=0.1924 Stop=0.0001\n",
            "[Epoch 1] Batch 300/405 Loss=0.3666 Mel=0.1878 Rmel=0.1787 Stop=0.0001\n",
            "[Epoch 1] Batch 310/405 Loss=0.3974 Mel=0.2033 Rmel=0.1939 Stop=0.0001\n",
            "[Epoch 1] Batch 320/405 Loss=0.3810 Mel=0.1927 Rmel=0.1882 Stop=0.0001\n",
            "[Epoch 1] Batch 330/405 Loss=0.3674 Mel=0.1875 Rmel=0.1798 Stop=0.0001\n",
            "[Epoch 1] Batch 340/405 Loss=0.4067 Mel=0.2076 Rmel=0.1990 Stop=0.0001\n",
            "[Epoch 1] Batch 350/405 Loss=0.4040 Mel=0.2046 Rmel=0.1993 Stop=0.0001\n",
            "[Epoch 1] Batch 360/405 Loss=0.3587 Mel=0.1830 Rmel=0.1756 Stop=0.0001\n",
            "[Epoch 1] Batch 370/405 Loss=0.3776 Mel=0.1941 Rmel=0.1833 Stop=0.0001\n",
            "[Epoch 1] Batch 380/405 Loss=0.3392 Mel=0.1728 Rmel=0.1663 Stop=0.0001\n",
            "[Epoch 1] Batch 390/405 Loss=0.3818 Mel=0.1951 Rmel=0.1865 Stop=0.0001\n",
            "[Epoch 1] Batch 400/405 Loss=0.3633 Mel=0.1865 Rmel=0.1767 Stop=0.0001\n",
            "Validation Loss: 0.3133\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_1\n",
            "\n",
            "Epoch 2\n",
            "[Epoch 2] Batch 0/405 Loss=0.3907 Mel=0.1995 Rmel=0.1910 Stop=0.0001\n",
            "[Epoch 2] Batch 10/405 Loss=0.3595 Mel=0.1848 Rmel=0.1746 Stop=0.0001\n",
            "[Epoch 2] Batch 20/405 Loss=0.3558 Mel=0.1829 Rmel=0.1728 Stop=0.0001\n",
            "[Epoch 2] Batch 30/405 Loss=0.3819 Mel=0.1922 Rmel=0.1896 Stop=0.0001\n",
            "[Epoch 2] Batch 40/405 Loss=0.3311 Mel=0.1708 Rmel=0.1602 Stop=0.0001\n",
            "[Epoch 2] Batch 50/405 Loss=0.3842 Mel=0.1969 Rmel=0.1872 Stop=0.0001\n",
            "[Epoch 2] Batch 60/405 Loss=0.3125 Mel=0.1591 Rmel=0.1534 Stop=0.0001\n",
            "[Epoch 2] Batch 70/405 Loss=0.3514 Mel=0.1803 Rmel=0.1710 Stop=0.0001\n",
            "[Epoch 2] Batch 80/405 Loss=0.3562 Mel=0.1797 Rmel=0.1764 Stop=0.0001\n",
            "[Epoch 2] Batch 90/405 Loss=0.3319 Mel=0.1710 Rmel=0.1608 Stop=0.0001\n",
            "[Epoch 2] Batch 100/405 Loss=0.3791 Mel=0.1902 Rmel=0.1888 Stop=0.0001\n",
            "[Epoch 2] Batch 110/405 Loss=0.3659 Mel=0.1885 Rmel=0.1772 Stop=0.0001\n",
            "[Epoch 2] Batch 120/405 Loss=0.3285 Mel=0.1687 Rmel=0.1598 Stop=0.0001\n",
            "[Epoch 2] Batch 130/405 Loss=0.3290 Mel=0.1690 Rmel=0.1599 Stop=0.0001\n",
            "[Epoch 2] Batch 140/405 Loss=0.3379 Mel=0.1751 Rmel=0.1627 Stop=0.0001\n",
            "[Epoch 2] Batch 150/405 Loss=0.3389 Mel=0.1752 Rmel=0.1635 Stop=0.0001\n",
            "[Epoch 2] Batch 160/405 Loss=0.3310 Mel=0.1712 Rmel=0.1597 Stop=0.0001\n",
            "[Epoch 2] Batch 170/405 Loss=0.3481 Mel=0.1797 Rmel=0.1684 Stop=0.0001\n",
            "[Epoch 2] Batch 180/405 Loss=0.3456 Mel=0.1789 Rmel=0.1666 Stop=0.0001\n",
            "[Epoch 2] Batch 190/405 Loss=0.3169 Mel=0.1648 Rmel=0.1520 Stop=0.0001\n",
            "[Epoch 2] Batch 200/405 Loss=0.3596 Mel=0.1848 Rmel=0.1747 Stop=0.0001\n",
            "[Epoch 2] Batch 210/405 Loss=0.3244 Mel=0.1678 Rmel=0.1565 Stop=0.0001\n",
            "[Epoch 2] Batch 220/405 Loss=0.3400 Mel=0.1762 Rmel=0.1637 Stop=0.0001\n",
            "[Epoch 2] Batch 230/405 Loss=0.3337 Mel=0.1731 Rmel=0.1605 Stop=0.0001\n",
            "[Epoch 2] Batch 240/405 Loss=0.3318 Mel=0.1722 Rmel=0.1596 Stop=0.0001\n",
            "[Epoch 2] Batch 250/405 Loss=0.3202 Mel=0.1657 Rmel=0.1545 Stop=0.0001\n",
            "[Epoch 2] Batch 260/405 Loss=0.3079 Mel=0.1581 Rmel=0.1498 Stop=0.0001\n",
            "[Epoch 2] Batch 270/405 Loss=0.3176 Mel=0.1641 Rmel=0.1534 Stop=0.0001\n",
            "[Epoch 2] Batch 280/405 Loss=0.3373 Mel=0.1746 Rmel=0.1626 Stop=0.0001\n",
            "[Epoch 2] Batch 290/405 Loss=0.3456 Mel=0.1775 Rmel=0.1680 Stop=0.0001\n",
            "[Epoch 2] Batch 300/405 Loss=0.3525 Mel=0.1776 Rmel=0.1748 Stop=0.0001\n",
            "[Epoch 2] Batch 310/405 Loss=0.3442 Mel=0.1785 Rmel=0.1656 Stop=0.0001\n",
            "[Epoch 2] Batch 320/405 Loss=0.3315 Mel=0.1722 Rmel=0.1593 Stop=0.0001\n",
            "[Epoch 2] Batch 330/405 Loss=0.3370 Mel=0.1744 Rmel=0.1625 Stop=0.0001\n",
            "[Epoch 2] Batch 340/405 Loss=0.3264 Mel=0.1694 Rmel=0.1570 Stop=0.0001\n",
            "[Epoch 2] Batch 350/405 Loss=0.2790 Mel=0.1450 Rmel=0.1339 Stop=0.0001\n",
            "[Epoch 2] Batch 360/405 Loss=0.3173 Mel=0.1650 Rmel=0.1522 Stop=0.0001\n",
            "[Epoch 2] Batch 370/405 Loss=0.3442 Mel=0.1790 Rmel=0.1651 Stop=0.0001\n",
            "[Epoch 2] Batch 380/405 Loss=0.3112 Mel=0.1624 Rmel=0.1487 Stop=0.0001\n",
            "[Epoch 2] Batch 390/405 Loss=0.3177 Mel=0.1645 Rmel=0.1531 Stop=0.0001\n",
            "[Epoch 2] Batch 400/405 Loss=0.3248 Mel=0.1684 Rmel=0.1563 Stop=0.0001\n",
            "Validation Loss: 0.2792\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_2\n",
            "\n",
            "Epoch 3\n",
            "[Epoch 3] Batch 0/405 Loss=0.3400 Mel=0.1761 Rmel=0.1638 Stop=0.0001\n",
            "[Epoch 3] Batch 10/405 Loss=0.3163 Mel=0.1642 Rmel=0.1521 Stop=0.0001\n",
            "[Epoch 3] Batch 20/405 Loss=0.3390 Mel=0.1752 Rmel=0.1638 Stop=0.0001\n",
            "[Epoch 3] Batch 30/405 Loss=0.3042 Mel=0.1584 Rmel=0.1458 Stop=0.0001\n",
            "[Epoch 3] Batch 40/405 Loss=0.3177 Mel=0.1639 Rmel=0.1538 Stop=0.0001\n",
            "[Epoch 3] Batch 50/405 Loss=0.2887 Mel=0.1490 Rmel=0.1396 Stop=0.0001\n",
            "[Epoch 3] Batch 60/405 Loss=0.3049 Mel=0.1592 Rmel=0.1456 Stop=0.0001\n",
            "[Epoch 3] Batch 70/405 Loss=0.3259 Mel=0.1696 Rmel=0.1562 Stop=0.0001\n",
            "[Epoch 3] Batch 80/405 Loss=0.3070 Mel=0.1596 Rmel=0.1473 Stop=0.0001\n",
            "[Epoch 3] Batch 90/405 Loss=0.2923 Mel=0.1522 Rmel=0.1401 Stop=0.0001\n",
            "[Epoch 3] Batch 100/405 Loss=0.3153 Mel=0.1620 Rmel=0.1533 Stop=0.0001\n",
            "[Epoch 3] Batch 110/405 Loss=0.3278 Mel=0.1707 Rmel=0.1570 Stop=0.0001\n",
            "[Epoch 3] Batch 120/405 Loss=0.3197 Mel=0.1665 Rmel=0.1531 Stop=0.0001\n",
            "[Epoch 3] Batch 130/405 Loss=0.3193 Mel=0.1668 Rmel=0.1524 Stop=0.0001\n",
            "[Epoch 3] Batch 140/405 Loss=0.3105 Mel=0.1622 Rmel=0.1482 Stop=0.0001\n",
            "[Epoch 3] Batch 150/405 Loss=0.3536 Mel=0.1834 Rmel=0.1701 Stop=0.0001\n",
            "[Epoch 3] Batch 160/405 Loss=0.3013 Mel=0.1572 Rmel=0.1441 Stop=0.0001\n",
            "[Epoch 3] Batch 170/405 Loss=0.3135 Mel=0.1639 Rmel=0.1495 Stop=0.0001\n",
            "[Epoch 3] Batch 180/405 Loss=0.3007 Mel=0.1569 Rmel=0.1437 Stop=0.0001\n",
            "[Epoch 3] Batch 190/405 Loss=0.3161 Mel=0.1651 Rmel=0.1508 Stop=0.0001\n",
            "[Epoch 3] Batch 200/405 Loss=0.3004 Mel=0.1575 Rmel=0.1429 Stop=0.0001\n",
            "[Epoch 3] Batch 210/405 Loss=0.2869 Mel=0.1493 Rmel=0.1375 Stop=0.0001\n",
            "[Epoch 3] Batch 220/405 Loss=0.3260 Mel=0.1707 Rmel=0.1553 Stop=0.0001\n",
            "[Epoch 3] Batch 230/405 Loss=0.3368 Mel=0.1740 Rmel=0.1628 Stop=0.0001\n",
            "[Epoch 3] Batch 240/405 Loss=0.3251 Mel=0.1702 Rmel=0.1549 Stop=0.0001\n",
            "[Epoch 3] Batch 250/405 Loss=0.3167 Mel=0.1656 Rmel=0.1510 Stop=0.0001\n",
            "[Epoch 3] Batch 260/405 Loss=0.3257 Mel=0.1702 Rmel=0.1554 Stop=0.0001\n",
            "[Epoch 3] Batch 270/405 Loss=0.3080 Mel=0.1608 Rmel=0.1472 Stop=0.0001\n",
            "[Epoch 3] Batch 280/405 Loss=0.3159 Mel=0.1652 Rmel=0.1507 Stop=0.0001\n",
            "[Epoch 3] Batch 290/405 Loss=0.2980 Mel=0.1560 Rmel=0.1420 Stop=0.0001\n",
            "[Epoch 3] Batch 300/405 Loss=0.3411 Mel=0.1781 Rmel=0.1630 Stop=0.0001\n",
            "[Epoch 3] Batch 310/405 Loss=0.3310 Mel=0.1722 Rmel=0.1587 Stop=0.0001\n",
            "[Epoch 3] Batch 320/405 Loss=0.2931 Mel=0.1532 Rmel=0.1399 Stop=0.0001\n",
            "[Epoch 3] Batch 330/405 Loss=0.3105 Mel=0.1623 Rmel=0.1482 Stop=0.0001\n",
            "[Epoch 3] Batch 340/405 Loss=0.3437 Mel=0.1782 Rmel=0.1654 Stop=0.0001\n",
            "[Epoch 3] Batch 350/405 Loss=0.3013 Mel=0.1573 Rmel=0.1439 Stop=0.0001\n",
            "[Epoch 3] Batch 360/405 Loss=0.3114 Mel=0.1633 Rmel=0.1480 Stop=0.0001\n",
            "[Epoch 3] Batch 370/405 Loss=0.2977 Mel=0.1555 Rmel=0.1421 Stop=0.0001\n",
            "[Epoch 3] Batch 380/405 Loss=0.2819 Mel=0.1477 Rmel=0.1342 Stop=0.0001\n",
            "[Epoch 3] Batch 390/405 Loss=0.3120 Mel=0.1634 Rmel=0.1485 Stop=0.0001\n",
            "[Epoch 3] Batch 400/405 Loss=0.2857 Mel=0.1502 Rmel=0.1355 Stop=0.0001\n",
            "Validation Loss: 0.2585\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_3\n",
            "\n",
            "Epoch 4\n",
            "[Epoch 4] Batch 0/405 Loss=0.2951 Mel=0.1543 Rmel=0.1408 Stop=0.0001\n",
            "[Epoch 4] Batch 10/405 Loss=0.2804 Mel=0.1470 Rmel=0.1334 Stop=0.0001\n",
            "[Epoch 4] Batch 20/405 Loss=0.2852 Mel=0.1495 Rmel=0.1356 Stop=0.0001\n",
            "[Epoch 4] Batch 30/405 Loss=0.3117 Mel=0.1608 Rmel=0.1508 Stop=0.0001\n",
            "[Epoch 4] Batch 40/405 Loss=0.2763 Mel=0.1442 Rmel=0.1320 Stop=0.0001\n",
            "[Epoch 4] Batch 50/405 Loss=0.2599 Mel=0.1355 Rmel=0.1244 Stop=0.0000\n",
            "[Epoch 4] Batch 60/405 Loss=0.3072 Mel=0.1609 Rmel=0.1462 Stop=0.0001\n",
            "[Epoch 4] Batch 70/405 Loss=0.2636 Mel=0.1376 Rmel=0.1259 Stop=0.0001\n",
            "[Epoch 4] Batch 80/405 Loss=0.3211 Mel=0.1674 Rmel=0.1536 Stop=0.0001\n",
            "[Epoch 4] Batch 90/405 Loss=0.3226 Mel=0.1680 Rmel=0.1546 Stop=0.0001\n",
            "[Epoch 4] Batch 100/405 Loss=0.2959 Mel=0.1530 Rmel=0.1429 Stop=0.0001\n",
            "[Epoch 4] Batch 110/405 Loss=0.2871 Mel=0.1474 Rmel=0.1397 Stop=0.0001\n",
            "[Epoch 4] Batch 120/405 Loss=0.2627 Mel=0.1370 Rmel=0.1256 Stop=0.0001\n",
            "[Epoch 4] Batch 130/405 Loss=0.3024 Mel=0.1567 Rmel=0.1457 Stop=0.0001\n",
            "[Epoch 4] Batch 140/405 Loss=0.3163 Mel=0.1633 Rmel=0.1530 Stop=0.0001\n",
            "[Epoch 4] Batch 150/405 Loss=0.2794 Mel=0.1464 Rmel=0.1329 Stop=0.0001\n",
            "[Epoch 4] Batch 160/405 Loss=0.2791 Mel=0.1465 Rmel=0.1325 Stop=0.0001\n",
            "[Epoch 4] Batch 170/405 Loss=0.3106 Mel=0.1627 Rmel=0.1478 Stop=0.0001\n",
            "[Epoch 4] Batch 180/405 Loss=0.3104 Mel=0.1627 Rmel=0.1477 Stop=0.0001\n",
            "[Epoch 4] Batch 190/405 Loss=0.3244 Mel=0.1690 Rmel=0.1553 Stop=0.0001\n",
            "[Epoch 4] Batch 200/405 Loss=0.3201 Mel=0.1668 Rmel=0.1532 Stop=0.0001\n",
            "[Epoch 4] Batch 210/405 Loss=0.2964 Mel=0.1558 Rmel=0.1405 Stop=0.0001\n",
            "[Epoch 4] Batch 220/405 Loss=0.2995 Mel=0.1576 Rmel=0.1419 Stop=0.0001\n",
            "[Epoch 4] Batch 230/405 Loss=0.3150 Mel=0.1655 Rmel=0.1494 Stop=0.0001\n",
            "[Epoch 4] Batch 240/405 Loss=0.3210 Mel=0.1666 Rmel=0.1543 Stop=0.0001\n",
            "[Epoch 4] Batch 250/405 Loss=0.2862 Mel=0.1507 Rmel=0.1354 Stop=0.0001\n",
            "[Epoch 4] Batch 260/405 Loss=0.3111 Mel=0.1639 Rmel=0.1471 Stop=0.0001\n",
            "[Epoch 4] Batch 270/405 Loss=0.2606 Mel=0.1373 Rmel=0.1233 Stop=0.0000\n",
            "[Epoch 4] Batch 280/405 Loss=0.2854 Mel=0.1497 Rmel=0.1356 Stop=0.0001\n",
            "[Epoch 4] Batch 290/405 Loss=0.3133 Mel=0.1647 Rmel=0.1485 Stop=0.0001\n",
            "[Epoch 4] Batch 300/405 Loss=0.2637 Mel=0.1386 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 4] Batch 310/405 Loss=0.3261 Mel=0.1709 Rmel=0.1551 Stop=0.0001\n",
            "[Epoch 4] Batch 320/405 Loss=0.2874 Mel=0.1515 Rmel=0.1358 Stop=0.0001\n",
            "[Epoch 4] Batch 330/405 Loss=0.2933 Mel=0.1548 Rmel=0.1385 Stop=0.0001\n",
            "[Epoch 4] Batch 340/405 Loss=0.2714 Mel=0.1426 Rmel=0.1288 Stop=0.0000\n",
            "[Epoch 4] Batch 350/405 Loss=0.2776 Mel=0.1460 Rmel=0.1316 Stop=0.0001\n",
            "[Epoch 4] Batch 360/405 Loss=0.2722 Mel=0.1438 Rmel=0.1284 Stop=0.0001\n",
            "[Epoch 4] Batch 370/405 Loss=0.2613 Mel=0.1365 Rmel=0.1247 Stop=0.0000\n",
            "[Epoch 4] Batch 380/405 Loss=0.2656 Mel=0.1393 Rmel=0.1263 Stop=0.0000\n",
            "[Epoch 4] Batch 390/405 Loss=0.2955 Mel=0.1554 Rmel=0.1400 Stop=0.0001\n",
            "[Epoch 4] Batch 400/405 Loss=0.3077 Mel=0.1620 Rmel=0.1457 Stop=0.0001\n",
            "Validation Loss: 0.2479\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_4\n",
            "\n",
            "Epoch 5\n",
            "[Epoch 5] Batch 0/405 Loss=0.2919 Mel=0.1540 Rmel=0.1379 Stop=0.0000\n",
            "[Epoch 5] Batch 10/405 Loss=0.2855 Mel=0.1504 Rmel=0.1350 Stop=0.0001\n",
            "[Epoch 5] Batch 20/405 Loss=0.2815 Mel=0.1486 Rmel=0.1329 Stop=0.0001\n",
            "[Epoch 5] Batch 30/405 Loss=0.3045 Mel=0.1583 Rmel=0.1462 Stop=0.0001\n",
            "[Epoch 5] Batch 40/405 Loss=0.3003 Mel=0.1584 Rmel=0.1418 Stop=0.0001\n",
            "[Epoch 5] Batch 50/405 Loss=0.2748 Mel=0.1447 Rmel=0.1300 Stop=0.0001\n",
            "[Epoch 5] Batch 60/405 Loss=0.2764 Mel=0.1449 Rmel=0.1314 Stop=0.0001\n",
            "[Epoch 5] Batch 70/405 Loss=0.2682 Mel=0.1412 Rmel=0.1270 Stop=0.0001\n",
            "[Epoch 5] Batch 80/405 Loss=0.2811 Mel=0.1481 Rmel=0.1329 Stop=0.0001\n",
            "[Epoch 5] Batch 90/405 Loss=0.2701 Mel=0.1424 Rmel=0.1276 Stop=0.0000\n",
            "[Epoch 5] Batch 100/405 Loss=0.3102 Mel=0.1632 Rmel=0.1470 Stop=0.0001\n",
            "[Epoch 5] Batch 110/405 Loss=0.2726 Mel=0.1440 Rmel=0.1286 Stop=0.0001\n",
            "[Epoch 5] Batch 120/405 Loss=0.3076 Mel=0.1623 Rmel=0.1452 Stop=0.0001\n",
            "[Epoch 5] Batch 130/405 Loss=0.2724 Mel=0.1440 Rmel=0.1284 Stop=0.0000\n",
            "[Epoch 5] Batch 140/405 Loss=0.2863 Mel=0.1513 Rmel=0.1349 Stop=0.0001\n",
            "[Epoch 5] Batch 150/405 Loss=0.2832 Mel=0.1494 Rmel=0.1338 Stop=0.0000\n",
            "[Epoch 5] Batch 160/405 Loss=0.2780 Mel=0.1466 Rmel=0.1313 Stop=0.0001\n",
            "[Epoch 5] Batch 170/405 Loss=0.2646 Mel=0.1391 Rmel=0.1254 Stop=0.0000\n",
            "[Epoch 5] Batch 180/405 Loss=0.2653 Mel=0.1385 Rmel=0.1268 Stop=0.0000\n",
            "[Epoch 5] Batch 190/405 Loss=0.2722 Mel=0.1437 Rmel=0.1284 Stop=0.0000\n",
            "[Epoch 5] Batch 200/405 Loss=0.2814 Mel=0.1448 Rmel=0.1365 Stop=0.0000\n",
            "[Epoch 5] Batch 210/405 Loss=0.2552 Mel=0.1343 Rmel=0.1209 Stop=0.0000\n",
            "[Epoch 5] Batch 220/405 Loss=0.2836 Mel=0.1498 Rmel=0.1338 Stop=0.0001\n",
            "[Epoch 5] Batch 230/405 Loss=0.2800 Mel=0.1478 Rmel=0.1321 Stop=0.0000\n",
            "[Epoch 5] Batch 240/405 Loss=0.2801 Mel=0.1482 Rmel=0.1318 Stop=0.0000\n",
            "[Epoch 5] Batch 250/405 Loss=0.3084 Mel=0.1612 Rmel=0.1471 Stop=0.0001\n",
            "[Epoch 5] Batch 260/405 Loss=0.2813 Mel=0.1480 Rmel=0.1332 Stop=0.0001\n",
            "[Epoch 5] Batch 270/405 Loss=0.2642 Mel=0.1397 Rmel=0.1245 Stop=0.0000\n",
            "[Epoch 5] Batch 280/405 Loss=0.2820 Mel=0.1489 Rmel=0.1331 Stop=0.0000\n",
            "[Epoch 5] Batch 290/405 Loss=0.2845 Mel=0.1502 Rmel=0.1342 Stop=0.0000\n",
            "[Epoch 5] Batch 300/405 Loss=0.2717 Mel=0.1410 Rmel=0.1307 Stop=0.0000\n",
            "[Epoch 5] Batch 310/405 Loss=0.2682 Mel=0.1411 Rmel=0.1270 Stop=0.0000\n",
            "[Epoch 5] Batch 320/405 Loss=0.2956 Mel=0.1561 Rmel=0.1394 Stop=0.0000\n",
            "[Epoch 5] Batch 330/405 Loss=0.2922 Mel=0.1541 Rmel=0.1380 Stop=0.0000\n",
            "[Epoch 5] Batch 340/405 Loss=0.2587 Mel=0.1369 Rmel=0.1217 Stop=0.0000\n",
            "[Epoch 5] Batch 350/405 Loss=0.2533 Mel=0.1341 Rmel=0.1192 Stop=0.0000\n",
            "[Epoch 5] Batch 360/405 Loss=0.2725 Mel=0.1441 Rmel=0.1283 Stop=0.0000\n",
            "[Epoch 5] Batch 370/405 Loss=0.2886 Mel=0.1527 Rmel=0.1358 Stop=0.0000\n",
            "[Epoch 5] Batch 380/405 Loss=0.3179 Mel=0.1670 Rmel=0.1508 Stop=0.0001\n",
            "[Epoch 5] Batch 390/405 Loss=0.2933 Mel=0.1552 Rmel=0.1381 Stop=0.0000\n",
            "[Epoch 5] Batch 400/405 Loss=0.2553 Mel=0.1354 Rmel=0.1199 Stop=0.0000\n",
            "Validation Loss: 0.2336\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_5\n",
            "\n",
            "Epoch 6\n",
            "[Epoch 6] Batch 0/405 Loss=0.2793 Mel=0.1480 Rmel=0.1312 Stop=0.0000\n",
            "[Epoch 6] Batch 10/405 Loss=0.2743 Mel=0.1450 Rmel=0.1293 Stop=0.0000\n",
            "[Epoch 6] Batch 20/405 Loss=0.2805 Mel=0.1477 Rmel=0.1328 Stop=0.0000\n",
            "[Epoch 6] Batch 30/405 Loss=0.2458 Mel=0.1289 Rmel=0.1169 Stop=0.0000\n",
            "[Epoch 6] Batch 40/405 Loss=0.2515 Mel=0.1331 Rmel=0.1184 Stop=0.0000\n",
            "[Epoch 6] Batch 50/405 Loss=0.2943 Mel=0.1556 Rmel=0.1386 Stop=0.0001\n",
            "[Epoch 6] Batch 60/405 Loss=0.2679 Mel=0.1417 Rmel=0.1261 Stop=0.0000\n",
            "[Epoch 6] Batch 70/405 Loss=0.2519 Mel=0.1337 Rmel=0.1182 Stop=0.0000\n",
            "[Epoch 6] Batch 80/405 Loss=0.2620 Mel=0.1389 Rmel=0.1231 Stop=0.0000\n",
            "[Epoch 6] Batch 90/405 Loss=0.2851 Mel=0.1508 Rmel=0.1342 Stop=0.0000\n",
            "[Epoch 6] Batch 100/405 Loss=0.2808 Mel=0.1485 Rmel=0.1322 Stop=0.0000\n",
            "[Epoch 6] Batch 110/405 Loss=0.2894 Mel=0.1528 Rmel=0.1366 Stop=0.0000\n",
            "[Epoch 6] Batch 120/405 Loss=0.2661 Mel=0.1410 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 6] Batch 130/405 Loss=0.2876 Mel=0.1522 Rmel=0.1354 Stop=0.0000\n",
            "[Epoch 6] Batch 140/405 Loss=0.2581 Mel=0.1367 Rmel=0.1214 Stop=0.0000\n",
            "[Epoch 6] Batch 150/405 Loss=0.2547 Mel=0.1350 Rmel=0.1196 Stop=0.0000\n",
            "[Epoch 6] Batch 160/405 Loss=0.2751 Mel=0.1457 Rmel=0.1293 Stop=0.0000\n",
            "[Epoch 6] Batch 170/405 Loss=0.2626 Mel=0.1392 Rmel=0.1233 Stop=0.0000\n",
            "[Epoch 6] Batch 180/405 Loss=0.2348 Mel=0.1245 Rmel=0.1103 Stop=0.0000\n",
            "[Epoch 6] Batch 190/405 Loss=0.2552 Mel=0.1352 Rmel=0.1200 Stop=0.0000\n",
            "[Epoch 6] Batch 200/405 Loss=0.2758 Mel=0.1459 Rmel=0.1298 Stop=0.0000\n",
            "[Epoch 6] Batch 210/405 Loss=0.2710 Mel=0.1437 Rmel=0.1272 Stop=0.0000\n",
            "[Epoch 6] Batch 220/405 Loss=0.2686 Mel=0.1425 Rmel=0.1261 Stop=0.0000\n",
            "[Epoch 6] Batch 230/405 Loss=0.2368 Mel=0.1255 Rmel=0.1112 Stop=0.0000\n",
            "[Epoch 6] Batch 240/405 Loss=0.2675 Mel=0.1416 Rmel=0.1259 Stop=0.0000\n",
            "[Epoch 6] Batch 250/405 Loss=0.2432 Mel=0.1288 Rmel=0.1143 Stop=0.0000\n",
            "[Epoch 6] Batch 260/405 Loss=0.2468 Mel=0.1304 Rmel=0.1163 Stop=0.0000\n",
            "[Epoch 6] Batch 270/405 Loss=0.2679 Mel=0.1398 Rmel=0.1281 Stop=0.0000\n",
            "[Epoch 6] Batch 280/405 Loss=0.2654 Mel=0.1393 Rmel=0.1261 Stop=0.0000\n",
            "[Epoch 6] Batch 290/405 Loss=0.2646 Mel=0.1404 Rmel=0.1242 Stop=0.0000\n",
            "[Epoch 6] Batch 300/405 Loss=0.2665 Mel=0.1412 Rmel=0.1252 Stop=0.0000\n",
            "[Epoch 6] Batch 310/405 Loss=0.2664 Mel=0.1412 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 6] Batch 320/405 Loss=0.2731 Mel=0.1446 Rmel=0.1285 Stop=0.0000\n",
            "[Epoch 6] Batch 330/405 Loss=0.2931 Mel=0.1551 Rmel=0.1379 Stop=0.0000\n",
            "[Epoch 6] Batch 340/405 Loss=0.2570 Mel=0.1365 Rmel=0.1205 Stop=0.0000\n",
            "[Epoch 6] Batch 350/405 Loss=0.2589 Mel=0.1373 Rmel=0.1216 Stop=0.0000\n",
            "[Epoch 6] Batch 360/405 Loss=0.2543 Mel=0.1348 Rmel=0.1194 Stop=0.0000\n",
            "[Epoch 6] Batch 370/405 Loss=0.2589 Mel=0.1373 Rmel=0.1216 Stop=0.0000\n",
            "[Epoch 6] Batch 380/405 Loss=0.2826 Mel=0.1495 Rmel=0.1331 Stop=0.0000\n",
            "[Epoch 6] Batch 390/405 Loss=0.2771 Mel=0.1469 Rmel=0.1301 Stop=0.0000\n",
            "[Epoch 6] Batch 400/405 Loss=0.2505 Mel=0.1331 Rmel=0.1174 Stop=0.0000\n",
            "Validation Loss: 0.2237\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_6\n",
            "\n",
            "Epoch 7\n",
            "[Epoch 7] Batch 0/405 Loss=0.2485 Mel=0.1321 Rmel=0.1164 Stop=0.0000\n",
            "[Epoch 7] Batch 10/405 Loss=0.2549 Mel=0.1350 Rmel=0.1199 Stop=0.0000\n",
            "[Epoch 7] Batch 20/405 Loss=0.2389 Mel=0.1254 Rmel=0.1135 Stop=0.0000\n",
            "[Epoch 7] Batch 30/405 Loss=0.2367 Mel=0.1257 Rmel=0.1110 Stop=0.0000\n",
            "[Epoch 7] Batch 40/405 Loss=0.2610 Mel=0.1387 Rmel=0.1222 Stop=0.0000\n",
            "[Epoch 7] Batch 50/405 Loss=0.2525 Mel=0.1341 Rmel=0.1183 Stop=0.0000\n",
            "[Epoch 7] Batch 60/405 Loss=0.2455 Mel=0.1303 Rmel=0.1151 Stop=0.0000\n",
            "[Epoch 7] Batch 70/405 Loss=0.2564 Mel=0.1364 Rmel=0.1200 Stop=0.0000\n",
            "[Epoch 7] Batch 80/405 Loss=0.2643 Mel=0.1402 Rmel=0.1241 Stop=0.0000\n",
            "[Epoch 7] Batch 90/405 Loss=0.2618 Mel=0.1390 Rmel=0.1227 Stop=0.0000\n",
            "[Epoch 7] Batch 100/405 Loss=0.2576 Mel=0.1367 Rmel=0.1209 Stop=0.0000\n",
            "[Epoch 7] Batch 110/405 Loss=0.2806 Mel=0.1484 Rmel=0.1322 Stop=0.0000\n",
            "[Epoch 7] Batch 120/405 Loss=0.2559 Mel=0.1364 Rmel=0.1195 Stop=0.0000\n",
            "[Epoch 7] Batch 130/405 Loss=0.2686 Mel=0.1428 Rmel=0.1258 Stop=0.0000\n",
            "[Epoch 7] Batch 140/405 Loss=0.2713 Mel=0.1437 Rmel=0.1276 Stop=0.0000\n",
            "[Epoch 7] Batch 150/405 Loss=0.2624 Mel=0.1394 Rmel=0.1230 Stop=0.0000\n",
            "[Epoch 7] Batch 160/405 Loss=0.2570 Mel=0.1364 Rmel=0.1206 Stop=0.0000\n",
            "[Epoch 7] Batch 170/405 Loss=0.2651 Mel=0.1410 Rmel=0.1240 Stop=0.0000\n",
            "[Epoch 7] Batch 180/405 Loss=0.2696 Mel=0.1433 Rmel=0.1262 Stop=0.0000\n",
            "[Epoch 7] Batch 190/405 Loss=0.2825 Mel=0.1499 Rmel=0.1326 Stop=0.0000\n",
            "[Epoch 7] Batch 200/405 Loss=0.2546 Mel=0.1355 Rmel=0.1190 Stop=0.0000\n",
            "[Epoch 7] Batch 210/405 Loss=0.2643 Mel=0.1409 Rmel=0.1234 Stop=0.0000\n",
            "[Epoch 7] Batch 220/405 Loss=0.2424 Mel=0.1278 Rmel=0.1146 Stop=0.0000\n",
            "[Epoch 7] Batch 230/405 Loss=0.2472 Mel=0.1315 Rmel=0.1157 Stop=0.0000\n",
            "[Epoch 7] Batch 240/405 Loss=0.2738 Mel=0.1460 Rmel=0.1278 Stop=0.0000\n",
            "[Epoch 7] Batch 250/405 Loss=0.2506 Mel=0.1334 Rmel=0.1171 Stop=0.0000\n",
            "[Epoch 7] Batch 260/405 Loss=0.2388 Mel=0.1270 Rmel=0.1117 Stop=0.0000\n",
            "[Epoch 7] Batch 270/405 Loss=0.2642 Mel=0.1408 Rmel=0.1234 Stop=0.0000\n",
            "[Epoch 7] Batch 280/405 Loss=0.2592 Mel=0.1380 Rmel=0.1212 Stop=0.0000\n",
            "[Epoch 7] Batch 290/405 Loss=0.2893 Mel=0.1540 Rmel=0.1353 Stop=0.0000\n",
            "[Epoch 7] Batch 300/405 Loss=0.2532 Mel=0.1353 Rmel=0.1178 Stop=0.0000\n",
            "[Epoch 7] Batch 310/405 Loss=0.2590 Mel=0.1380 Rmel=0.1209 Stop=0.0000\n",
            "[Epoch 7] Batch 320/405 Loss=0.2510 Mel=0.1334 Rmel=0.1176 Stop=0.0000\n",
            "[Epoch 7] Batch 330/405 Loss=0.2696 Mel=0.1438 Rmel=0.1258 Stop=0.0000\n",
            "[Epoch 7] Batch 340/405 Loss=0.2806 Mel=0.1493 Rmel=0.1313 Stop=0.0000\n",
            "[Epoch 7] Batch 350/405 Loss=0.2660 Mel=0.1409 Rmel=0.1250 Stop=0.0000\n",
            "[Epoch 7] Batch 360/405 Loss=0.2632 Mel=0.1396 Rmel=0.1236 Stop=0.0000\n",
            "[Epoch 7] Batch 370/405 Loss=0.2639 Mel=0.1405 Rmel=0.1234 Stop=0.0000\n",
            "[Epoch 7] Batch 380/405 Loss=0.2422 Mel=0.1289 Rmel=0.1133 Stop=0.0000\n",
            "[Epoch 7] Batch 390/405 Loss=0.2348 Mel=0.1252 Rmel=0.1095 Stop=0.0000\n",
            "[Epoch 7] Batch 400/405 Loss=0.2576 Mel=0.1370 Rmel=0.1205 Stop=0.0000\n",
            "Validation Loss: 0.2173\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_7\n",
            "\n",
            "Epoch 8\n",
            "[Epoch 8] Batch 0/405 Loss=0.2932 Mel=0.1545 Rmel=0.1386 Stop=0.0000\n",
            "[Epoch 8] Batch 10/405 Loss=0.2230 Mel=0.1185 Rmel=0.1045 Stop=0.0000\n",
            "[Epoch 8] Batch 20/405 Loss=0.2674 Mel=0.1419 Rmel=0.1254 Stop=0.0000\n",
            "[Epoch 8] Batch 30/405 Loss=0.2780 Mel=0.1480 Rmel=0.1300 Stop=0.0000\n",
            "[Epoch 8] Batch 40/405 Loss=0.2518 Mel=0.1343 Rmel=0.1175 Stop=0.0000\n",
            "[Epoch 8] Batch 50/405 Loss=0.2178 Mel=0.1158 Rmel=0.1020 Stop=0.0000\n",
            "[Epoch 8] Batch 60/405 Loss=0.2444 Mel=0.1304 Rmel=0.1140 Stop=0.0000\n",
            "[Epoch 8] Batch 70/405 Loss=0.2672 Mel=0.1421 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 8] Batch 80/405 Loss=0.2562 Mel=0.1366 Rmel=0.1196 Stop=0.0000\n",
            "[Epoch 8] Batch 90/405 Loss=0.2261 Mel=0.1202 Rmel=0.1059 Stop=0.0000\n",
            "[Epoch 8] Batch 100/405 Loss=0.2722 Mel=0.1448 Rmel=0.1273 Stop=0.0000\n",
            "[Epoch 8] Batch 110/405 Loss=0.2579 Mel=0.1375 Rmel=0.1204 Stop=0.0000\n",
            "[Epoch 8] Batch 120/405 Loss=0.2718 Mel=0.1441 Rmel=0.1277 Stop=0.0000\n",
            "[Epoch 8] Batch 130/405 Loss=0.2584 Mel=0.1378 Rmel=0.1205 Stop=0.0000\n",
            "[Epoch 8] Batch 140/405 Loss=0.2409 Mel=0.1286 Rmel=0.1123 Stop=0.0000\n",
            "[Epoch 8] Batch 150/405 Loss=0.2349 Mel=0.1254 Rmel=0.1094 Stop=0.0000\n",
            "[Epoch 8] Batch 160/405 Loss=0.2452 Mel=0.1309 Rmel=0.1143 Stop=0.0000\n",
            "[Epoch 8] Batch 170/405 Loss=0.2730 Mel=0.1456 Rmel=0.1274 Stop=0.0000\n",
            "[Epoch 8] Batch 180/405 Loss=0.2410 Mel=0.1286 Rmel=0.1125 Stop=0.0000\n",
            "[Epoch 8] Batch 190/405 Loss=0.2445 Mel=0.1306 Rmel=0.1138 Stop=0.0000\n",
            "[Epoch 8] Batch 200/405 Loss=0.2463 Mel=0.1316 Rmel=0.1147 Stop=0.0000\n",
            "[Epoch 8] Batch 210/405 Loss=0.2680 Mel=0.1430 Rmel=0.1250 Stop=0.0000\n",
            "[Epoch 8] Batch 220/405 Loss=0.2596 Mel=0.1384 Rmel=0.1211 Stop=0.0000\n",
            "[Epoch 8] Batch 230/405 Loss=0.2672 Mel=0.1423 Rmel=0.1249 Stop=0.0000\n",
            "[Epoch 8] Batch 240/405 Loss=0.2414 Mel=0.1289 Rmel=0.1125 Stop=0.0000\n",
            "[Epoch 8] Batch 250/405 Loss=0.2480 Mel=0.1323 Rmel=0.1157 Stop=0.0000\n",
            "[Epoch 8] Batch 260/405 Loss=0.2425 Mel=0.1293 Rmel=0.1132 Stop=0.0000\n",
            "[Epoch 8] Batch 270/405 Loss=0.2570 Mel=0.1371 Rmel=0.1199 Stop=0.0000\n",
            "[Epoch 8] Batch 280/405 Loss=0.2483 Mel=0.1320 Rmel=0.1162 Stop=0.0000\n",
            "[Epoch 8] Batch 290/405 Loss=0.2410 Mel=0.1285 Rmel=0.1124 Stop=0.0000\n",
            "[Epoch 8] Batch 300/405 Loss=0.2544 Mel=0.1359 Rmel=0.1185 Stop=0.0000\n",
            "[Epoch 8] Batch 310/405 Loss=0.2358 Mel=0.1254 Rmel=0.1104 Stop=0.0000\n",
            "[Epoch 8] Batch 320/405 Loss=0.2385 Mel=0.1266 Rmel=0.1119 Stop=0.0000\n",
            "[Epoch 8] Batch 330/405 Loss=0.2488 Mel=0.1325 Rmel=0.1163 Stop=0.0000\n",
            "[Epoch 8] Batch 340/405 Loss=0.2496 Mel=0.1332 Rmel=0.1165 Stop=0.0000\n",
            "[Epoch 8] Batch 350/405 Loss=0.2414 Mel=0.1291 Rmel=0.1122 Stop=0.0000\n",
            "[Epoch 8] Batch 360/405 Loss=0.2388 Mel=0.1279 Rmel=0.1109 Stop=0.0000\n",
            "[Epoch 8] Batch 370/405 Loss=0.2495 Mel=0.1329 Rmel=0.1165 Stop=0.0000\n",
            "[Epoch 8] Batch 380/405 Loss=0.2531 Mel=0.1350 Rmel=0.1181 Stop=0.0000\n",
            "[Epoch 8] Batch 390/405 Loss=0.2672 Mel=0.1419 Rmel=0.1253 Stop=0.0000\n",
            "[Epoch 8] Batch 400/405 Loss=0.2481 Mel=0.1324 Rmel=0.1156 Stop=0.0000\n",
            "Validation Loss: 0.2070\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_8\n",
            "\n",
            "Epoch 9\n",
            "[Epoch 9] Batch 0/405 Loss=0.2330 Mel=0.1232 Rmel=0.1098 Stop=0.0000\n",
            "[Epoch 9] Batch 10/405 Loss=0.2636 Mel=0.1401 Rmel=0.1235 Stop=0.0000\n",
            "[Epoch 9] Batch 20/405 Loss=0.2548 Mel=0.1358 Rmel=0.1190 Stop=0.0000\n",
            "[Epoch 9] Batch 30/405 Loss=0.2451 Mel=0.1305 Rmel=0.1146 Stop=0.0000\n",
            "[Epoch 9] Batch 40/405 Loss=0.2724 Mel=0.1445 Rmel=0.1279 Stop=0.0000\n",
            "[Epoch 9] Batch 50/405 Loss=0.2688 Mel=0.1437 Rmel=0.1250 Stop=0.0000\n",
            "[Epoch 9] Batch 60/405 Loss=0.2620 Mel=0.1395 Rmel=0.1225 Stop=0.0000\n",
            "[Epoch 9] Batch 70/405 Loss=0.2703 Mel=0.1444 Rmel=0.1259 Stop=0.0000\n",
            "[Epoch 9] Batch 80/405 Loss=0.2473 Mel=0.1323 Rmel=0.1150 Stop=0.0000\n",
            "[Epoch 9] Batch 90/405 Loss=0.2459 Mel=0.1311 Rmel=0.1148 Stop=0.0000\n",
            "[Epoch 9] Batch 100/405 Loss=0.2638 Mel=0.1405 Rmel=0.1232 Stop=0.0000\n",
            "[Epoch 9] Batch 110/405 Loss=0.2494 Mel=0.1334 Rmel=0.1159 Stop=0.0000\n",
            "[Epoch 9] Batch 120/405 Loss=0.2730 Mel=0.1453 Rmel=0.1277 Stop=0.0000\n",
            "[Epoch 9] Batch 130/405 Loss=0.2429 Mel=0.1297 Rmel=0.1132 Stop=0.0000\n",
            "[Epoch 9] Batch 140/405 Loss=0.2290 Mel=0.1223 Rmel=0.1067 Stop=0.0000\n",
            "[Epoch 9] Batch 150/405 Loss=0.2574 Mel=0.1372 Rmel=0.1202 Stop=0.0000\n",
            "[Epoch 9] Batch 160/405 Loss=0.2370 Mel=0.1264 Rmel=0.1106 Stop=0.0000\n",
            "[Epoch 9] Batch 170/405 Loss=0.2400 Mel=0.1280 Rmel=0.1119 Stop=0.0000\n",
            "[Epoch 9] Batch 180/405 Loss=0.2387 Mel=0.1271 Rmel=0.1116 Stop=0.0000\n",
            "[Epoch 9] Batch 190/405 Loss=0.2271 Mel=0.1211 Rmel=0.1060 Stop=0.0000\n",
            "[Epoch 9] Batch 200/405 Loss=0.2384 Mel=0.1270 Rmel=0.1114 Stop=0.0000\n",
            "[Epoch 9] Batch 210/405 Loss=0.2260 Mel=0.1201 Rmel=0.1058 Stop=0.0000\n",
            "[Epoch 9] Batch 220/405 Loss=0.2312 Mel=0.1234 Rmel=0.1078 Stop=0.0000\n",
            "[Epoch 9] Batch 230/405 Loss=0.2433 Mel=0.1296 Rmel=0.1137 Stop=0.0000\n",
            "[Epoch 9] Batch 240/405 Loss=0.2391 Mel=0.1274 Rmel=0.1117 Stop=0.0000\n",
            "[Epoch 9] Batch 250/405 Loss=0.2547 Mel=0.1358 Rmel=0.1189 Stop=0.0000\n",
            "[Epoch 9] Batch 260/405 Loss=0.2420 Mel=0.1289 Rmel=0.1131 Stop=0.0000\n",
            "[Epoch 9] Batch 270/405 Loss=0.2285 Mel=0.1219 Rmel=0.1065 Stop=0.0000\n",
            "[Epoch 9] Batch 280/405 Loss=0.2237 Mel=0.1190 Rmel=0.1047 Stop=0.0000\n",
            "[Epoch 9] Batch 290/405 Loss=0.2101 Mel=0.1117 Rmel=0.0984 Stop=0.0000\n",
            "[Epoch 9] Batch 300/405 Loss=0.2226 Mel=0.1189 Rmel=0.1036 Stop=0.0000\n",
            "[Epoch 9] Batch 310/405 Loss=0.2459 Mel=0.1312 Rmel=0.1147 Stop=0.0000\n",
            "[Epoch 9] Batch 320/405 Loss=0.2345 Mel=0.1252 Rmel=0.1094 Stop=0.0000\n",
            "[Epoch 9] Batch 330/405 Loss=0.2403 Mel=0.1284 Rmel=0.1119 Stop=0.0000\n",
            "[Epoch 9] Batch 340/405 Loss=0.2334 Mel=0.1243 Rmel=0.1091 Stop=0.0000\n",
            "[Epoch 9] Batch 350/405 Loss=0.2459 Mel=0.1306 Rmel=0.1153 Stop=0.0000\n",
            "[Epoch 9] Batch 360/405 Loss=0.2466 Mel=0.1311 Rmel=0.1155 Stop=0.0000\n",
            "[Epoch 9] Batch 370/405 Loss=0.2269 Mel=0.1205 Rmel=0.1064 Stop=0.0000\n",
            "[Epoch 9] Batch 380/405 Loss=0.2494 Mel=0.1324 Rmel=0.1169 Stop=0.0000\n",
            "[Epoch 9] Batch 390/405 Loss=0.2396 Mel=0.1275 Rmel=0.1120 Stop=0.0000\n",
            "[Epoch 9] Batch 400/405 Loss=0.2357 Mel=0.1245 Rmel=0.1111 Stop=0.0000\n",
            "Validation Loss: 0.1964\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_9\n",
            "\n",
            "Epoch 10\n",
            "[Epoch 10] Batch 0/405 Loss=0.2141 Mel=0.1130 Rmel=0.1011 Stop=0.0000\n",
            "[Epoch 10] Batch 10/405 Loss=0.2255 Mel=0.1200 Rmel=0.1054 Stop=0.0000\n",
            "[Epoch 10] Batch 20/405 Loss=0.2125 Mel=0.1133 Rmel=0.0992 Stop=0.0000\n",
            "[Epoch 10] Batch 30/405 Loss=0.2215 Mel=0.1177 Rmel=0.1038 Stop=0.0000\n",
            "[Epoch 10] Batch 40/405 Loss=0.2257 Mel=0.1201 Rmel=0.1055 Stop=0.0000\n",
            "[Epoch 10] Batch 50/405 Loss=0.2266 Mel=0.1198 Rmel=0.1068 Stop=0.0000\n",
            "[Epoch 10] Batch 60/405 Loss=0.2388 Mel=0.1273 Rmel=0.1115 Stop=0.0000\n",
            "[Epoch 10] Batch 70/405 Loss=0.2306 Mel=0.1228 Rmel=0.1078 Stop=0.0000\n",
            "[Epoch 10] Batch 80/405 Loss=0.2352 Mel=0.1253 Rmel=0.1099 Stop=0.0000\n",
            "[Epoch 10] Batch 90/405 Loss=0.2211 Mel=0.1178 Rmel=0.1033 Stop=0.0000\n",
            "[Epoch 10] Batch 100/405 Loss=0.2330 Mel=0.1239 Rmel=0.1091 Stop=0.0000\n",
            "[Epoch 10] Batch 110/405 Loss=0.2398 Mel=0.1275 Rmel=0.1122 Stop=0.0000\n",
            "[Epoch 10] Batch 120/405 Loss=0.2543 Mel=0.1346 Rmel=0.1197 Stop=0.0000\n",
            "[Epoch 10] Batch 130/405 Loss=0.2303 Mel=0.1219 Rmel=0.1083 Stop=0.0000\n",
            "[Epoch 10] Batch 140/405 Loss=0.2305 Mel=0.1227 Rmel=0.1078 Stop=0.0000\n",
            "[Epoch 10] Batch 150/405 Loss=0.1977 Mel=0.1049 Rmel=0.0928 Stop=0.0000\n",
            "[Epoch 10] Batch 160/405 Loss=0.2342 Mel=0.1245 Rmel=0.1097 Stop=0.0000\n",
            "[Epoch 10] Batch 170/405 Loss=0.2157 Mel=0.1148 Rmel=0.1009 Stop=0.0000\n",
            "[Epoch 10] Batch 180/405 Loss=0.2394 Mel=0.1275 Rmel=0.1118 Stop=0.0000\n",
            "[Epoch 10] Batch 190/405 Loss=0.2454 Mel=0.1312 Rmel=0.1142 Stop=0.0000\n",
            "[Epoch 10] Batch 200/405 Loss=0.2431 Mel=0.1301 Rmel=0.1130 Stop=0.0000\n",
            "[Epoch 10] Batch 210/405 Loss=0.2139 Mel=0.1139 Rmel=0.0999 Stop=0.0000\n",
            "[Epoch 10] Batch 220/405 Loss=0.1992 Mel=0.1060 Rmel=0.0932 Stop=0.0000\n",
            "[Epoch 10] Batch 230/405 Loss=0.2264 Mel=0.1205 Rmel=0.1059 Stop=0.0000\n",
            "[Epoch 10] Batch 240/405 Loss=0.2241 Mel=0.1192 Rmel=0.1049 Stop=0.0000\n",
            "[Epoch 10] Batch 250/405 Loss=0.2250 Mel=0.1197 Rmel=0.1053 Stop=0.0000\n",
            "[Epoch 10] Batch 260/405 Loss=0.2042 Mel=0.1083 Rmel=0.0958 Stop=0.0000\n",
            "[Epoch 10] Batch 270/405 Loss=0.2321 Mel=0.1237 Rmel=0.1084 Stop=0.0000\n",
            "[Epoch 10] Batch 280/405 Loss=0.2407 Mel=0.1277 Rmel=0.1130 Stop=0.0000\n",
            "[Epoch 10] Batch 290/405 Loss=0.2160 Mel=0.1141 Rmel=0.1019 Stop=0.0000\n",
            "[Epoch 10] Batch 300/405 Loss=0.2148 Mel=0.1141 Rmel=0.1007 Stop=0.0000\n",
            "[Epoch 10] Batch 310/405 Loss=0.2511 Mel=0.1317 Rmel=0.1193 Stop=0.0000\n",
            "[Epoch 10] Batch 320/405 Loss=0.2311 Mel=0.1228 Rmel=0.1083 Stop=0.0000\n",
            "[Epoch 10] Batch 330/405 Loss=0.2286 Mel=0.1222 Rmel=0.1064 Stop=0.0000\n",
            "[Epoch 10] Batch 340/405 Loss=0.2195 Mel=0.1170 Rmel=0.1024 Stop=0.0000\n",
            "[Epoch 10] Batch 350/405 Loss=0.2293 Mel=0.1224 Rmel=0.1068 Stop=0.0000\n",
            "[Epoch 10] Batch 360/405 Loss=0.2167 Mel=0.1149 Rmel=0.1018 Stop=0.0000\n",
            "[Epoch 10] Batch 370/405 Loss=0.2092 Mel=0.1111 Rmel=0.0981 Stop=0.0000\n",
            "[Epoch 10] Batch 380/405 Loss=0.2110 Mel=0.1122 Rmel=0.0988 Stop=0.0000\n",
            "[Epoch 10] Batch 390/405 Loss=0.2298 Mel=0.1223 Rmel=0.1075 Stop=0.0000\n",
            "[Epoch 10] Batch 400/405 Loss=0.2252 Mel=0.1201 Rmel=0.1051 Stop=0.0000\n",
            "Validation Loss: 0.1874\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_10\n",
            "\n",
            "Epoch 11\n",
            "[Epoch 11] Batch 0/405 Loss=0.2339 Mel=0.1246 Rmel=0.1093 Stop=0.0000\n",
            "[Epoch 11] Batch 10/405 Loss=0.2108 Mel=0.1123 Rmel=0.0985 Stop=0.0000\n",
            "[Epoch 11] Batch 20/405 Loss=0.2359 Mel=0.1256 Rmel=0.1103 Stop=0.0000\n",
            "[Epoch 11] Batch 30/405 Loss=0.2120 Mel=0.1129 Rmel=0.0991 Stop=0.0000\n",
            "[Epoch 11] Batch 40/405 Loss=0.2286 Mel=0.1220 Rmel=0.1066 Stop=0.0000\n",
            "[Epoch 11] Batch 50/405 Loss=0.2169 Mel=0.1155 Rmel=0.1014 Stop=0.0000\n",
            "[Epoch 11] Batch 60/405 Loss=0.2007 Mel=0.1066 Rmel=0.0941 Stop=0.0000\n",
            "[Epoch 11] Batch 70/405 Loss=0.2107 Mel=0.1125 Rmel=0.0982 Stop=0.0000\n",
            "[Epoch 11] Batch 80/405 Loss=0.2361 Mel=0.1258 Rmel=0.1102 Stop=0.0000\n",
            "[Epoch 11] Batch 90/405 Loss=0.2130 Mel=0.1134 Rmel=0.0996 Stop=0.0000\n",
            "[Epoch 11] Batch 100/405 Loss=0.2015 Mel=0.1075 Rmel=0.0940 Stop=0.0000\n",
            "[Epoch 11] Batch 110/405 Loss=0.2216 Mel=0.1180 Rmel=0.1036 Stop=0.0000\n",
            "[Epoch 11] Batch 120/405 Loss=0.2261 Mel=0.1205 Rmel=0.1056 Stop=0.0000\n",
            "[Epoch 11] Batch 130/405 Loss=0.2128 Mel=0.1135 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 11] Batch 140/405 Loss=0.2173 Mel=0.1153 Rmel=0.1019 Stop=0.0000\n",
            "[Epoch 11] Batch 150/405 Loss=0.2120 Mel=0.1130 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 11] Batch 160/405 Loss=0.2176 Mel=0.1159 Rmel=0.1017 Stop=0.0000\n",
            "[Epoch 11] Batch 170/405 Loss=0.2298 Mel=0.1226 Rmel=0.1072 Stop=0.0000\n",
            "[Epoch 11] Batch 180/405 Loss=0.2221 Mel=0.1181 Rmel=0.1040 Stop=0.0000\n",
            "[Epoch 11] Batch 190/405 Loss=0.2271 Mel=0.1214 Rmel=0.1057 Stop=0.0000\n",
            "[Epoch 11] Batch 200/405 Loss=0.2164 Mel=0.1151 Rmel=0.1013 Stop=0.0000\n",
            "[Epoch 11] Batch 210/405 Loss=0.2235 Mel=0.1191 Rmel=0.1044 Stop=0.0000\n",
            "[Epoch 11] Batch 220/405 Loss=0.2365 Mel=0.1259 Rmel=0.1106 Stop=0.0000\n",
            "[Epoch 11] Batch 230/405 Loss=0.2268 Mel=0.1210 Rmel=0.1058 Stop=0.0000\n",
            "[Epoch 11] Batch 240/405 Loss=0.2159 Mel=0.1150 Rmel=0.1009 Stop=0.0000\n",
            "[Epoch 11] Batch 250/405 Loss=0.2367 Mel=0.1253 Rmel=0.1114 Stop=0.0000\n",
            "[Epoch 11] Batch 260/405 Loss=0.1950 Mel=0.1038 Rmel=0.0912 Stop=0.0000\n",
            "[Epoch 11] Batch 270/405 Loss=0.2171 Mel=0.1154 Rmel=0.1017 Stop=0.0000\n",
            "[Epoch 11] Batch 280/405 Loss=0.2336 Mel=0.1247 Rmel=0.1088 Stop=0.0000\n",
            "[Epoch 11] Batch 290/405 Loss=0.2194 Mel=0.1167 Rmel=0.1027 Stop=0.0000\n",
            "[Epoch 11] Batch 300/405 Loss=0.2233 Mel=0.1184 Rmel=0.1049 Stop=0.0000\n",
            "[Epoch 11] Batch 310/405 Loss=0.2127 Mel=0.1134 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 11] Batch 320/405 Loss=0.2117 Mel=0.1130 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 11] Batch 330/405 Loss=0.2248 Mel=0.1195 Rmel=0.1053 Stop=0.0000\n",
            "[Epoch 11] Batch 340/405 Loss=0.2160 Mel=0.1149 Rmel=0.1010 Stop=0.0000\n",
            "[Epoch 11] Batch 350/405 Loss=0.2131 Mel=0.1135 Rmel=0.0996 Stop=0.0000\n",
            "[Epoch 11] Batch 360/405 Loss=0.2113 Mel=0.1125 Rmel=0.0988 Stop=0.0000\n",
            "[Epoch 11] Batch 370/405 Loss=0.2199 Mel=0.1171 Rmel=0.1027 Stop=0.0000\n",
            "[Epoch 11] Batch 380/405 Loss=0.2237 Mel=0.1191 Rmel=0.1046 Stop=0.0000\n",
            "[Epoch 11] Batch 390/405 Loss=0.2301 Mel=0.1226 Rmel=0.1075 Stop=0.0000\n",
            "[Epoch 11] Batch 400/405 Loss=0.2005 Mel=0.1071 Rmel=0.0934 Stop=0.0000\n",
            "Validation Loss: 0.1793\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_11\n",
            "\n",
            "Epoch 12\n",
            "[Epoch 12] Batch 0/405 Loss=0.2401 Mel=0.1277 Rmel=0.1124 Stop=0.0000\n",
            "[Epoch 12] Batch 10/405 Loss=0.2021 Mel=0.1075 Rmel=0.0946 Stop=0.0000\n",
            "[Epoch 12] Batch 20/405 Loss=0.1961 Mel=0.1043 Rmel=0.0918 Stop=0.0000\n",
            "[Epoch 12] Batch 30/405 Loss=0.2153 Mel=0.1141 Rmel=0.1011 Stop=0.0000\n",
            "[Epoch 12] Batch 40/405 Loss=0.2172 Mel=0.1153 Rmel=0.1019 Stop=0.0000\n",
            "[Epoch 12] Batch 50/405 Loss=0.2007 Mel=0.1070 Rmel=0.0937 Stop=0.0000\n",
            "[Epoch 12] Batch 60/405 Loss=0.2063 Mel=0.1098 Rmel=0.0965 Stop=0.0000\n",
            "[Epoch 12] Batch 70/405 Loss=0.2014 Mel=0.1074 Rmel=0.0940 Stop=0.0000\n",
            "[Epoch 12] Batch 80/405 Loss=0.2226 Mel=0.1185 Rmel=0.1040 Stop=0.0000\n",
            "[Epoch 12] Batch 90/405 Loss=0.2275 Mel=0.1213 Rmel=0.1062 Stop=0.0000\n",
            "[Epoch 12] Batch 100/405 Loss=0.2465 Mel=0.1332 Rmel=0.1133 Stop=0.0000\n",
            "[Epoch 12] Batch 110/405 Loss=0.2463 Mel=0.1326 Rmel=0.1136 Stop=0.0000\n",
            "[Epoch 12] Batch 120/405 Loss=0.2495 Mel=0.1349 Rmel=0.1146 Stop=0.0000\n",
            "[Epoch 12] Batch 130/405 Loss=0.2367 Mel=0.1276 Rmel=0.1091 Stop=0.0000\n",
            "[Epoch 12] Batch 140/405 Loss=0.2626 Mel=0.1413 Rmel=0.1213 Stop=0.0000\n",
            "[Epoch 12] Batch 150/405 Loss=0.2251 Mel=0.1210 Rmel=0.1041 Stop=0.0000\n",
            "[Epoch 12] Batch 160/405 Loss=0.2195 Mel=0.1172 Rmel=0.1023 Stop=0.0000\n",
            "[Epoch 12] Batch 170/405 Loss=0.2313 Mel=0.1238 Rmel=0.1075 Stop=0.0000\n",
            "[Epoch 12] Batch 180/405 Loss=0.2301 Mel=0.1231 Rmel=0.1070 Stop=0.0000\n",
            "[Epoch 12] Batch 190/405 Loss=0.2109 Mel=0.1127 Rmel=0.0983 Stop=0.0000\n",
            "[Epoch 12] Batch 200/405 Loss=0.2121 Mel=0.1137 Rmel=0.0984 Stop=0.0000\n",
            "[Epoch 12] Batch 210/405 Loss=0.2362 Mel=0.1266 Rmel=0.1095 Stop=0.0000\n",
            "[Epoch 12] Batch 220/405 Loss=0.2317 Mel=0.1236 Rmel=0.1080 Stop=0.0000\n",
            "[Epoch 12] Batch 230/405 Loss=0.2083 Mel=0.1117 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 12] Batch 240/405 Loss=0.2214 Mel=0.1179 Rmel=0.1034 Stop=0.0000\n",
            "[Epoch 12] Batch 250/405 Loss=0.2130 Mel=0.1139 Rmel=0.0991 Stop=0.0000\n",
            "[Epoch 12] Batch 260/405 Loss=0.1801 Mel=0.0960 Rmel=0.0841 Stop=0.0000\n",
            "[Epoch 12] Batch 270/405 Loss=0.2070 Mel=0.1109 Rmel=0.0961 Stop=0.0000\n",
            "[Epoch 12] Batch 280/405 Loss=0.2219 Mel=0.1186 Rmel=0.1033 Stop=0.0000\n",
            "[Epoch 12] Batch 290/405 Loss=0.2217 Mel=0.1187 Rmel=0.1030 Stop=0.0000\n",
            "[Epoch 12] Batch 300/405 Loss=0.2229 Mel=0.1193 Rmel=0.1036 Stop=0.0000\n",
            "[Epoch 12] Batch 310/405 Loss=0.2130 Mel=0.1140 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 12] Batch 320/405 Loss=0.2242 Mel=0.1198 Rmel=0.1044 Stop=0.0000\n",
            "[Epoch 12] Batch 330/405 Loss=0.1969 Mel=0.1053 Rmel=0.0915 Stop=0.0000\n",
            "[Epoch 12] Batch 340/405 Loss=0.2397 Mel=0.1282 Rmel=0.1114 Stop=0.0000\n",
            "[Epoch 12] Batch 350/405 Loss=0.2101 Mel=0.1116 Rmel=0.0984 Stop=0.0000\n",
            "[Epoch 12] Batch 360/405 Loss=0.2126 Mel=0.1136 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 12] Batch 370/405 Loss=0.2157 Mel=0.1154 Rmel=0.1003 Stop=0.0000\n",
            "[Epoch 12] Batch 380/405 Loss=0.2244 Mel=0.1203 Rmel=0.1041 Stop=0.0000\n",
            "[Epoch 12] Batch 390/405 Loss=0.2159 Mel=0.1152 Rmel=0.1006 Stop=0.0000\n",
            "[Epoch 12] Batch 400/405 Loss=0.2156 Mel=0.1151 Rmel=0.1005 Stop=0.0000\n",
            "Validation Loss: 0.1762\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_12\n",
            "\n",
            "Epoch 13\n",
            "[Epoch 13] Batch 0/405 Loss=0.2175 Mel=0.1164 Rmel=0.1011 Stop=0.0000\n",
            "[Epoch 13] Batch 10/405 Loss=0.2204 Mel=0.1171 Rmel=0.1033 Stop=0.0000\n",
            "[Epoch 13] Batch 20/405 Loss=0.2309 Mel=0.1225 Rmel=0.1083 Stop=0.0000\n",
            "[Epoch 13] Batch 30/405 Loss=0.2145 Mel=0.1143 Rmel=0.1001 Stop=0.0000\n",
            "[Epoch 13] Batch 40/405 Loss=0.2034 Mel=0.1085 Rmel=0.0949 Stop=0.0000\n",
            "[Epoch 13] Batch 50/405 Loss=0.2321 Mel=0.1239 Rmel=0.1081 Stop=0.0000\n",
            "[Epoch 13] Batch 60/405 Loss=0.2175 Mel=0.1159 Rmel=0.1015 Stop=0.0000\n",
            "[Epoch 13] Batch 70/405 Loss=0.2196 Mel=0.1172 Rmel=0.1024 Stop=0.0000\n",
            "[Epoch 13] Batch 80/405 Loss=0.2100 Mel=0.1120 Rmel=0.0980 Stop=0.0000\n",
            "[Epoch 13] Batch 90/405 Loss=0.1992 Mel=0.1067 Rmel=0.0924 Stop=0.0000\n",
            "[Epoch 13] Batch 100/405 Loss=0.2186 Mel=0.1168 Rmel=0.1018 Stop=0.0000\n",
            "[Epoch 13] Batch 110/405 Loss=0.1942 Mel=0.1039 Rmel=0.0902 Stop=0.0000\n",
            "[Epoch 13] Batch 120/405 Loss=0.2068 Mel=0.1097 Rmel=0.0971 Stop=0.0000\n",
            "[Epoch 13] Batch 130/405 Loss=0.2351 Mel=0.1257 Rmel=0.1093 Stop=0.0000\n",
            "[Epoch 13] Batch 140/405 Loss=0.1957 Mel=0.1042 Rmel=0.0915 Stop=0.0000\n",
            "[Epoch 13] Batch 150/405 Loss=0.2233 Mel=0.1192 Rmel=0.1040 Stop=0.0000\n",
            "[Epoch 13] Batch 160/405 Loss=0.2165 Mel=0.1153 Rmel=0.1011 Stop=0.0000\n",
            "[Epoch 13] Batch 170/405 Loss=0.2278 Mel=0.1220 Rmel=0.1058 Stop=0.0000\n",
            "[Epoch 13] Batch 180/405 Loss=0.1998 Mel=0.1066 Rmel=0.0931 Stop=0.0000\n",
            "[Epoch 13] Batch 190/405 Loss=0.1993 Mel=0.1067 Rmel=0.0926 Stop=0.0000\n",
            "[Epoch 13] Batch 200/405 Loss=0.1977 Mel=0.1056 Rmel=0.0921 Stop=0.0000\n",
            "[Epoch 13] Batch 210/405 Loss=0.2222 Mel=0.1189 Rmel=0.1033 Stop=0.0000\n",
            "[Epoch 13] Batch 220/405 Loss=0.2242 Mel=0.1202 Rmel=0.1040 Stop=0.0000\n",
            "[Epoch 13] Batch 230/405 Loss=0.2076 Mel=0.1109 Rmel=0.0967 Stop=0.0000\n",
            "[Epoch 13] Batch 240/405 Loss=0.2094 Mel=0.1119 Rmel=0.0975 Stop=0.0000\n",
            "[Epoch 13] Batch 250/405 Loss=0.2175 Mel=0.1166 Rmel=0.1009 Stop=0.0000\n",
            "[Epoch 13] Batch 260/405 Loss=0.2087 Mel=0.1118 Rmel=0.0968 Stop=0.0000\n",
            "[Epoch 13] Batch 270/405 Loss=0.2144 Mel=0.1142 Rmel=0.1002 Stop=0.0000\n",
            "[Epoch 13] Batch 280/405 Loss=0.2134 Mel=0.1132 Rmel=0.1001 Stop=0.0000\n",
            "[Epoch 13] Batch 290/405 Loss=0.2112 Mel=0.1129 Rmel=0.0983 Stop=0.0000\n",
            "[Epoch 13] Batch 300/405 Loss=0.2174 Mel=0.1160 Rmel=0.1014 Stop=0.0000\n",
            "[Epoch 13] Batch 310/405 Loss=0.2057 Mel=0.1103 Rmel=0.0954 Stop=0.0000\n",
            "[Epoch 13] Batch 320/405 Loss=0.2246 Mel=0.1200 Rmel=0.1045 Stop=0.0000\n",
            "[Epoch 13] Batch 330/405 Loss=0.2132 Mel=0.1139 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 13] Batch 340/405 Loss=0.2248 Mel=0.1204 Rmel=0.1043 Stop=0.0000\n",
            "[Epoch 13] Batch 350/405 Loss=0.1976 Mel=0.1060 Rmel=0.0916 Stop=0.0000\n",
            "[Epoch 13] Batch 360/405 Loss=0.2224 Mel=0.1191 Rmel=0.1033 Stop=0.0000\n",
            "[Epoch 13] Batch 370/405 Loss=0.2090 Mel=0.1120 Rmel=0.0970 Stop=0.0000\n",
            "[Epoch 13] Batch 380/405 Loss=0.2178 Mel=0.1168 Rmel=0.1010 Stop=0.0000\n",
            "[Epoch 13] Batch 390/405 Loss=0.2182 Mel=0.1168 Rmel=0.1013 Stop=0.0000\n",
            "[Epoch 13] Batch 400/405 Loss=0.1932 Mel=0.1033 Rmel=0.0899 Stop=0.0000\n",
            "Validation Loss: 0.1719\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_13\n",
            "\n",
            "Epoch 14\n",
            "[Epoch 14] Batch 0/405 Loss=0.1918 Mel=0.1020 Rmel=0.0897 Stop=0.0000\n",
            "[Epoch 14] Batch 10/405 Loss=0.2332 Mel=0.1232 Rmel=0.1099 Stop=0.0000\n",
            "[Epoch 14] Batch 20/405 Loss=0.2126 Mel=0.1138 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 14] Batch 30/405 Loss=0.1597 Mel=0.0853 Rmel=0.0744 Stop=0.0000\n",
            "[Epoch 14] Batch 40/405 Loss=0.2088 Mel=0.1118 Rmel=0.0971 Stop=0.0000\n",
            "[Epoch 14] Batch 50/405 Loss=0.2224 Mel=0.1188 Rmel=0.1036 Stop=0.0000\n",
            "[Epoch 14] Batch 60/405 Loss=0.2035 Mel=0.1089 Rmel=0.0946 Stop=0.0000\n",
            "[Epoch 14] Batch 70/405 Loss=0.2044 Mel=0.1094 Rmel=0.0950 Stop=0.0000\n",
            "[Epoch 14] Batch 80/405 Loss=0.1843 Mel=0.0983 Rmel=0.0860 Stop=0.0000\n",
            "[Epoch 14] Batch 90/405 Loss=0.2015 Mel=0.1078 Rmel=0.0937 Stop=0.0000\n",
            "[Epoch 14] Batch 100/405 Loss=0.2005 Mel=0.1072 Rmel=0.0933 Stop=0.0000\n",
            "[Epoch 14] Batch 110/405 Loss=0.1893 Mel=0.1012 Rmel=0.0880 Stop=0.0000\n",
            "[Epoch 14] Batch 120/405 Loss=0.2163 Mel=0.1154 Rmel=0.1010 Stop=0.0000\n",
            "[Epoch 14] Batch 130/405 Loss=0.2129 Mel=0.1139 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 14] Batch 140/405 Loss=0.1972 Mel=0.1055 Rmel=0.0917 Stop=0.0000\n",
            "[Epoch 14] Batch 150/405 Loss=0.2038 Mel=0.1091 Rmel=0.0947 Stop=0.0000\n",
            "[Epoch 14] Batch 160/405 Loss=0.2069 Mel=0.1108 Rmel=0.0961 Stop=0.0000\n",
            "[Epoch 14] Batch 170/405 Loss=0.1895 Mel=0.1017 Rmel=0.0879 Stop=0.0000\n",
            "[Epoch 14] Batch 180/405 Loss=0.1786 Mel=0.0955 Rmel=0.0831 Stop=0.0000\n",
            "[Epoch 14] Batch 190/405 Loss=0.2061 Mel=0.1105 Rmel=0.0956 Stop=0.0000\n",
            "[Epoch 14] Batch 200/405 Loss=0.1856 Mel=0.0995 Rmel=0.0860 Stop=0.0000\n",
            "[Epoch 14] Batch 210/405 Loss=0.2033 Mel=0.1090 Rmel=0.0944 Stop=0.0000\n",
            "[Epoch 14] Batch 220/405 Loss=0.2061 Mel=0.1106 Rmel=0.0955 Stop=0.0000\n",
            "[Epoch 14] Batch 230/405 Loss=0.1922 Mel=0.1030 Rmel=0.0891 Stop=0.0000\n",
            "[Epoch 14] Batch 240/405 Loss=0.1938 Mel=0.1035 Rmel=0.0902 Stop=0.0000\n",
            "[Epoch 14] Batch 250/405 Loss=0.2054 Mel=0.1102 Rmel=0.0952 Stop=0.0000\n",
            "[Epoch 14] Batch 260/405 Loss=0.2006 Mel=0.1072 Rmel=0.0935 Stop=0.0000\n",
            "[Epoch 14] Batch 270/405 Loss=0.1857 Mel=0.0995 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 14] Batch 280/405 Loss=0.1916 Mel=0.1025 Rmel=0.0890 Stop=0.0000\n",
            "[Epoch 14] Batch 290/405 Loss=0.1948 Mel=0.1045 Rmel=0.0904 Stop=0.0000\n",
            "[Epoch 14] Batch 300/405 Loss=0.2090 Mel=0.1121 Rmel=0.0969 Stop=0.0000\n",
            "[Epoch 14] Batch 310/405 Loss=0.2162 Mel=0.1157 Rmel=0.1004 Stop=0.0000\n",
            "[Epoch 14] Batch 320/405 Loss=0.2118 Mel=0.1134 Rmel=0.0984 Stop=0.0000\n",
            "[Epoch 14] Batch 330/405 Loss=0.2038 Mel=0.1091 Rmel=0.0946 Stop=0.0000\n",
            "[Epoch 14] Batch 340/405 Loss=0.1942 Mel=0.1041 Rmel=0.0901 Stop=0.0000\n",
            "[Epoch 14] Batch 350/405 Loss=0.1901 Mel=0.1019 Rmel=0.0882 Stop=0.0000\n",
            "[Epoch 14] Batch 360/405 Loss=0.1987 Mel=0.1067 Rmel=0.0921 Stop=0.0000\n",
            "[Epoch 14] Batch 370/405 Loss=0.2135 Mel=0.1146 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 14] Batch 380/405 Loss=0.2020 Mel=0.1086 Rmel=0.0934 Stop=0.0000\n",
            "[Epoch 14] Batch 390/405 Loss=0.1905 Mel=0.1022 Rmel=0.0882 Stop=0.0000\n",
            "[Epoch 14] Batch 400/405 Loss=0.2182 Mel=0.1176 Rmel=0.1006 Stop=0.0000\n",
            "Validation Loss: 0.1745\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_14\n",
            "\n",
            "Epoch 15\n",
            "[Epoch 15] Batch 0/405 Loss=0.1902 Mel=0.1021 Rmel=0.0881 Stop=0.0000\n",
            "[Epoch 15] Batch 10/405 Loss=0.2273 Mel=0.1218 Rmel=0.1055 Stop=0.0000\n",
            "[Epoch 15] Batch 20/405 Loss=0.2063 Mel=0.1110 Rmel=0.0953 Stop=0.0000\n",
            "[Epoch 15] Batch 30/405 Loss=0.2339 Mel=0.1250 Rmel=0.1089 Stop=0.0000\n",
            "[Epoch 15] Batch 40/405 Loss=0.2028 Mel=0.1090 Rmel=0.0939 Stop=0.0000\n",
            "[Epoch 15] Batch 50/405 Loss=0.1969 Mel=0.1058 Rmel=0.0911 Stop=0.0000\n",
            "[Epoch 15] Batch 60/405 Loss=0.2080 Mel=0.1118 Rmel=0.0962 Stop=0.0000\n",
            "[Epoch 15] Batch 70/405 Loss=0.1883 Mel=0.1010 Rmel=0.0873 Stop=0.0000\n",
            "[Epoch 15] Batch 80/405 Loss=0.2157 Mel=0.1155 Rmel=0.1002 Stop=0.0000\n",
            "[Epoch 15] Batch 90/405 Loss=0.2091 Mel=0.1124 Rmel=0.0967 Stop=0.0000\n",
            "[Epoch 15] Batch 100/405 Loss=0.1990 Mel=0.1065 Rmel=0.0924 Stop=0.0000\n",
            "[Epoch 15] Batch 110/405 Loss=0.1988 Mel=0.1065 Rmel=0.0923 Stop=0.0000\n",
            "[Epoch 15] Batch 120/405 Loss=0.2142 Mel=0.1151 Rmel=0.0991 Stop=0.0000\n",
            "[Epoch 15] Batch 130/405 Loss=0.2051 Mel=0.1097 Rmel=0.0954 Stop=0.0000\n",
            "[Epoch 15] Batch 140/405 Loss=0.1886 Mel=0.1009 Rmel=0.0876 Stop=0.0000\n",
            "[Epoch 15] Batch 150/405 Loss=0.1999 Mel=0.1071 Rmel=0.0928 Stop=0.0000\n",
            "[Epoch 15] Batch 160/405 Loss=0.1991 Mel=0.1069 Rmel=0.0922 Stop=0.0000\n",
            "[Epoch 15] Batch 170/405 Loss=0.1786 Mel=0.0958 Rmel=0.0828 Stop=0.0000\n",
            "[Epoch 15] Batch 180/405 Loss=0.2050 Mel=0.1100 Rmel=0.0950 Stop=0.0000\n",
            "[Epoch 15] Batch 190/405 Loss=0.1822 Mel=0.0977 Rmel=0.0845 Stop=0.0000\n",
            "[Epoch 15] Batch 200/405 Loss=0.1866 Mel=0.0999 Rmel=0.0866 Stop=0.0000\n",
            "[Epoch 15] Batch 210/405 Loss=0.2233 Mel=0.1194 Rmel=0.1039 Stop=0.0000\n",
            "[Epoch 15] Batch 220/405 Loss=0.1803 Mel=0.0964 Rmel=0.0839 Stop=0.0000\n",
            "[Epoch 15] Batch 230/405 Loss=0.1968 Mel=0.1057 Rmel=0.0910 Stop=0.0000\n",
            "[Epoch 15] Batch 240/405 Loss=0.1986 Mel=0.1066 Rmel=0.0920 Stop=0.0000\n",
            "[Epoch 15] Batch 250/405 Loss=0.1976 Mel=0.1059 Rmel=0.0917 Stop=0.0000\n",
            "[Epoch 15] Batch 260/405 Loss=0.2300 Mel=0.1240 Rmel=0.1060 Stop=0.0000\n",
            "[Epoch 15] Batch 270/405 Loss=0.1961 Mel=0.1054 Rmel=0.0907 Stop=0.0000\n",
            "[Epoch 15] Batch 280/405 Loss=0.2099 Mel=0.1130 Rmel=0.0969 Stop=0.0000\n",
            "[Epoch 15] Batch 290/405 Loss=0.1825 Mel=0.0979 Rmel=0.0846 Stop=0.0000\n",
            "[Epoch 15] Batch 300/405 Loss=0.2078 Mel=0.1119 Rmel=0.0960 Stop=0.0000\n",
            "[Epoch 15] Batch 310/405 Loss=0.2042 Mel=0.1097 Rmel=0.0945 Stop=0.0000\n",
            "[Epoch 15] Batch 320/405 Loss=0.1867 Mel=0.1005 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 15] Batch 330/405 Loss=0.1886 Mel=0.1015 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 15] Batch 340/405 Loss=0.2093 Mel=0.1127 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 15] Batch 350/405 Loss=0.1923 Mel=0.1034 Rmel=0.0889 Stop=0.0000\n",
            "[Epoch 15] Batch 360/405 Loss=0.2061 Mel=0.1111 Rmel=0.0950 Stop=0.0000\n",
            "[Epoch 15] Batch 370/405 Loss=0.2283 Mel=0.1224 Rmel=0.1059 Stop=0.0000\n",
            "[Epoch 15] Batch 380/405 Loss=0.1884 Mel=0.1014 Rmel=0.0870 Stop=0.0000\n",
            "[Epoch 15] Batch 390/405 Loss=0.2133 Mel=0.1146 Rmel=0.0986 Stop=0.0000\n",
            "[Epoch 15] Batch 400/405 Loss=0.2107 Mel=0.1132 Rmel=0.0975 Stop=0.0000\n",
            "Validation Loss: 0.1680\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_15\n",
            "\n",
            "Epoch 16\n",
            "[Epoch 16] Batch 0/405 Loss=0.1936 Mel=0.1039 Rmel=0.0898 Stop=0.0000\n",
            "[Epoch 16] Batch 10/405 Loss=0.1904 Mel=0.1021 Rmel=0.0883 Stop=0.0000\n",
            "[Epoch 16] Batch 20/405 Loss=0.1937 Mel=0.1041 Rmel=0.0895 Stop=0.0000\n",
            "[Epoch 16] Batch 30/405 Loss=0.1913 Mel=0.1029 Rmel=0.0884 Stop=0.0000\n",
            "[Epoch 16] Batch 40/405 Loss=0.1920 Mel=0.1032 Rmel=0.0888 Stop=0.0000\n",
            "[Epoch 16] Batch 50/405 Loss=0.1962 Mel=0.1056 Rmel=0.0906 Stop=0.0000\n",
            "[Epoch 16] Batch 60/405 Loss=0.2072 Mel=0.1111 Rmel=0.0961 Stop=0.0000\n",
            "[Epoch 16] Batch 70/405 Loss=0.1886 Mel=0.1013 Rmel=0.0872 Stop=0.0000\n",
            "[Epoch 16] Batch 80/405 Loss=0.2056 Mel=0.1104 Rmel=0.0953 Stop=0.0000\n",
            "[Epoch 16] Batch 90/405 Loss=0.1783 Mel=0.0959 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 16] Batch 100/405 Loss=0.1994 Mel=0.1073 Rmel=0.0921 Stop=0.0000\n",
            "[Epoch 16] Batch 110/405 Loss=0.1972 Mel=0.1059 Rmel=0.0913 Stop=0.0000\n",
            "[Epoch 16] Batch 120/405 Loss=0.1857 Mel=0.0997 Rmel=0.0859 Stop=0.0000\n",
            "[Epoch 16] Batch 130/405 Loss=0.2004 Mel=0.1075 Rmel=0.0929 Stop=0.0000\n",
            "[Epoch 16] Batch 140/405 Loss=0.2077 Mel=0.1118 Rmel=0.0959 Stop=0.0000\n",
            "[Epoch 16] Batch 150/405 Loss=0.1928 Mel=0.1035 Rmel=0.0894 Stop=0.0000\n",
            "[Epoch 16] Batch 160/405 Loss=0.1700 Mel=0.0913 Rmel=0.0787 Stop=0.0000\n",
            "[Epoch 16] Batch 170/405 Loss=0.1833 Mel=0.0986 Rmel=0.0847 Stop=0.0000\n",
            "[Epoch 16] Batch 180/405 Loss=0.1977 Mel=0.1064 Rmel=0.0913 Stop=0.0000\n",
            "[Epoch 16] Batch 190/405 Loss=0.1986 Mel=0.1066 Rmel=0.0920 Stop=0.0000\n",
            "[Epoch 16] Batch 200/405 Loss=0.1861 Mel=0.0999 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 16] Batch 210/405 Loss=0.1777 Mel=0.0955 Rmel=0.0821 Stop=0.0000\n",
            "[Epoch 16] Batch 220/405 Loss=0.2062 Mel=0.1110 Rmel=0.0952 Stop=0.0000\n",
            "[Epoch 16] Batch 230/405 Loss=0.1727 Mel=0.0927 Rmel=0.0800 Stop=0.0000\n",
            "[Epoch 16] Batch 240/405 Loss=0.1794 Mel=0.0963 Rmel=0.0831 Stop=0.0000\n",
            "[Epoch 16] Batch 250/405 Loss=0.2075 Mel=0.1116 Rmel=0.0959 Stop=0.0000\n",
            "[Epoch 16] Batch 260/405 Loss=0.2176 Mel=0.1168 Rmel=0.1007 Stop=0.0000\n",
            "[Epoch 16] Batch 270/405 Loss=0.2090 Mel=0.1122 Rmel=0.0968 Stop=0.0000\n",
            "[Epoch 16] Batch 280/405 Loss=0.1912 Mel=0.1030 Rmel=0.0882 Stop=0.0000\n",
            "[Epoch 16] Batch 290/405 Loss=0.1889 Mel=0.1017 Rmel=0.0872 Stop=0.0000\n",
            "[Epoch 16] Batch 300/405 Loss=0.1987 Mel=0.1068 Rmel=0.0919 Stop=0.0000\n",
            "[Epoch 16] Batch 310/405 Loss=0.1980 Mel=0.1065 Rmel=0.0915 Stop=0.0000\n",
            "[Epoch 16] Batch 320/405 Loss=0.1931 Mel=0.1039 Rmel=0.0892 Stop=0.0000\n",
            "[Epoch 16] Batch 330/405 Loss=0.1877 Mel=0.1010 Rmel=0.0867 Stop=0.0000\n",
            "[Epoch 16] Batch 340/405 Loss=0.1996 Mel=0.1074 Rmel=0.0922 Stop=0.0000\n",
            "[Epoch 16] Batch 350/405 Loss=0.1718 Mel=0.0921 Rmel=0.0796 Stop=0.0000\n",
            "[Epoch 16] Batch 360/405 Loss=0.1758 Mel=0.0946 Rmel=0.0812 Stop=0.0000\n",
            "[Epoch 16] Batch 370/405 Loss=0.2068 Mel=0.1109 Rmel=0.0959 Stop=0.0000\n",
            "[Epoch 16] Batch 380/405 Loss=0.1814 Mel=0.0974 Rmel=0.0840 Stop=0.0000\n",
            "[Epoch 16] Batch 390/405 Loss=0.2146 Mel=0.1146 Rmel=0.0999 Stop=0.0000\n",
            "[Epoch 16] Batch 400/405 Loss=0.1998 Mel=0.1074 Rmel=0.0924 Stop=0.0000\n",
            "Validation Loss: 0.1618\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_16\n",
            "\n",
            "Epoch 17\n",
            "[Epoch 17] Batch 0/405 Loss=0.1800 Mel=0.0968 Rmel=0.0832 Stop=0.0000\n",
            "[Epoch 17] Batch 10/405 Loss=0.1967 Mel=0.1059 Rmel=0.0908 Stop=0.0000\n",
            "[Epoch 17] Batch 20/405 Loss=0.1929 Mel=0.1037 Rmel=0.0892 Stop=0.0000\n",
            "[Epoch 17] Batch 30/405 Loss=0.1962 Mel=0.1058 Rmel=0.0904 Stop=0.0000\n",
            "[Epoch 17] Batch 40/405 Loss=0.1793 Mel=0.0966 Rmel=0.0827 Stop=0.0000\n",
            "[Epoch 17] Batch 50/405 Loss=0.1838 Mel=0.0987 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 17] Batch 60/405 Loss=0.1981 Mel=0.1065 Rmel=0.0916 Stop=0.0000\n",
            "[Epoch 17] Batch 70/405 Loss=0.1823 Mel=0.0980 Rmel=0.0844 Stop=0.0000\n",
            "[Epoch 17] Batch 80/405 Loss=0.2054 Mel=0.1106 Rmel=0.0948 Stop=0.0000\n",
            "[Epoch 17] Batch 90/405 Loss=0.1963 Mel=0.1056 Rmel=0.0907 Stop=0.0000\n",
            "[Epoch 17] Batch 100/405 Loss=0.2029 Mel=0.1093 Rmel=0.0936 Stop=0.0000\n",
            "[Epoch 17] Batch 110/405 Loss=0.1962 Mel=0.1055 Rmel=0.0907 Stop=0.0000\n",
            "[Epoch 17] Batch 120/405 Loss=0.1913 Mel=0.1030 Rmel=0.0883 Stop=0.0000\n",
            "[Epoch 17] Batch 130/405 Loss=0.1874 Mel=0.1009 Rmel=0.0864 Stop=0.0000\n",
            "[Epoch 17] Batch 140/405 Loss=0.2013 Mel=0.1085 Rmel=0.0929 Stop=0.0000\n",
            "[Epoch 17] Batch 150/405 Loss=0.1764 Mel=0.0949 Rmel=0.0814 Stop=0.0000\n",
            "[Epoch 17] Batch 160/405 Loss=0.2000 Mel=0.1072 Rmel=0.0927 Stop=0.0000\n",
            "[Epoch 17] Batch 170/405 Loss=0.1979 Mel=0.1063 Rmel=0.0916 Stop=0.0000\n",
            "[Epoch 17] Batch 180/405 Loss=0.2018 Mel=0.1087 Rmel=0.0931 Stop=0.0000\n",
            "[Epoch 17] Batch 190/405 Loss=0.1834 Mel=0.0989 Rmel=0.0844 Stop=0.0000\n",
            "[Epoch 17] Batch 200/405 Loss=0.1909 Mel=0.1029 Rmel=0.0880 Stop=0.0000\n",
            "[Epoch 17] Batch 210/405 Loss=0.1746 Mel=0.0939 Rmel=0.0806 Stop=0.0000\n",
            "[Epoch 17] Batch 220/405 Loss=0.1990 Mel=0.1070 Rmel=0.0920 Stop=0.0000\n",
            "[Epoch 17] Batch 230/405 Loss=0.1999 Mel=0.1073 Rmel=0.0926 Stop=0.0000\n",
            "[Epoch 17] Batch 240/405 Loss=0.1778 Mel=0.0954 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 17] Batch 250/405 Loss=0.1720 Mel=0.0923 Rmel=0.0797 Stop=0.0000\n",
            "[Epoch 17] Batch 260/405 Loss=0.1985 Mel=0.1063 Rmel=0.0922 Stop=0.0000\n",
            "[Epoch 17] Batch 270/405 Loss=0.1898 Mel=0.1021 Rmel=0.0877 Stop=0.0000\n",
            "[Epoch 17] Batch 280/405 Loss=0.2005 Mel=0.1077 Rmel=0.0928 Stop=0.0000\n",
            "[Epoch 17] Batch 290/405 Loss=0.1849 Mel=0.0995 Rmel=0.0854 Stop=0.0000\n",
            "[Epoch 17] Batch 300/405 Loss=0.2196 Mel=0.1177 Rmel=0.1019 Stop=0.0000\n",
            "[Epoch 17] Batch 310/405 Loss=0.2001 Mel=0.1076 Rmel=0.0925 Stop=0.0000\n",
            "[Epoch 17] Batch 320/405 Loss=0.1761 Mel=0.0947 Rmel=0.0813 Stop=0.0000\n",
            "[Epoch 17] Batch 330/405 Loss=0.1915 Mel=0.1029 Rmel=0.0886 Stop=0.0000\n",
            "[Epoch 17] Batch 340/405 Loss=0.1981 Mel=0.1070 Rmel=0.0911 Stop=0.0000\n",
            "[Epoch 17] Batch 350/405 Loss=0.2592 Mel=0.1427 Rmel=0.1164 Stop=0.0000\n",
            "[Epoch 17] Batch 360/405 Loss=0.2573 Mel=0.1413 Rmel=0.1160 Stop=0.0000\n",
            "[Epoch 17] Batch 370/405 Loss=0.2205 Mel=0.1210 Rmel=0.0995 Stop=0.0000\n",
            "[Epoch 17] Batch 380/405 Loss=0.2295 Mel=0.1263 Rmel=0.1032 Stop=0.0000\n",
            "[Epoch 17] Batch 390/405 Loss=0.2583 Mel=0.1414 Rmel=0.1168 Stop=0.0000\n",
            "[Epoch 17] Batch 400/405 Loss=0.2613 Mel=0.1432 Rmel=0.1180 Stop=0.0000\n",
            "Validation Loss: 0.1916\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_17\n",
            "\n",
            "Epoch 18\n",
            "[Epoch 18] Batch 0/405 Loss=0.2209 Mel=0.1213 Rmel=0.0996 Stop=0.0000\n",
            "[Epoch 18] Batch 10/405 Loss=0.2242 Mel=0.1233 Rmel=0.1008 Stop=0.0000\n",
            "[Epoch 18] Batch 20/405 Loss=0.2123 Mel=0.1168 Rmel=0.0955 Stop=0.0000\n",
            "[Epoch 18] Batch 30/405 Loss=0.2251 Mel=0.1240 Rmel=0.1011 Stop=0.0000\n",
            "[Epoch 18] Batch 40/405 Loss=0.2334 Mel=0.1285 Rmel=0.1049 Stop=0.0000\n",
            "[Epoch 18] Batch 50/405 Loss=0.1946 Mel=0.1070 Rmel=0.0875 Stop=0.0000\n",
            "[Epoch 18] Batch 60/405 Loss=0.2097 Mel=0.1154 Rmel=0.0943 Stop=0.0000\n",
            "[Epoch 18] Batch 70/405 Loss=0.2161 Mel=0.1188 Rmel=0.0973 Stop=0.0000\n",
            "[Epoch 18] Batch 80/405 Loss=0.2486 Mel=0.1365 Rmel=0.1121 Stop=0.0000\n",
            "[Epoch 18] Batch 90/405 Loss=0.1941 Mel=0.1067 Rmel=0.0874 Stop=0.0000\n",
            "[Epoch 18] Batch 100/405 Loss=0.2339 Mel=0.1287 Rmel=0.1051 Stop=0.0000\n",
            "[Epoch 18] Batch 110/405 Loss=0.2136 Mel=0.1176 Rmel=0.0960 Stop=0.0000\n",
            "[Epoch 18] Batch 120/405 Loss=0.2330 Mel=0.1278 Rmel=0.1052 Stop=0.0000\n",
            "[Epoch 18] Batch 130/405 Loss=0.1875 Mel=0.1031 Rmel=0.0845 Stop=0.0000\n",
            "[Epoch 18] Batch 140/405 Loss=0.2217 Mel=0.1220 Rmel=0.0997 Stop=0.0000\n",
            "[Epoch 18] Batch 150/405 Loss=0.2104 Mel=0.1156 Rmel=0.0949 Stop=0.0000\n",
            "[Epoch 18] Batch 160/405 Loss=0.2198 Mel=0.1210 Rmel=0.0988 Stop=0.0000\n",
            "[Epoch 18] Batch 170/405 Loss=0.2278 Mel=0.1251 Rmel=0.1028 Stop=0.0000\n",
            "[Epoch 18] Batch 180/405 Loss=0.2082 Mel=0.1145 Rmel=0.0937 Stop=0.0000\n",
            "[Epoch 18] Batch 190/405 Loss=0.2292 Mel=0.1260 Rmel=0.1032 Stop=0.0000\n",
            "[Epoch 18] Batch 200/405 Loss=0.2169 Mel=0.1194 Rmel=0.0975 Stop=0.0000\n",
            "[Epoch 18] Batch 210/405 Loss=0.2383 Mel=0.1311 Rmel=0.1072 Stop=0.0000\n",
            "[Epoch 18] Batch 220/405 Loss=0.2301 Mel=0.1261 Rmel=0.1039 Stop=0.0000\n",
            "[Epoch 18] Batch 230/405 Loss=0.2253 Mel=0.1240 Rmel=0.1013 Stop=0.0000\n",
            "[Epoch 18] Batch 240/405 Loss=0.2275 Mel=0.1250 Rmel=0.1025 Stop=0.0000\n",
            "[Epoch 18] Batch 250/405 Loss=0.2286 Mel=0.1258 Rmel=0.1028 Stop=0.0000\n",
            "[Epoch 18] Batch 260/405 Loss=0.2145 Mel=0.1179 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 18] Batch 270/405 Loss=0.2211 Mel=0.1216 Rmel=0.0995 Stop=0.0000\n",
            "[Epoch 18] Batch 280/405 Loss=0.2278 Mel=0.1250 Rmel=0.1027 Stop=0.0000\n",
            "[Epoch 18] Batch 290/405 Loss=0.2206 Mel=0.1208 Rmel=0.0998 Stop=0.0000\n",
            "[Epoch 18] Batch 300/405 Loss=0.2441 Mel=0.1341 Rmel=0.1099 Stop=0.0000\n",
            "[Epoch 18] Batch 310/405 Loss=0.2246 Mel=0.1236 Rmel=0.1010 Stop=0.0000\n",
            "[Epoch 18] Batch 320/405 Loss=0.2113 Mel=0.1159 Rmel=0.0953 Stop=0.0000\n",
            "[Epoch 18] Batch 330/405 Loss=0.2234 Mel=0.1228 Rmel=0.1006 Stop=0.0000\n",
            "[Epoch 18] Batch 340/405 Loss=0.2138 Mel=0.1173 Rmel=0.0964 Stop=0.0000\n",
            "[Epoch 18] Batch 350/405 Loss=0.2320 Mel=0.1276 Rmel=0.1044 Stop=0.0000\n",
            "[Epoch 18] Batch 360/405 Loss=0.1940 Mel=0.1069 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 18] Batch 370/405 Loss=0.2207 Mel=0.1214 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 18] Batch 380/405 Loss=0.2166 Mel=0.1189 Rmel=0.0976 Stop=0.0000\n",
            "[Epoch 18] Batch 390/405 Loss=0.2193 Mel=0.1204 Rmel=0.0989 Stop=0.0000\n",
            "[Epoch 18] Batch 400/405 Loss=0.2351 Mel=0.1295 Rmel=0.1056 Stop=0.0000\n",
            "Validation Loss: 0.1785\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_18\n",
            "\n",
            "Epoch 19\n",
            "[Epoch 19] Batch 0/405 Loss=0.2319 Mel=0.1269 Rmel=0.1050 Stop=0.0000\n",
            "[Epoch 19] Batch 10/405 Loss=0.2255 Mel=0.1240 Rmel=0.1015 Stop=0.0000\n",
            "[Epoch 19] Batch 20/405 Loss=0.2161 Mel=0.1190 Rmel=0.0971 Stop=0.0000\n",
            "[Epoch 19] Batch 30/405 Loss=0.1930 Mel=0.1060 Rmel=0.0870 Stop=0.0000\n",
            "[Epoch 19] Batch 40/405 Loss=0.1951 Mel=0.1070 Rmel=0.0881 Stop=0.0000\n",
            "[Epoch 19] Batch 50/405 Loss=0.2197 Mel=0.1209 Rmel=0.0988 Stop=0.0000\n",
            "[Epoch 19] Batch 60/405 Loss=0.2086 Mel=0.1146 Rmel=0.0940 Stop=0.0000\n",
            "[Epoch 19] Batch 70/405 Loss=0.2106 Mel=0.1156 Rmel=0.0950 Stop=0.0000\n",
            "[Epoch 19] Batch 80/405 Loss=0.2187 Mel=0.1202 Rmel=0.0985 Stop=0.0000\n",
            "[Epoch 19] Batch 90/405 Loss=0.2277 Mel=0.1253 Rmel=0.1024 Stop=0.0000\n",
            "[Epoch 19] Batch 100/405 Loss=0.2088 Mel=0.1151 Rmel=0.0937 Stop=0.0000\n",
            "[Epoch 19] Batch 110/405 Loss=0.2245 Mel=0.1231 Rmel=0.1013 Stop=0.0000\n",
            "[Epoch 19] Batch 120/405 Loss=0.2198 Mel=0.1207 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 19] Batch 130/405 Loss=0.2190 Mel=0.1203 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 19] Batch 140/405 Loss=0.2196 Mel=0.1203 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 19] Batch 150/405 Loss=0.2039 Mel=0.1120 Rmel=0.0919 Stop=0.0000\n",
            "[Epoch 19] Batch 160/405 Loss=0.2100 Mel=0.1155 Rmel=0.0945 Stop=0.0000\n",
            "[Epoch 19] Batch 170/405 Loss=0.2265 Mel=0.1241 Rmel=0.1024 Stop=0.0000\n",
            "[Epoch 19] Batch 180/405 Loss=0.2305 Mel=0.1271 Rmel=0.1034 Stop=0.0000\n",
            "[Epoch 19] Batch 190/405 Loss=0.2305 Mel=0.1267 Rmel=0.1038 Stop=0.0000\n",
            "[Epoch 19] Batch 200/405 Loss=0.2193 Mel=0.1205 Rmel=0.0988 Stop=0.0000\n",
            "[Epoch 19] Batch 210/405 Loss=0.1996 Mel=0.1097 Rmel=0.0899 Stop=0.0000\n",
            "[Epoch 19] Batch 220/405 Loss=0.2151 Mel=0.1184 Rmel=0.0967 Stop=0.0000\n",
            "[Epoch 19] Batch 230/405 Loss=0.2271 Mel=0.1248 Rmel=0.1023 Stop=0.0000\n",
            "[Epoch 19] Batch 240/405 Loss=0.1862 Mel=0.1022 Rmel=0.0841 Stop=0.0000\n",
            "[Epoch 19] Batch 250/405 Loss=0.2002 Mel=0.1102 Rmel=0.0900 Stop=0.0000\n",
            "[Epoch 19] Batch 260/405 Loss=0.2263 Mel=0.1240 Rmel=0.1023 Stop=0.0000\n",
            "[Epoch 19] Batch 270/405 Loss=0.2146 Mel=0.1175 Rmel=0.0970 Stop=0.0000\n",
            "[Epoch 19] Batch 280/405 Loss=0.2036 Mel=0.1119 Rmel=0.0917 Stop=0.0000\n",
            "[Epoch 19] Batch 290/405 Loss=0.2182 Mel=0.1197 Rmel=0.0985 Stop=0.0000\n",
            "[Epoch 19] Batch 300/405 Loss=0.1867 Mel=0.1026 Rmel=0.0841 Stop=0.0000\n",
            "[Epoch 19] Batch 310/405 Loss=0.2029 Mel=0.1119 Rmel=0.0910 Stop=0.0000\n",
            "[Epoch 19] Batch 320/405 Loss=0.2252 Mel=0.1239 Rmel=0.1014 Stop=0.0000\n",
            "[Epoch 19] Batch 330/405 Loss=0.2215 Mel=0.1218 Rmel=0.0997 Stop=0.0000\n",
            "[Epoch 19] Batch 340/405 Loss=0.1914 Mel=0.1052 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 19] Batch 350/405 Loss=0.1860 Mel=0.1024 Rmel=0.0836 Stop=0.0000\n",
            "[Epoch 19] Batch 360/405 Loss=0.2106 Mel=0.1159 Rmel=0.0946 Stop=0.0000\n",
            "[Epoch 19] Batch 370/405 Loss=0.1950 Mel=0.1069 Rmel=0.0881 Stop=0.0000\n",
            "[Epoch 19] Batch 380/405 Loss=0.1785 Mel=0.0981 Rmel=0.0804 Stop=0.0000\n",
            "[Epoch 19] Batch 390/405 Loss=0.2057 Mel=0.1129 Rmel=0.0927 Stop=0.0000\n",
            "[Epoch 19] Batch 400/405 Loss=0.1891 Mel=0.1036 Rmel=0.0855 Stop=0.0000\n",
            "Validation Loss: 0.1753\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_19\n",
            "\n",
            "Epoch 20\n",
            "[Epoch 20] Batch 0/405 Loss=0.1974 Mel=0.1086 Rmel=0.0888 Stop=0.0000\n",
            "[Epoch 20] Batch 10/405 Loss=0.2207 Mel=0.1211 Rmel=0.0996 Stop=0.0000\n",
            "[Epoch 20] Batch 20/405 Loss=0.2049 Mel=0.1125 Rmel=0.0924 Stop=0.0000\n",
            "[Epoch 20] Batch 30/405 Loss=0.2171 Mel=0.1193 Rmel=0.0978 Stop=0.0000\n",
            "[Epoch 20] Batch 40/405 Loss=0.2104 Mel=0.1158 Rmel=0.0945 Stop=0.0000\n",
            "[Epoch 20] Batch 50/405 Loss=0.2089 Mel=0.1146 Rmel=0.0943 Stop=0.0000\n",
            "[Epoch 20] Batch 60/405 Loss=0.2203 Mel=0.1213 Rmel=0.0991 Stop=0.0000\n",
            "[Epoch 20] Batch 70/405 Loss=0.2260 Mel=0.1242 Rmel=0.1019 Stop=0.0000\n",
            "[Epoch 20] Batch 80/405 Loss=0.1872 Mel=0.1029 Rmel=0.0843 Stop=0.0000\n",
            "[Epoch 20] Batch 90/405 Loss=0.2001 Mel=0.1101 Rmel=0.0900 Stop=0.0000\n",
            "[Epoch 20] Batch 100/405 Loss=0.2180 Mel=0.1201 Rmel=0.0979 Stop=0.0000\n",
            "[Epoch 20] Batch 110/405 Loss=0.1825 Mel=0.1002 Rmel=0.0823 Stop=0.0000\n",
            "[Epoch 20] Batch 120/405 Loss=0.2015 Mel=0.1109 Rmel=0.0905 Stop=0.0000\n",
            "[Epoch 20] Batch 130/405 Loss=0.2085 Mel=0.1146 Rmel=0.0939 Stop=0.0000\n",
            "[Epoch 20] Batch 140/405 Loss=0.2159 Mel=0.1187 Rmel=0.0972 Stop=0.0000\n",
            "[Epoch 20] Batch 150/405 Loss=0.1956 Mel=0.1074 Rmel=0.0882 Stop=0.0000\n",
            "[Epoch 20] Batch 160/405 Loss=0.2067 Mel=0.1139 Rmel=0.0928 Stop=0.0000\n",
            "[Epoch 20] Batch 170/405 Loss=0.2198 Mel=0.1208 Rmel=0.0990 Stop=0.0000\n",
            "[Epoch 20] Batch 180/405 Loss=0.1946 Mel=0.1069 Rmel=0.0877 Stop=0.0000\n",
            "[Epoch 20] Batch 190/405 Loss=0.1879 Mel=0.1032 Rmel=0.0847 Stop=0.0000\n",
            "[Epoch 20] Batch 200/405 Loss=0.2015 Mel=0.1106 Rmel=0.0908 Stop=0.0000\n",
            "[Epoch 20] Batch 210/405 Loss=0.1947 Mel=0.1067 Rmel=0.0879 Stop=0.0000\n",
            "[Epoch 20] Batch 220/405 Loss=0.2062 Mel=0.1131 Rmel=0.0931 Stop=0.0000\n",
            "[Epoch 20] Batch 230/405 Loss=0.1886 Mel=0.1032 Rmel=0.0854 Stop=0.0000\n",
            "[Epoch 20] Batch 240/405 Loss=0.1960 Mel=0.1068 Rmel=0.0891 Stop=0.0000\n",
            "[Epoch 20] Batch 250/405 Loss=0.2033 Mel=0.1108 Rmel=0.0925 Stop=0.0000\n",
            "[Epoch 20] Batch 260/405 Loss=0.2037 Mel=0.1110 Rmel=0.0927 Stop=0.0000\n",
            "[Epoch 20] Batch 270/405 Loss=0.1870 Mel=0.1017 Rmel=0.0852 Stop=0.0000\n",
            "[Epoch 20] Batch 280/405 Loss=0.1959 Mel=0.1069 Rmel=0.0890 Stop=0.0000\n",
            "[Epoch 20] Batch 290/405 Loss=0.1868 Mel=0.1015 Rmel=0.0853 Stop=0.0000\n",
            "[Epoch 20] Batch 300/405 Loss=0.1976 Mel=0.1079 Rmel=0.0897 Stop=0.0000\n",
            "[Epoch 20] Batch 310/405 Loss=0.2001 Mel=0.1089 Rmel=0.0911 Stop=0.0000\n",
            "[Epoch 20] Batch 320/405 Loss=0.2101 Mel=0.1149 Rmel=0.0951 Stop=0.0000\n",
            "[Epoch 20] Batch 330/405 Loss=0.3401 Mel=0.1785 Rmel=0.1616 Stop=0.0000\n",
            "[Epoch 20] Batch 340/405 Loss=0.2491 Mel=0.1354 Rmel=0.1136 Stop=0.0000\n",
            "[Epoch 20] Batch 350/405 Loss=0.2180 Mel=0.1191 Rmel=0.0989 Stop=0.0000\n",
            "[Epoch 20] Batch 360/405 Loss=0.2312 Mel=0.1264 Rmel=0.1048 Stop=0.0000\n",
            "[Epoch 20] Batch 370/405 Loss=0.2170 Mel=0.1192 Rmel=0.0978 Stop=0.0000\n",
            "[Epoch 20] Batch 380/405 Loss=0.2187 Mel=0.1200 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 20] Batch 390/405 Loss=0.2216 Mel=0.1218 Rmel=0.0998 Stop=0.0000\n",
            "[Epoch 20] Batch 400/405 Loss=0.1991 Mel=0.1096 Rmel=0.0895 Stop=0.0000\n",
            "Validation Loss: 0.1799\n",
            "Removed shared tensor {'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.bias_ih_l0', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.bias_hh_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: /content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_20\n",
            "\n",
            "Epoch 21\n",
            "[Epoch 21] Batch 0/405 Loss=0.2090 Mel=0.1153 Rmel=0.0936 Stop=0.0000\n",
            "[Epoch 21] Batch 10/405 Loss=0.2280 Mel=0.1256 Rmel=0.1024 Stop=0.0000\n",
            "[Epoch 21] Batch 20/405 Loss=0.2185 Mel=0.1202 Rmel=0.0983 Stop=0.0000\n",
            "[Epoch 21] Batch 30/405 Loss=0.1888 Mel=0.1038 Rmel=0.0849 Stop=0.0000\n",
            "[Epoch 21] Batch 40/405 Loss=0.2148 Mel=0.1182 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 21] Batch 50/405 Loss=0.1916 Mel=0.1051 Rmel=0.0864 Stop=0.0000\n",
            "[Epoch 21] Batch 60/405 Loss=0.2260 Mel=0.1241 Rmel=0.1019 Stop=0.0000\n",
            "[Epoch 21] Batch 70/405 Loss=0.2160 Mel=0.1191 Rmel=0.0969 Stop=0.0000\n",
            "[Epoch 21] Batch 80/405 Loss=0.2040 Mel=0.1125 Rmel=0.0915 Stop=0.0000\n",
            "[Epoch 21] Batch 90/405 Loss=0.1878 Mel=0.1032 Rmel=0.0846 Stop=0.0000\n",
            "[Epoch 21] Batch 100/405 Loss=0.1899 Mel=0.1045 Rmel=0.0855 Stop=0.0000\n",
            "[Epoch 21] Batch 110/405 Loss=0.2208 Mel=0.1214 Rmel=0.0994 Stop=0.0000\n",
            "[Epoch 21] Batch 120/405 Loss=0.2161 Mel=0.1189 Rmel=0.0972 Stop=0.0000\n",
            "[Epoch 21] Batch 130/405 Loss=0.2159 Mel=0.1190 Rmel=0.0970 Stop=0.0000\n",
            "[Epoch 21] Batch 140/405 Loss=0.2018 Mel=0.1110 Rmel=0.0908 Stop=0.0000\n",
            "[Epoch 21] Batch 150/405 Loss=0.2150 Mel=0.1184 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 21] Batch 160/405 Loss=0.2247 Mel=0.1240 Rmel=0.1007 Stop=0.0000\n",
            "[Epoch 21] Batch 170/405 Loss=0.2270 Mel=0.1247 Rmel=0.1023 Stop=0.0000\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 1264, in wait\n",
            "    return self._wait(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 2053, in _wait\n",
            "    (pid, sts) = self._try_wait(0)\n",
            "                 ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 2011, in _try_wait\n",
            "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 1281, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/commands/launch.py\", line 866, in simple_launcher\n",
            "    process.wait()\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 1277, in wait\n",
            "    self._wait(timeout=sigint_timeout)\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 2043, in _wait\n",
            "    remaining = self._remaining_time(endtime)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/subprocess.py\", line 1244, in _remaining_time\n",
            "    return endtime - _time()\n",
            "                     ^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/266 Group Project/train_taco.py\", line 237, in <module>\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/266 Group Project/train_taco.py\", line 138, in main\n",
            "    accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/accelerate/accelerator.py\", line 2852, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\", line 625, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\", line 354, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\", line 841, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "Nm2kPPUlQk7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import Accelerator\n",
        "from tokenizer import Tokenizer\n",
        "from model import Tacotron2Config\n",
        "from emotion_tacotron_model import EmotionTacotron2\n",
        "import torch\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/266 Group Project/tacotron2_emotion/checkpoint_16\"\n",
        "\n",
        "# 1. Create accelerator FIRST\n",
        "accelerator = Accelerator()\n",
        "\n",
        "# 2. Build model normally (NOT prepared!)\n",
        "tokenizer = Tokenizer()\n",
        "config = Tacotron2Config(\n",
        "    num_chars=tokenizer.vocab_size,\n",
        "    pad_token_id=tokenizer.pad_token_id\n",
        ")\n",
        "\n",
        "model = EmotionTacotron2(config)\n",
        "\n",
        "# 3. Load accelerate checkpoint BEFORE preparing\n",
        "accelerator.load_state(checkpoint_dir, model=model)\n",
        "\n",
        "# 4. NOW prepare the model\n",
        "model = accelerator.prepare(model)\n",
        "\n",
        "# 5. Move to GPU\n",
        "model = model.to(\"cuda\").eval()\n",
        "\n",
        "print(\"✔ Model loaded successfully from accelerate checkpoint!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A1peP8CqJBx7",
        "outputId": "e8824249-c222-469c-9531-d8e4d5bc5c5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ Model loaded successfully from accelerate checkpoint!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
