{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate librosa soundfile\n"
      ],
      "metadata": {
        "id": "xZA1H7czMhgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "e33b137e-00fd-4322-ad50-eea3aefa000b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (1.12.0)\r\n",
            "Requirement already satisfied: librosa in /opt/conda/lib/python3.10/site-packages (0.11.0)\r\n",
            "Requirement already satisfied: soundfile in /opt/conda/lib/python3.10/site-packages (0.13.1)\r\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.2)\r\n",
            "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (23.1)\r\n",
            "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.0)\r\n",
            "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\r\n",
            "Requirement already satisfied: torch>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\r\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.1.7)\r\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.7.0)\r\n",
            "Requirement already satisfied: audioread>=2.1.9 in /opt/conda/lib/python3.10/site-packages (from librosa) (3.1.0)\r\n",
            "Requirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.62.1)\r\n",
            "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.15.3)\r\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.7.2)\r\n",
            "Requirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.5.2)\r\n",
            "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (5.1.1)\r\n",
            "Requirement already satisfied: pooch>=1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.8.2)\r\n",
            "Requirement already satisfied: soxr>=0.3.2 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.0.0)\r\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (4.15.0)\r\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /opt/conda/lib/python3.10/site-packages (from librosa) (0.4)\r\n",
            "Requirement already satisfied: msgpack>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa) (1.1.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
            "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2023.12.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.65.0)\n",
            "Requirement already satisfied: typer-slim in /opt/conda/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (0.20.0)\n",
            "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.45.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from pooch>=1.1->librosa) (2.31.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (4.1.0)\n",
            "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.9)\n",
            "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (3.4)\n",
            "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (1.26.18)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer-slim->huggingface_hub>=0.21.0->accelerate) (8.1.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate) (1.0.4)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CELL 4 â€” Confirm required files exist (your 3 core files)"
      ],
      "metadata": {
        "id": "7_34CmIaN4Os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "required = [\"dataset.py\", \"model.py\", \"train_taco.py\",\"tokenizer.py\"]\n",
        "\n",
        "print(\"Checking required files:\\n\")\n",
        "for f in required:\n",
        "    print(f, \" FOUND\" if os.path.exists(f) else \" MISSING\")"
      ],
      "metadata": {
        "id": "mtyl1FcSMnIk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8f84d64-bb36-448c-e4a9-f6ab01681072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking required files:\n",
            "\n",
            "dataset.py  FOUND\n",
            "model.py  FOUND\n",
            "train_taco.py  FOUND\n",
            "tokenizer.py  FOUND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "ljs_base = \"LJSpeech-1.1/wavs\"\n",
        "print(\"LJSpeech wavs folder:\", \" FOUND\" if os.path.exists(ljs_base) else \" MISSING\")\n"
      ],
      "metadata": {
        "id": "6B6wLvaLOHDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a52ab167-8c28-403e-9d85-581f7537505e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LJSpeech wavs folder:  FOUND\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"datasplit/train_metadata.csv\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "e2oS5JyRtxpv",
        "outputId": "9179df72-8b24-4dbc-9644-a547e96bd3c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           file_path  \\\n",
              "0  /content/drive/MyDrive/266 Group Project/LJSpe...   \n",
              "1  /content/drive/MyDrive/266 Group Project/LJSpe...   \n",
              "2  /content/drive/MyDrive/266 Group Project/LJSpe...   \n",
              "3  /content/drive/MyDrive/266 Group Project/LJSpe...   \n",
              "4  /content/drive/MyDrive/266 Group Project/LJSpe...   \n",
              "\n",
              "                               normalized_transcript  duration  neutral  \n",
              "0  based on the foregoing considerations, is conv...  10.09619  neutral  \n",
              "1  of attempting to draw any general distinction ...  10.09619  neutral  \n",
              "2  the FBI, which was established within the Depa...  10.09619  neutral  \n",
              "3  hundreds of women and children came in every m...  10.09619  neutral  \n",
              "4  West Fifth Street, Irving, Texas. After receiv...  10.09619  neutral  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_path</th>\n",
              "      <th>normalized_transcript</th>\n",
              "      <th>duration</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>/content/drive/MyDrive/266 Group Project/LJSpe...</td>\n",
              "      <td>based on the foregoing considerations, is conv...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>/content/drive/MyDrive/266 Group Project/LJSpe...</td>\n",
              "      <td>of attempting to draw any general distinction ...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>/content/drive/MyDrive/266 Group Project/LJSpe...</td>\n",
              "      <td>the FBI, which was established within the Depa...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>/content/drive/MyDrive/266 Group Project/LJSpe...</td>\n",
              "      <td>hundreds of women and children came in every m...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>/content/drive/MyDrive/266 Group Project/LJSpe...</td>\n",
              "      <td>West Fifth Street, Irving, Texas. After receiv...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_manifest = \"datasplit/train_metadata.csv\"\n",
        "\n",
        "df = pd.read_csv(train_manifest)\n",
        "\n",
        "df[\"file_path\"] = df[\"file_path\"].apply(\n",
        "    lambda x: x.replace(\n",
        "        \"/content/drive/MyDrive/266 Group Project/LJSpeech-1.1/wavs\",\n",
        "        \"LJSpeech-1.1/wavs\"\n",
        "    )\n",
        ")\n",
        "\n",
        "df.to_csv(train_manifest, index=False)\n",
        "\n",
        "print(\"Train manifest updated.\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "dcYs4sSK2aCW",
        "outputId": "36fcebfa-bbe5-4846-f19e-ac3e1b2641eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train manifest updated.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          file_path  \\\n",
              "0  LJSpeech-1.1/wavs/LJ026-0014.wav   \n",
              "1  LJSpeech-1.1/wavs/LJ025-0103.wav   \n",
              "2  LJSpeech-1.1/wavs/LJ049-0163.wav   \n",
              "3  LJSpeech-1.1/wavs/LJ003-0007.wav   \n",
              "4  LJSpeech-1.1/wavs/LJ047-0150.wav   \n",
              "\n",
              "                               normalized_transcript  duration  neutral  \n",
              "0  based on the foregoing considerations, is conv...  10.09619  neutral  \n",
              "1  of attempting to draw any general distinction ...  10.09619  neutral  \n",
              "2  the FBI, which was established within the Depa...  10.09619  neutral  \n",
              "3  hundreds of women and children came in every m...  10.09619  neutral  \n",
              "4  West Fifth Street, Irving, Texas. After receiv...  10.09619  neutral  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_path</th>\n",
              "      <th>normalized_transcript</th>\n",
              "      <th>duration</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ026-0014.wav</td>\n",
              "      <td>based on the foregoing considerations, is conv...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ025-0103.wav</td>\n",
              "      <td>of attempting to draw any general distinction ...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ049-0163.wav</td>\n",
              "      <td>the FBI, which was established within the Depa...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ003-0007.wav</td>\n",
              "      <td>hundreds of women and children came in every m...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ047-0150.wav</td>\n",
              "      <td>West Fifth Street, Irving, Texas. After receiv...</td>\n",
              "      <td>10.09619</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_manifest = \"datasplit/test_metadata.csv\"\n",
        "\n",
        "df = pd.read_csv(val_manifest)\n",
        "\n",
        "df[\"file_path\"] = df[\"file_path\"].apply(\n",
        "    lambda x: x.replace(\n",
        "        \"/content/drive/MyDrive/266 Group Project/LJSpeech-1.1/wavs\",\n",
        "        \"LJSpeech-1.1/wavs\"\n",
        "    )\n",
        ")\n",
        "\n",
        "df.to_csv(val_manifest, index=False)\n",
        "\n",
        "print(\"Validation manifest updated.\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "H6w_c7eC2QP-",
        "outputId": "232663e4-d90f-4f06-de15-b92fc93546ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation manifest updated.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          file_path  \\\n",
              "0  LJSpeech-1.1/wavs/LJ002-0169.wav   \n",
              "1  LJSpeech-1.1/wavs/LJ035-0085.wav   \n",
              "2  LJSpeech-1.1/wavs/LJ024-0013.wav   \n",
              "3  LJSpeech-1.1/wavs/LJ040-0208.wav   \n",
              "4  LJSpeech-1.1/wavs/LJ013-0135.wav   \n",
              "\n",
              "                               normalized_transcript  duration  neutral  \n",
              "0  He had been in the employ of a corn-chandler a...  7.669705  neutral  \n",
              "1  from the southeast corner of the sixth floor d...  4.604671  neutral  \n",
              "2  This plan will save our national Constitution ...  6.137188  neutral  \n",
              "3  Carro reported that when questioned about his ...  5.231610  neutral  \n",
              "4  At most of the clubs the servants had been mul...  7.600045  neutral  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_path</th>\n",
              "      <th>normalized_transcript</th>\n",
              "      <th>duration</th>\n",
              "      <th>neutral</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ002-0169.wav</td>\n",
              "      <td>He had been in the employ of a corn-chandler a...</td>\n",
              "      <td>7.669705</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ035-0085.wav</td>\n",
              "      <td>from the southeast corner of the sixth floor d...</td>\n",
              "      <td>4.604671</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ024-0013.wav</td>\n",
              "      <td>This plan will save our national Constitution ...</td>\n",
              "      <td>6.137188</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ040-0208.wav</td>\n",
              "      <td>Carro reported that when questioned about his ...</td>\n",
              "      <td>5.231610</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LJSpeech-1.1/wavs/LJ013-0135.wav</td>\n",
              "      <td>At most of the clubs the servants had been mul...</td>\n",
              "      <td>7.600045</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch train_taco.py \\\n",
        "  --experiment_name tacotron2 \\\n",
        "  --run_name tacotron2_run \\\n",
        "  --working_directory work_dir \\\n",
        "  --save_audio_gen inference_outputs \\\n",
        "  --path_to_train_manifest datasplit/train_metadata.csv \\\n",
        "  --path_to_val_manifest datasplit/test_metadata.csv \\\n",
        "  --training_epochs 25 \\\n",
        "  --batch_size 32 \\\n",
        "  --learning_rate 0.001 \\\n",
        "  --console_out_iters 50 \\\n",
        "  --checkpoint_epochs 1 \\\n",
        "  --num_workers 0 \\\n",
        "  --no-log_wandb\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsK-AAy51EDG",
        "outputId": "e2c8ffea-dded-4af6-fab9-84e970eacd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following values were not passed to `accelerate launch` and had defaults used instead:\r\n",
            "\t`--num_processes` was set to a value of `1`\r\n",
            "\t`--num_machines` was set to a value of `1`\r\n",
            "\t`--mixed_precision` was set to a value of `'no'`\r\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\r\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Total Trainable Params: 29227777\n",
            "\n",
            "Epoch 0\n",
            "[Epoch 0] Batch 0/405 Loss=4.9930 Mel=1.5429 Rmel=2.9827 Stop=0.4674\n",
            "[Epoch 0] Batch 10/405 Loss=2.0866 Mel=0.9986 Rmel=1.0834 Stop=0.0046\n",
            "[Epoch 0] Batch 20/405 Loss=1.3824 Mel=0.5643 Rmel=0.8166 Stop=0.0014\n",
            "[Epoch 0] Batch 30/405 Loss=1.1752 Mel=0.3955 Rmel=0.7787 Stop=0.0009\n",
            "[Epoch 0] Batch 40/405 Loss=1.0374 Mel=0.4331 Rmel=0.6026 Stop=0.0016\n",
            "[Epoch 0] Batch 50/405 Loss=0.9659 Mel=0.4136 Rmel=0.5498 Stop=0.0024\n",
            "[Epoch 0] Batch 60/405 Loss=1.1213 Mel=0.4482 Rmel=0.6696 Stop=0.0035\n",
            "[Epoch 0] Batch 70/405 Loss=0.8081 Mel=0.3780 Rmel=0.4284 Stop=0.0017\n",
            "[Epoch 0] Batch 80/405 Loss=1.0199 Mel=0.4225 Rmel=0.5964 Stop=0.0011\n",
            "[Epoch 0] Batch 90/405 Loss=0.7365 Mel=0.3511 Rmel=0.3846 Stop=0.0008\n",
            "[Epoch 0] Batch 100/405 Loss=0.6443 Mel=0.3178 Rmel=0.3260 Stop=0.0005\n",
            "[Epoch 0] Batch 110/405 Loss=0.6791 Mel=0.3273 Rmel=0.3513 Stop=0.0005\n",
            "[Epoch 0] Batch 120/405 Loss=0.6175 Mel=0.3046 Rmel=0.3125 Stop=0.0005\n",
            "[Epoch 0] Batch 130/405 Loss=0.6987 Mel=0.3376 Rmel=0.3607 Stop=0.0004\n",
            "[Epoch 0] Batch 140/405 Loss=0.6394 Mel=0.3135 Rmel=0.3254 Stop=0.0004\n",
            "[Epoch 0] Batch 150/405 Loss=0.6843 Mel=0.3104 Rmel=0.3734 Stop=0.0005\n",
            "[Epoch 0] Batch 160/405 Loss=0.5750 Mel=0.2873 Rmel=0.2873 Stop=0.0004\n",
            "[Epoch 0] Batch 170/405 Loss=0.6841 Mel=0.3161 Rmel=0.3676 Stop=0.0004\n",
            "[Epoch 0] Batch 180/405 Loss=0.5680 Mel=0.2838 Rmel=0.2838 Stop=0.0004\n",
            "[Epoch 0] Batch 190/405 Loss=0.5349 Mel=0.2653 Rmel=0.2692 Stop=0.0004\n",
            "[Epoch 0] Batch 200/405 Loss=0.5656 Mel=0.2828 Rmel=0.2824 Stop=0.0004\n",
            "[Epoch 0] Batch 210/405 Loss=0.5114 Mel=0.2568 Rmel=0.2543 Stop=0.0003\n",
            "[Epoch 0] Batch 220/405 Loss=0.5645 Mel=0.2767 Rmel=0.2874 Stop=0.0003\n",
            "[Epoch 0] Batch 230/405 Loss=0.5461 Mel=0.2736 Rmel=0.2722 Stop=0.0003\n",
            "[Epoch 0] Batch 240/405 Loss=0.5505 Mel=0.2755 Rmel=0.2748 Stop=0.0003\n",
            "[Epoch 0] Batch 250/405 Loss=0.4832 Mel=0.2328 Rmel=0.2502 Stop=0.0003\n",
            "[Epoch 0] Batch 260/405 Loss=0.5451 Mel=0.2687 Rmel=0.2761 Stop=0.0003\n",
            "[Epoch 0] Batch 270/405 Loss=0.5043 Mel=0.2391 Rmel=0.2650 Stop=0.0003\n",
            "[Epoch 0] Batch 280/405 Loss=0.4802 Mel=0.2393 Rmel=0.2406 Stop=0.0003\n",
            "[Epoch 0] Batch 290/405 Loss=0.5314 Mel=0.2680 Rmel=0.2631 Stop=0.0003\n",
            "[Epoch 0] Batch 300/405 Loss=0.5409 Mel=0.2703 Rmel=0.2703 Stop=0.0003\n",
            "[Epoch 0] Batch 310/405 Loss=0.4959 Mel=0.2467 Rmel=0.2490 Stop=0.0002\n",
            "[Epoch 0] Batch 320/405 Loss=0.4660 Mel=0.2346 Rmel=0.2312 Stop=0.0002\n",
            "[Epoch 0] Batch 330/405 Loss=0.4972 Mel=0.2493 Rmel=0.2477 Stop=0.0002\n",
            "[Epoch 0] Batch 340/405 Loss=0.4812 Mel=0.2410 Rmel=0.2400 Stop=0.0002\n",
            "[Epoch 0] Batch 350/405 Loss=0.4858 Mel=0.2445 Rmel=0.2410 Stop=0.0002\n",
            "[Epoch 0] Batch 360/405 Loss=0.4416 Mel=0.2219 Rmel=0.2195 Stop=0.0002\n",
            "[Epoch 0] Batch 370/405 Loss=0.4767 Mel=0.2396 Rmel=0.2369 Stop=0.0002\n",
            "[Epoch 0] Batch 380/405 Loss=0.4087 Mel=0.2061 Rmel=0.2024 Stop=0.0002\n",
            "[Epoch 0] Batch 390/405 Loss=0.4667 Mel=0.2352 Rmel=0.2314 Stop=0.0002\n",
            "[Epoch 0] Batch 400/405 Loss=0.4992 Mel=0.2430 Rmel=0.2560 Stop=0.0002\n",
            "Validation Loss: 0.3831\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_0\n",
            "\n",
            "Epoch 1\n",
            "[Epoch 1] Batch 0/405 Loss=0.4558 Mel=0.2282 Rmel=0.2274 Stop=0.0002\n",
            "[Epoch 1] Batch 10/405 Loss=0.4688 Mel=0.2370 Rmel=0.2316 Stop=0.0002\n",
            "[Epoch 1] Batch 20/405 Loss=0.4612 Mel=0.2317 Rmel=0.2293 Stop=0.0002\n",
            "[Epoch 1] Batch 30/405 Loss=0.4381 Mel=0.2188 Rmel=0.2190 Stop=0.0002\n",
            "[Epoch 1] Batch 40/405 Loss=0.4134 Mel=0.2092 Rmel=0.2041 Stop=0.0002\n",
            "[Epoch 1] Batch 50/405 Loss=0.4286 Mel=0.2166 Rmel=0.2118 Stop=0.0002\n",
            "[Epoch 1] Batch 60/405 Loss=0.4138 Mel=0.2092 Rmel=0.2044 Stop=0.0002\n",
            "[Epoch 1] Batch 70/405 Loss=0.4232 Mel=0.2146 Rmel=0.2084 Stop=0.0002\n",
            "[Epoch 1] Batch 80/405 Loss=0.3590 Mel=0.1795 Rmel=0.1794 Stop=0.0001\n",
            "[Epoch 1] Batch 90/405 Loss=0.4425 Mel=0.2240 Rmel=0.2183 Stop=0.0002\n",
            "[Epoch 1] Batch 100/405 Loss=0.4510 Mel=0.2245 Rmel=0.2262 Stop=0.0002\n",
            "[Epoch 1] Batch 110/405 Loss=0.4150 Mel=0.2103 Rmel=0.2044 Stop=0.0002\n",
            "[Epoch 1] Batch 120/405 Loss=0.3992 Mel=0.2028 Rmel=0.1963 Stop=0.0002\n",
            "[Epoch 1] Batch 130/405 Loss=0.3815 Mel=0.1936 Rmel=0.1877 Stop=0.0002\n",
            "[Epoch 1] Batch 140/405 Loss=0.4922 Mel=0.2342 Rmel=0.2578 Stop=0.0002\n",
            "[Epoch 1] Batch 150/405 Loss=0.4115 Mel=0.2084 Rmel=0.2029 Stop=0.0002\n",
            "[Epoch 1] Batch 160/405 Loss=0.4047 Mel=0.2057 Rmel=0.1989 Stop=0.0002\n",
            "[Epoch 1] Batch 170/405 Loss=0.3952 Mel=0.2008 Rmel=0.1942 Stop=0.0002\n",
            "[Epoch 1] Batch 180/405 Loss=0.4012 Mel=0.2038 Rmel=0.1972 Stop=0.0002\n",
            "[Epoch 1] Batch 190/405 Loss=0.3501 Mel=0.1771 Rmel=0.1728 Stop=0.0002\n",
            "[Epoch 1] Batch 200/405 Loss=0.3426 Mel=0.1745 Rmel=0.1679 Stop=0.0001\n",
            "[Epoch 1] Batch 210/405 Loss=0.3893 Mel=0.1972 Rmel=0.1920 Stop=0.0002\n",
            "[Epoch 1] Batch 220/405 Loss=0.3972 Mel=0.2027 Rmel=0.1944 Stop=0.0002\n",
            "[Epoch 1] Batch 230/405 Loss=0.4203 Mel=0.2096 Rmel=0.2105 Stop=0.0002\n",
            "[Epoch 1] Batch 240/405 Loss=0.4235 Mel=0.2131 Rmel=0.2102 Stop=0.0002\n",
            "[Epoch 1] Batch 250/405 Loss=0.3646 Mel=0.1832 Rmel=0.1813 Stop=0.0001\n",
            "[Epoch 1] Batch 260/405 Loss=0.3738 Mel=0.1911 Rmel=0.1825 Stop=0.0001\n",
            "[Epoch 1] Batch 270/405 Loss=0.4544 Mel=0.2116 Rmel=0.2426 Stop=0.0002\n",
            "[Epoch 1] Batch 280/405 Loss=0.3768 Mel=0.1924 Rmel=0.1843 Stop=0.0001\n",
            "[Epoch 1] Batch 290/405 Loss=0.3937 Mel=0.1999 Rmel=0.1937 Stop=0.0002\n",
            "[Epoch 1] Batch 300/405 Loss=0.3948 Mel=0.2015 Rmel=0.1931 Stop=0.0002\n",
            "[Epoch 1] Batch 310/405 Loss=0.3787 Mel=0.1933 Rmel=0.1852 Stop=0.0002\n",
            "[Epoch 1] Batch 320/405 Loss=0.3483 Mel=0.1781 Rmel=0.1701 Stop=0.0001\n",
            "[Epoch 1] Batch 330/405 Loss=0.3675 Mel=0.1884 Rmel=0.1789 Stop=0.0001\n",
            "[Epoch 1] Batch 340/405 Loss=0.3937 Mel=0.2018 Rmel=0.1917 Stop=0.0001\n",
            "[Epoch 1] Batch 350/405 Loss=0.3602 Mel=0.1827 Rmel=0.1774 Stop=0.0001\n",
            "[Epoch 1] Batch 360/405 Loss=0.3816 Mel=0.1953 Rmel=0.1861 Stop=0.0001\n",
            "[Epoch 1] Batch 370/405 Loss=0.4146 Mel=0.2109 Rmel=0.2036 Stop=0.0001\n",
            "[Epoch 1] Batch 380/405 Loss=0.3511 Mel=0.1799 Rmel=0.1711 Stop=0.0001\n",
            "[Epoch 1] Batch 390/405 Loss=0.3415 Mel=0.1744 Rmel=0.1670 Stop=0.0001\n",
            "[Epoch 1] Batch 400/405 Loss=0.3694 Mel=0.1873 Rmel=0.1819 Stop=0.0001\n",
            "Validation Loss: 0.3078\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_1\n",
            "\n",
            "Epoch 2\n",
            "[Epoch 2] Batch 0/405 Loss=0.3348 Mel=0.1712 Rmel=0.1634 Stop=0.0001\n",
            "[Epoch 2] Batch 10/405 Loss=0.3783 Mel=0.1944 Rmel=0.1838 Stop=0.0001\n",
            "[Epoch 2] Batch 20/405 Loss=0.3768 Mel=0.1918 Rmel=0.1849 Stop=0.0001\n",
            "[Epoch 2] Batch 30/405 Loss=0.3439 Mel=0.1757 Rmel=0.1681 Stop=0.0001\n",
            "[Epoch 2] Batch 40/405 Loss=0.3386 Mel=0.1740 Rmel=0.1645 Stop=0.0001\n",
            "[Epoch 2] Batch 50/405 Loss=0.3991 Mel=0.2027 Rmel=0.1962 Stop=0.0001\n",
            "[Epoch 2] Batch 60/405 Loss=0.3427 Mel=0.1758 Rmel=0.1668 Stop=0.0001\n",
            "[Epoch 2] Batch 70/405 Loss=0.3550 Mel=0.1807 Rmel=0.1742 Stop=0.0001\n",
            "[Epoch 2] Batch 80/405 Loss=0.3458 Mel=0.1780 Rmel=0.1677 Stop=0.0001\n",
            "[Epoch 2] Batch 90/405 Loss=0.3567 Mel=0.1835 Rmel=0.1731 Stop=0.0001\n",
            "[Epoch 2] Batch 100/405 Loss=0.3466 Mel=0.1787 Rmel=0.1678 Stop=0.0001\n",
            "[Epoch 2] Batch 110/405 Loss=0.3339 Mel=0.1716 Rmel=0.1622 Stop=0.0001\n",
            "[Epoch 2] Batch 120/405 Loss=0.3633 Mel=0.1865 Rmel=0.1766 Stop=0.0001\n",
            "[Epoch 2] Batch 130/405 Loss=0.3666 Mel=0.1859 Rmel=0.1806 Stop=0.0001\n",
            "[Epoch 2] Batch 140/405 Loss=0.3511 Mel=0.1796 Rmel=0.1714 Stop=0.0001\n",
            "[Epoch 2] Batch 150/405 Loss=0.3398 Mel=0.1740 Rmel=0.1657 Stop=0.0001\n",
            "[Epoch 2] Batch 160/405 Loss=0.3404 Mel=0.1754 Rmel=0.1649 Stop=0.0001\n",
            "[Epoch 2] Batch 170/405 Loss=0.3020 Mel=0.1560 Rmel=0.1460 Stop=0.0001\n",
            "[Epoch 2] Batch 180/405 Loss=0.3418 Mel=0.1766 Rmel=0.1651 Stop=0.0001\n",
            "[Epoch 2] Batch 190/405 Loss=0.3245 Mel=0.1677 Rmel=0.1568 Stop=0.0001\n",
            "[Epoch 2] Batch 200/405 Loss=0.3198 Mel=0.1649 Rmel=0.1548 Stop=0.0001\n",
            "[Epoch 2] Batch 210/405 Loss=0.3338 Mel=0.1724 Rmel=0.1613 Stop=0.0001\n",
            "[Epoch 2] Batch 220/405 Loss=0.3410 Mel=0.1757 Rmel=0.1652 Stop=0.0001\n",
            "[Epoch 2] Batch 230/405 Loss=0.3424 Mel=0.1769 Rmel=0.1654 Stop=0.0001\n",
            "[Epoch 2] Batch 240/405 Loss=0.3150 Mel=0.1613 Rmel=0.1535 Stop=0.0001\n",
            "[Epoch 2] Batch 250/405 Loss=0.3150 Mel=0.1610 Rmel=0.1539 Stop=0.0001\n",
            "[Epoch 2] Batch 260/405 Loss=0.3370 Mel=0.1740 Rmel=0.1629 Stop=0.0001\n",
            "[Epoch 2] Batch 270/405 Loss=0.3247 Mel=0.1679 Rmel=0.1567 Stop=0.0001\n",
            "[Epoch 2] Batch 280/405 Loss=0.3090 Mel=0.1589 Rmel=0.1500 Stop=0.0001\n",
            "[Epoch 2] Batch 290/405 Loss=0.3634 Mel=0.1855 Rmel=0.1778 Stop=0.0001\n",
            "[Epoch 2] Batch 300/405 Loss=0.3479 Mel=0.1804 Rmel=0.1675 Stop=0.0001\n",
            "[Epoch 2] Batch 310/405 Loss=0.3325 Mel=0.1726 Rmel=0.1598 Stop=0.0001\n",
            "[Epoch 2] Batch 320/405 Loss=0.3498 Mel=0.1801 Rmel=0.1696 Stop=0.0001\n",
            "[Epoch 2] Batch 330/405 Loss=0.3057 Mel=0.1584 Rmel=0.1472 Stop=0.0001\n",
            "[Epoch 2] Batch 340/405 Loss=0.3088 Mel=0.1601 Rmel=0.1486 Stop=0.0001\n",
            "[Epoch 2] Batch 350/405 Loss=0.3173 Mel=0.1646 Rmel=0.1526 Stop=0.0001\n",
            "[Epoch 2] Batch 360/405 Loss=0.3117 Mel=0.1616 Rmel=0.1500 Stop=0.0001\n",
            "[Epoch 2] Batch 370/405 Loss=0.3082 Mel=0.1599 Rmel=0.1482 Stop=0.0001\n",
            "[Epoch 2] Batch 380/405 Loss=0.3277 Mel=0.1703 Rmel=0.1573 Stop=0.0001\n",
            "[Epoch 2] Batch 390/405 Loss=0.3288 Mel=0.1707 Rmel=0.1581 Stop=0.0001\n",
            "[Epoch 2] Batch 400/405 Loss=0.2921 Mel=0.1518 Rmel=0.1401 Stop=0.0001\n",
            "Validation Loss: 0.2764\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_2\n",
            "\n",
            "Epoch 3\n",
            "[Epoch 3] Batch 0/405 Loss=0.3618 Mel=0.1848 Rmel=0.1769 Stop=0.0001\n",
            "[Epoch 3] Batch 10/405 Loss=0.3111 Mel=0.1617 Rmel=0.1494 Stop=0.0001\n",
            "[Epoch 3] Batch 20/405 Loss=0.3126 Mel=0.1607 Rmel=0.1518 Stop=0.0001\n",
            "[Epoch 3] Batch 30/405 Loss=0.3246 Mel=0.1684 Rmel=0.1562 Stop=0.0001\n",
            "[Epoch 3] Batch 40/405 Loss=0.3245 Mel=0.1690 Rmel=0.1555 Stop=0.0001\n",
            "[Epoch 3] Batch 50/405 Loss=0.2898 Mel=0.1491 Rmel=0.1406 Stop=0.0001\n",
            "[Epoch 3] Batch 60/405 Loss=0.3060 Mel=0.1589 Rmel=0.1470 Stop=0.0001\n",
            "[Epoch 3] Batch 70/405 Loss=0.2951 Mel=0.1533 Rmel=0.1417 Stop=0.0001\n",
            "[Epoch 3] Batch 80/405 Loss=0.3415 Mel=0.1758 Rmel=0.1657 Stop=0.0001\n",
            "[Epoch 3] Batch 90/405 Loss=0.3347 Mel=0.1739 Rmel=0.1607 Stop=0.0001\n",
            "[Epoch 3] Batch 100/405 Loss=0.3302 Mel=0.1685 Rmel=0.1616 Stop=0.0001\n",
            "[Epoch 3] Batch 110/405 Loss=0.3263 Mel=0.1692 Rmel=0.1571 Stop=0.0001\n",
            "[Epoch 3] Batch 120/405 Loss=0.3058 Mel=0.1589 Rmel=0.1468 Stop=0.0001\n",
            "[Epoch 3] Batch 130/405 Loss=0.3138 Mel=0.1636 Rmel=0.1501 Stop=0.0001\n",
            "[Epoch 3] Batch 140/405 Loss=0.3018 Mel=0.1569 Rmel=0.1448 Stop=0.0001\n",
            "[Epoch 3] Batch 150/405 Loss=0.3488 Mel=0.1818 Rmel=0.1669 Stop=0.0001\n",
            "[Epoch 3] Batch 160/405 Loss=0.3381 Mel=0.1762 Rmel=0.1617 Stop=0.0001\n",
            "[Epoch 3] Batch 170/405 Loss=0.3052 Mel=0.1585 Rmel=0.1466 Stop=0.0001\n",
            "[Epoch 3] Batch 180/405 Loss=0.3396 Mel=0.1746 Rmel=0.1649 Stop=0.0001\n",
            "[Epoch 3] Batch 190/405 Loss=0.3199 Mel=0.1664 Rmel=0.1535 Stop=0.0001\n",
            "[Epoch 3] Batch 200/405 Loss=0.2991 Mel=0.1512 Rmel=0.1478 Stop=0.0001\n",
            "[Epoch 3] Batch 210/405 Loss=0.3323 Mel=0.1709 Rmel=0.1613 Stop=0.0001\n",
            "[Epoch 3] Batch 220/405 Loss=0.3239 Mel=0.1688 Rmel=0.1550 Stop=0.0001\n",
            "[Epoch 3] Batch 230/405 Loss=0.3047 Mel=0.1587 Rmel=0.1459 Stop=0.0001\n",
            "[Epoch 3] Batch 240/405 Loss=0.2932 Mel=0.1532 Rmel=0.1400 Stop=0.0001\n",
            "[Epoch 3] Batch 250/405 Loss=0.3037 Mel=0.1582 Rmel=0.1454 Stop=0.0001\n",
            "[Epoch 3] Batch 260/405 Loss=0.2992 Mel=0.1561 Rmel=0.1431 Stop=0.0001\n",
            "[Epoch 3] Batch 270/405 Loss=0.3054 Mel=0.1594 Rmel=0.1460 Stop=0.0001\n",
            "[Epoch 3] Batch 280/405 Loss=0.3090 Mel=0.1608 Rmel=0.1481 Stop=0.0001\n",
            "[Epoch 3] Batch 290/405 Loss=0.2913 Mel=0.1517 Rmel=0.1395 Stop=0.0001\n",
            "[Epoch 3] Batch 300/405 Loss=0.3186 Mel=0.1661 Rmel=0.1524 Stop=0.0001\n",
            "[Epoch 3] Batch 310/405 Loss=0.3130 Mel=0.1634 Rmel=0.1495 Stop=0.0001\n",
            "[Epoch 3] Batch 320/405 Loss=0.2825 Mel=0.1473 Rmel=0.1351 Stop=0.0001\n",
            "[Epoch 3] Batch 330/405 Loss=0.3069 Mel=0.1599 Rmel=0.1469 Stop=0.0001\n",
            "[Epoch 3] Batch 340/405 Loss=0.2822 Mel=0.1472 Rmel=0.1349 Stop=0.0001\n",
            "[Epoch 3] Batch 350/405 Loss=0.3083 Mel=0.1609 Rmel=0.1473 Stop=0.0001\n",
            "[Epoch 3] Batch 360/405 Loss=0.2932 Mel=0.1533 Rmel=0.1398 Stop=0.0001\n",
            "[Epoch 3] Batch 370/405 Loss=0.3240 Mel=0.1695 Rmel=0.1545 Stop=0.0001\n",
            "[Epoch 3] Batch 380/405 Loss=0.2823 Mel=0.1477 Rmel=0.1346 Stop=0.0001\n",
            "[Epoch 3] Batch 390/405 Loss=0.2976 Mel=0.1553 Rmel=0.1422 Stop=0.0001\n",
            "[Epoch 3] Batch 400/405 Loss=0.3139 Mel=0.1643 Rmel=0.1495 Stop=0.0001\n",
            "Validation Loss: 0.2574\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_3\n",
            "\n",
            "Epoch 4\n",
            "[Epoch 4] Batch 0/405 Loss=0.3323 Mel=0.1737 Rmel=0.1585 Stop=0.0001\n",
            "[Epoch 4] Batch 10/405 Loss=0.3418 Mel=0.1787 Rmel=0.1631 Stop=0.0001\n",
            "[Epoch 4] Batch 20/405 Loss=0.3133 Mel=0.1641 Rmel=0.1491 Stop=0.0001\n",
            "[Epoch 4] Batch 30/405 Loss=0.3085 Mel=0.1607 Rmel=0.1477 Stop=0.0001\n",
            "[Epoch 4] Batch 40/405 Loss=0.2589 Mel=0.1356 Rmel=0.1232 Stop=0.0000\n",
            "[Epoch 4] Batch 50/405 Loss=0.2887 Mel=0.1513 Rmel=0.1373 Stop=0.0001\n",
            "[Epoch 4] Batch 60/405 Loss=0.2872 Mel=0.1503 Rmel=0.1368 Stop=0.0001\n",
            "[Epoch 4] Batch 70/405 Loss=0.3335 Mel=0.1736 Rmel=0.1599 Stop=0.0001\n",
            "[Epoch 4] Batch 80/405 Loss=0.3282 Mel=0.1688 Rmel=0.1594 Stop=0.0001\n",
            "[Epoch 4] Batch 90/405 Loss=0.2821 Mel=0.1473 Rmel=0.1347 Stop=0.0001\n",
            "[Epoch 4] Batch 100/405 Loss=0.2970 Mel=0.1540 Rmel=0.1429 Stop=0.0001\n",
            "[Epoch 4] Batch 110/405 Loss=0.3042 Mel=0.1586 Rmel=0.1455 Stop=0.0001\n",
            "[Epoch 4] Batch 120/405 Loss=0.3110 Mel=0.1629 Rmel=0.1481 Stop=0.0001\n",
            "[Epoch 4] Batch 130/405 Loss=0.3083 Mel=0.1613 Rmel=0.1470 Stop=0.0001\n",
            "[Epoch 4] Batch 140/405 Loss=0.2866 Mel=0.1503 Rmel=0.1363 Stop=0.0001\n",
            "[Epoch 4] Batch 150/405 Loss=0.2996 Mel=0.1571 Rmel=0.1424 Stop=0.0001\n",
            "[Epoch 4] Batch 160/405 Loss=0.2820 Mel=0.1477 Rmel=0.1342 Stop=0.0001\n",
            "[Epoch 4] Batch 170/405 Loss=0.2974 Mel=0.1557 Rmel=0.1416 Stop=0.0001\n",
            "[Epoch 4] Batch 180/405 Loss=0.3057 Mel=0.1596 Rmel=0.1460 Stop=0.0001\n",
            "[Epoch 4] Batch 190/405 Loss=0.3248 Mel=0.1651 Rmel=0.1597 Stop=0.0001\n",
            "[Epoch 4] Batch 200/405 Loss=0.2723 Mel=0.1426 Rmel=0.1296 Stop=0.0001\n",
            "[Epoch 4] Batch 210/405 Loss=0.2834 Mel=0.1480 Rmel=0.1353 Stop=0.0001\n",
            "[Epoch 4] Batch 220/405 Loss=0.2514 Mel=0.1318 Rmel=0.1195 Stop=0.0001\n",
            "[Epoch 4] Batch 230/405 Loss=0.3375 Mel=0.1752 Rmel=0.1622 Stop=0.0001\n",
            "[Epoch 4] Batch 240/405 Loss=0.2866 Mel=0.1500 Rmel=0.1366 Stop=0.0001\n",
            "[Epoch 4] Batch 250/405 Loss=0.2490 Mel=0.1307 Rmel=0.1182 Stop=0.0001\n",
            "[Epoch 4] Batch 260/405 Loss=0.2933 Mel=0.1536 Rmel=0.1397 Stop=0.0001\n",
            "[Epoch 4] Batch 270/405 Loss=0.3327 Mel=0.1731 Rmel=0.1596 Stop=0.0001\n",
            "[Epoch 4] Batch 280/405 Loss=0.2659 Mel=0.1393 Rmel=0.1265 Stop=0.0001\n",
            "[Epoch 4] Batch 290/405 Loss=0.2891 Mel=0.1518 Rmel=0.1372 Stop=0.0001\n",
            "[Epoch 4] Batch 300/405 Loss=0.2592 Mel=0.1361 Rmel=0.1231 Stop=0.0000\n",
            "[Epoch 4] Batch 310/405 Loss=0.2911 Mel=0.1511 Rmel=0.1400 Stop=0.0001\n",
            "[Epoch 4] Batch 320/405 Loss=0.2907 Mel=0.1527 Rmel=0.1380 Stop=0.0001\n",
            "[Epoch 4] Batch 330/405 Loss=0.2781 Mel=0.1464 Rmel=0.1316 Stop=0.0001\n",
            "[Epoch 4] Batch 340/405 Loss=0.3039 Mel=0.1592 Rmel=0.1446 Stop=0.0001\n",
            "[Epoch 4] Batch 350/405 Loss=0.3102 Mel=0.1618 Rmel=0.1484 Stop=0.0001\n",
            "[Epoch 4] Batch 360/405 Loss=0.2927 Mel=0.1536 Rmel=0.1391 Stop=0.0001\n",
            "[Epoch 4] Batch 370/405 Loss=0.2831 Mel=0.1489 Rmel=0.1342 Stop=0.0001\n",
            "[Epoch 4] Batch 380/405 Loss=0.3400 Mel=0.1777 Rmel=0.1622 Stop=0.0001\n",
            "[Epoch 4] Batch 390/405 Loss=0.2969 Mel=0.1565 Rmel=0.1404 Stop=0.0001\n",
            "[Epoch 4] Batch 400/405 Loss=0.2871 Mel=0.1508 Rmel=0.1362 Stop=0.0001\n",
            "Validation Loss: 0.2448\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_4\n",
            "\n",
            "Epoch 5\n",
            "[Epoch 5] Batch 0/405 Loss=0.2661 Mel=0.1394 Rmel=0.1266 Stop=0.0000\n",
            "[Epoch 5] Batch 10/405 Loss=0.2871 Mel=0.1503 Rmel=0.1368 Stop=0.0001\n",
            "[Epoch 5] Batch 20/405 Loss=0.2854 Mel=0.1490 Rmel=0.1364 Stop=0.0001\n",
            "[Epoch 5] Batch 30/405 Loss=0.2854 Mel=0.1501 Rmel=0.1352 Stop=0.0000\n",
            "[Epoch 5] Batch 40/405 Loss=0.2790 Mel=0.1466 Rmel=0.1324 Stop=0.0001\n",
            "[Epoch 5] Batch 50/405 Loss=0.3019 Mel=0.1586 Rmel=0.1432 Stop=0.0001\n",
            "[Epoch 5] Batch 60/405 Loss=0.2761 Mel=0.1452 Rmel=0.1308 Stop=0.0000\n",
            "[Epoch 5] Batch 70/405 Loss=0.2491 Mel=0.1313 Rmel=0.1178 Stop=0.0000\n",
            "[Epoch 5] Batch 80/405 Loss=0.2685 Mel=0.1406 Rmel=0.1278 Stop=0.0000\n",
            "[Epoch 5] Batch 90/405 Loss=0.2819 Mel=0.1483 Rmel=0.1335 Stop=0.0000\n",
            "[Epoch 5] Batch 100/405 Loss=0.2832 Mel=0.1489 Rmel=0.1343 Stop=0.0000\n",
            "[Epoch 5] Batch 110/405 Loss=0.2827 Mel=0.1492 Rmel=0.1335 Stop=0.0000\n",
            "[Epoch 5] Batch 120/405 Loss=0.2766 Mel=0.1458 Rmel=0.1308 Stop=0.0000\n",
            "[Epoch 5] Batch 130/405 Loss=0.2809 Mel=0.1480 Rmel=0.1329 Stop=0.0001\n",
            "[Epoch 5] Batch 140/405 Loss=0.3022 Mel=0.1582 Rmel=0.1440 Stop=0.0000\n",
            "[Epoch 5] Batch 150/405 Loss=0.2598 Mel=0.1368 Rmel=0.1229 Stop=0.0000\n",
            "[Epoch 5] Batch 160/405 Loss=0.3232 Mel=0.1660 Rmel=0.1572 Stop=0.0001\n",
            "[Epoch 5] Batch 170/405 Loss=0.2865 Mel=0.1510 Rmel=0.1355 Stop=0.0000\n",
            "[Epoch 5] Batch 180/405 Loss=0.2888 Mel=0.1512 Rmel=0.1376 Stop=0.0000\n",
            "[Epoch 5] Batch 190/405 Loss=0.2641 Mel=0.1375 Rmel=0.1266 Stop=0.0000\n",
            "[Epoch 5] Batch 200/405 Loss=0.2519 Mel=0.1328 Rmel=0.1190 Stop=0.0000\n",
            "[Epoch 5] Batch 210/405 Loss=0.2769 Mel=0.1457 Rmel=0.1311 Stop=0.0000\n",
            "[Epoch 5] Batch 220/405 Loss=0.2642 Mel=0.1370 Rmel=0.1271 Stop=0.0000\n",
            "[Epoch 5] Batch 230/405 Loss=0.2710 Mel=0.1429 Rmel=0.1281 Stop=0.0000\n",
            "[Epoch 5] Batch 240/405 Loss=0.2882 Mel=0.1511 Rmel=0.1370 Stop=0.0000\n",
            "[Epoch 5] Batch 250/405 Loss=0.2882 Mel=0.1505 Rmel=0.1377 Stop=0.0000\n",
            "[Epoch 5] Batch 260/405 Loss=0.2842 Mel=0.1491 Rmel=0.1351 Stop=0.0001\n",
            "[Epoch 5] Batch 270/405 Loss=0.2621 Mel=0.1381 Rmel=0.1240 Stop=0.0000\n",
            "[Epoch 5] Batch 280/405 Loss=0.2794 Mel=0.1470 Rmel=0.1323 Stop=0.0000\n",
            "[Epoch 5] Batch 290/405 Loss=0.3059 Mel=0.1597 Rmel=0.1462 Stop=0.0001\n",
            "[Epoch 5] Batch 300/405 Loss=0.2710 Mel=0.1421 Rmel=0.1289 Stop=0.0000\n",
            "[Epoch 5] Batch 310/405 Loss=0.2663 Mel=0.1400 Rmel=0.1262 Stop=0.0000\n",
            "[Epoch 5] Batch 320/405 Loss=0.2756 Mel=0.1456 Rmel=0.1300 Stop=0.0000\n",
            "[Epoch 5] Batch 330/405 Loss=0.2875 Mel=0.1518 Rmel=0.1357 Stop=0.0000\n",
            "[Epoch 5] Batch 340/405 Loss=0.3115 Mel=0.1639 Rmel=0.1476 Stop=0.0001\n",
            "[Epoch 5] Batch 350/405 Loss=0.2638 Mel=0.1389 Rmel=0.1249 Stop=0.0000\n",
            "[Epoch 5] Batch 360/405 Loss=0.2801 Mel=0.1479 Rmel=0.1322 Stop=0.0000\n",
            "[Epoch 5] Batch 370/405 Loss=0.2628 Mel=0.1388 Rmel=0.1240 Stop=0.0000\n",
            "[Epoch 5] Batch 380/405 Loss=0.2509 Mel=0.1323 Rmel=0.1185 Stop=0.0000\n",
            "[Epoch 5] Batch 390/405 Loss=0.2953 Mel=0.1559 Rmel=0.1393 Stop=0.0000\n",
            "[Epoch 5] Batch 400/405 Loss=0.2874 Mel=0.1519 Rmel=0.1354 Stop=0.0000\n",
            "Validation Loss: 0.2321\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_5\n",
            "\n",
            "Epoch 6\n",
            "[Epoch 6] Batch 0/405 Loss=0.2725 Mel=0.1436 Rmel=0.1289 Stop=0.0000\n",
            "[Epoch 6] Batch 10/405 Loss=0.2880 Mel=0.1522 Rmel=0.1357 Stop=0.0000\n",
            "[Epoch 6] Batch 20/405 Loss=0.2708 Mel=0.1430 Rmel=0.1278 Stop=0.0000\n",
            "[Epoch 6] Batch 30/405 Loss=0.2462 Mel=0.1298 Rmel=0.1164 Stop=0.0000\n",
            "[Epoch 6] Batch 40/405 Loss=0.2705 Mel=0.1427 Rmel=0.1278 Stop=0.0000\n",
            "[Epoch 6] Batch 50/405 Loss=0.2981 Mel=0.1571 Rmel=0.1409 Stop=0.0000\n",
            "[Epoch 6] Batch 60/405 Loss=0.2579 Mel=0.1352 Rmel=0.1226 Stop=0.0000\n",
            "[Epoch 6] Batch 70/405 Loss=0.2670 Mel=0.1408 Rmel=0.1262 Stop=0.0000\n",
            "[Epoch 6] Batch 80/405 Loss=0.2443 Mel=0.1293 Rmel=0.1150 Stop=0.0000\n",
            "[Epoch 6] Batch 90/405 Loss=0.2806 Mel=0.1482 Rmel=0.1324 Stop=0.0000\n",
            "[Epoch 6] Batch 100/405 Loss=0.2541 Mel=0.1338 Rmel=0.1202 Stop=0.0000\n",
            "[Epoch 6] Batch 110/405 Loss=0.2729 Mel=0.1440 Rmel=0.1289 Stop=0.0000\n",
            "[Epoch 6] Batch 120/405 Loss=0.2764 Mel=0.1462 Rmel=0.1301 Stop=0.0000\n",
            "[Epoch 6] Batch 130/405 Loss=0.2680 Mel=0.1417 Rmel=0.1263 Stop=0.0000\n",
            "[Epoch 6] Batch 140/405 Loss=0.2535 Mel=0.1341 Rmel=0.1194 Stop=0.0000\n",
            "[Epoch 6] Batch 150/405 Loss=0.2595 Mel=0.1369 Rmel=0.1226 Stop=0.0000\n",
            "[Epoch 6] Batch 160/405 Loss=0.2664 Mel=0.1408 Rmel=0.1256 Stop=0.0000\n",
            "[Epoch 6] Batch 170/405 Loss=0.2687 Mel=0.1422 Rmel=0.1265 Stop=0.0000\n",
            "[Epoch 6] Batch 180/405 Loss=0.2839 Mel=0.1501 Rmel=0.1337 Stop=0.0000\n",
            "[Epoch 6] Batch 190/405 Loss=0.2764 Mel=0.1462 Rmel=0.1302 Stop=0.0000\n",
            "[Epoch 6] Batch 200/405 Loss=0.2930 Mel=0.1549 Rmel=0.1381 Stop=0.0000\n",
            "[Epoch 6] Batch 210/405 Loss=0.2975 Mel=0.1554 Rmel=0.1420 Stop=0.0000\n",
            "[Epoch 6] Batch 220/405 Loss=0.2974 Mel=0.1570 Rmel=0.1404 Stop=0.0000\n",
            "[Epoch 6] Batch 230/405 Loss=0.2724 Mel=0.1420 Rmel=0.1304 Stop=0.0000\n",
            "[Epoch 6] Batch 240/405 Loss=0.2629 Mel=0.1386 Rmel=0.1243 Stop=0.0000\n",
            "[Epoch 6] Batch 250/405 Loss=0.2609 Mel=0.1384 Rmel=0.1224 Stop=0.0000\n",
            "[Epoch 6] Batch 260/405 Loss=0.2776 Mel=0.1462 Rmel=0.1314 Stop=0.0000\n",
            "[Epoch 6] Batch 270/405 Loss=0.2788 Mel=0.1481 Rmel=0.1307 Stop=0.0000\n",
            "[Epoch 6] Batch 280/405 Loss=0.2796 Mel=0.1481 Rmel=0.1315 Stop=0.0000\n",
            "[Epoch 6] Batch 290/405 Loss=0.2523 Mel=0.1338 Rmel=0.1184 Stop=0.0000\n",
            "[Epoch 6] Batch 300/405 Loss=0.2454 Mel=0.1299 Rmel=0.1155 Stop=0.0000\n",
            "[Epoch 6] Batch 310/405 Loss=0.2810 Mel=0.1482 Rmel=0.1328 Stop=0.0000\n",
            "[Epoch 6] Batch 320/405 Loss=0.2852 Mel=0.1506 Rmel=0.1346 Stop=0.0000\n",
            "[Epoch 6] Batch 330/405 Loss=0.2886 Mel=0.1528 Rmel=0.1357 Stop=0.0000\n",
            "[Epoch 6] Batch 340/405 Loss=0.2330 Mel=0.1225 Rmel=0.1105 Stop=0.0000\n",
            "[Epoch 6] Batch 350/405 Loss=0.2618 Mel=0.1380 Rmel=0.1238 Stop=0.0000\n",
            "[Epoch 6] Batch 360/405 Loss=0.2361 Mel=0.1247 Rmel=0.1114 Stop=0.0000\n",
            "[Epoch 6] Batch 370/405 Loss=0.2728 Mel=0.1445 Rmel=0.1283 Stop=0.0000\n",
            "[Epoch 6] Batch 380/405 Loss=0.2494 Mel=0.1316 Rmel=0.1178 Stop=0.0000\n",
            "[Epoch 6] Batch 390/405 Loss=0.2570 Mel=0.1362 Rmel=0.1208 Stop=0.0000\n",
            "[Epoch 6] Batch 400/405 Loss=0.2556 Mel=0.1349 Rmel=0.1206 Stop=0.0000\n",
            "Validation Loss: 0.2234\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_6\n",
            "\n",
            "Epoch 7\n",
            "[Epoch 7] Batch 0/405 Loss=0.2752 Mel=0.1456 Rmel=0.1296 Stop=0.0000\n",
            "[Epoch 7] Batch 10/405 Loss=0.2597 Mel=0.1380 Rmel=0.1218 Stop=0.0000\n",
            "[Epoch 7] Batch 20/405 Loss=0.2823 Mel=0.1467 Rmel=0.1355 Stop=0.0000\n",
            "[Epoch 7] Batch 30/405 Loss=0.2675 Mel=0.1415 Rmel=0.1260 Stop=0.0000\n",
            "[Epoch 7] Batch 40/405 Loss=0.2550 Mel=0.1349 Rmel=0.1200 Stop=0.0000\n",
            "[Epoch 7] Batch 50/405 Loss=0.2497 Mel=0.1325 Rmel=0.1171 Stop=0.0000\n",
            "[Epoch 7] Batch 60/405 Loss=0.2828 Mel=0.1499 Rmel=0.1329 Stop=0.0000\n",
            "[Epoch 7] Batch 70/405 Loss=0.2881 Mel=0.1528 Rmel=0.1352 Stop=0.0000\n",
            "[Epoch 7] Batch 80/405 Loss=0.2572 Mel=0.1356 Rmel=0.1215 Stop=0.0000\n",
            "[Epoch 7] Batch 90/405 Loss=0.2739 Mel=0.1452 Rmel=0.1287 Stop=0.0000\n",
            "[Epoch 7] Batch 100/405 Loss=0.2482 Mel=0.1315 Rmel=0.1166 Stop=0.0000\n",
            "[Epoch 7] Batch 110/405 Loss=0.2609 Mel=0.1383 Rmel=0.1226 Stop=0.0000\n",
            "[Epoch 7] Batch 120/405 Loss=0.2656 Mel=0.1404 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 7] Batch 130/405 Loss=0.2452 Mel=0.1297 Rmel=0.1155 Stop=0.0000\n",
            "[Epoch 7] Batch 140/405 Loss=0.2666 Mel=0.1412 Rmel=0.1254 Stop=0.0000\n",
            "[Epoch 7] Batch 150/405 Loss=0.2494 Mel=0.1322 Rmel=0.1171 Stop=0.0000\n",
            "[Epoch 7] Batch 160/405 Loss=0.2396 Mel=0.1264 Rmel=0.1132 Stop=0.0000\n",
            "[Epoch 7] Batch 170/405 Loss=0.2632 Mel=0.1395 Rmel=0.1237 Stop=0.0000\n",
            "[Epoch 7] Batch 180/405 Loss=0.2700 Mel=0.1426 Rmel=0.1273 Stop=0.0000\n",
            "[Epoch 7] Batch 190/405 Loss=0.2720 Mel=0.1444 Rmel=0.1276 Stop=0.0000\n",
            "[Epoch 7] Batch 200/405 Loss=0.2750 Mel=0.1460 Rmel=0.1290 Stop=0.0000\n",
            "[Epoch 7] Batch 210/405 Loss=0.2930 Mel=0.1553 Rmel=0.1376 Stop=0.0000\n",
            "[Epoch 7] Batch 220/405 Loss=0.2626 Mel=0.1391 Rmel=0.1234 Stop=0.0000\n",
            "[Epoch 7] Batch 230/405 Loss=0.2546 Mel=0.1349 Rmel=0.1197 Stop=0.0000\n",
            "[Epoch 7] Batch 240/405 Loss=0.2540 Mel=0.1353 Rmel=0.1187 Stop=0.0000\n",
            "[Epoch 7] Batch 250/405 Loss=0.2663 Mel=0.1412 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 7] Batch 260/405 Loss=0.2755 Mel=0.1465 Rmel=0.1290 Stop=0.0000\n",
            "[Epoch 7] Batch 270/405 Loss=0.2498 Mel=0.1323 Rmel=0.1175 Stop=0.0000\n",
            "[Epoch 7] Batch 280/405 Loss=0.2651 Mel=0.1407 Rmel=0.1244 Stop=0.0000\n",
            "[Epoch 7] Batch 290/405 Loss=0.2788 Mel=0.1482 Rmel=0.1306 Stop=0.0000\n",
            "[Epoch 7] Batch 300/405 Loss=0.2520 Mel=0.1336 Rmel=0.1183 Stop=0.0000\n",
            "[Epoch 7] Batch 310/405 Loss=0.2662 Mel=0.1415 Rmel=0.1247 Stop=0.0000\n",
            "[Epoch 7] Batch 320/405 Loss=0.2452 Mel=0.1290 Rmel=0.1162 Stop=0.0000\n",
            "[Epoch 7] Batch 330/405 Loss=0.2714 Mel=0.1440 Rmel=0.1274 Stop=0.0000\n",
            "[Epoch 7] Batch 340/405 Loss=0.2511 Mel=0.1334 Rmel=0.1177 Stop=0.0000\n",
            "[Epoch 7] Batch 350/405 Loss=0.2581 Mel=0.1365 Rmel=0.1216 Stop=0.0000\n",
            "[Epoch 7] Batch 360/405 Loss=0.2501 Mel=0.1325 Rmel=0.1175 Stop=0.0000\n",
            "[Epoch 7] Batch 370/405 Loss=0.2648 Mel=0.1405 Rmel=0.1243 Stop=0.0000\n",
            "[Epoch 7] Batch 380/405 Loss=0.2553 Mel=0.1356 Rmel=0.1196 Stop=0.0000\n",
            "[Epoch 7] Batch 390/405 Loss=0.2811 Mel=0.1490 Rmel=0.1320 Stop=0.0000\n",
            "[Epoch 7] Batch 400/405 Loss=0.2493 Mel=0.1326 Rmel=0.1166 Stop=0.0000\n",
            "Validation Loss: 0.2165\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_7\n",
            "\n",
            "Epoch 8\n",
            "[Epoch 8] Batch 0/405 Loss=0.2655 Mel=0.1409 Rmel=0.1245 Stop=0.0000\n",
            "[Epoch 8] Batch 10/405 Loss=0.2668 Mel=0.1416 Rmel=0.1251 Stop=0.0000\n",
            "[Epoch 8] Batch 20/405 Loss=0.2708 Mel=0.1441 Rmel=0.1267 Stop=0.0000\n",
            "[Epoch 8] Batch 30/405 Loss=0.2646 Mel=0.1408 Rmel=0.1237 Stop=0.0000\n",
            "[Epoch 8] Batch 40/405 Loss=0.2538 Mel=0.1349 Rmel=0.1189 Stop=0.0000\n",
            "[Epoch 8] Batch 50/405 Loss=0.2551 Mel=0.1352 Rmel=0.1199 Stop=0.0000\n",
            "[Epoch 8] Batch 60/405 Loss=0.2651 Mel=0.1409 Rmel=0.1241 Stop=0.0000\n",
            "[Epoch 8] Batch 70/405 Loss=0.2744 Mel=0.1460 Rmel=0.1284 Stop=0.0000\n",
            "[Epoch 8] Batch 80/405 Loss=0.2491 Mel=0.1321 Rmel=0.1170 Stop=0.0000\n",
            "[Epoch 8] Batch 90/405 Loss=0.2744 Mel=0.1459 Rmel=0.1284 Stop=0.0000\n",
            "[Epoch 8] Batch 100/405 Loss=0.2524 Mel=0.1345 Rmel=0.1179 Stop=0.0000\n",
            "[Epoch 8] Batch 110/405 Loss=0.2341 Mel=0.1246 Rmel=0.1094 Stop=0.0000\n",
            "[Epoch 8] Batch 120/405 Loss=0.2736 Mel=0.1451 Rmel=0.1285 Stop=0.0000\n",
            "[Epoch 8] Batch 130/405 Loss=0.2531 Mel=0.1342 Rmel=0.1189 Stop=0.0000\n",
            "[Epoch 8] Batch 140/405 Loss=0.2581 Mel=0.1373 Rmel=0.1208 Stop=0.0000\n",
            "[Epoch 8] Batch 150/405 Loss=0.2497 Mel=0.1326 Rmel=0.1171 Stop=0.0000\n",
            "[Epoch 8] Batch 160/405 Loss=0.2542 Mel=0.1354 Rmel=0.1188 Stop=0.0000\n",
            "[Epoch 8] Batch 170/405 Loss=0.2368 Mel=0.1256 Rmel=0.1112 Stop=0.0000\n",
            "[Epoch 8] Batch 180/405 Loss=0.2368 Mel=0.1259 Rmel=0.1109 Stop=0.0000\n",
            "[Epoch 8] Batch 190/405 Loss=0.2372 Mel=0.1262 Rmel=0.1109 Stop=0.0000\n",
            "[Epoch 8] Batch 200/405 Loss=0.2794 Mel=0.1490 Rmel=0.1303 Stop=0.0000\n",
            "[Epoch 8] Batch 210/405 Loss=0.2282 Mel=0.1213 Rmel=0.1069 Stop=0.0000\n",
            "[Epoch 8] Batch 220/405 Loss=0.2727 Mel=0.1452 Rmel=0.1275 Stop=0.0000\n",
            "[Epoch 8] Batch 230/405 Loss=0.2349 Mel=0.1246 Rmel=0.1103 Stop=0.0000\n",
            "[Epoch 8] Batch 240/405 Loss=0.2722 Mel=0.1443 Rmel=0.1279 Stop=0.0000\n",
            "[Epoch 8] Batch 250/405 Loss=0.2372 Mel=0.1250 Rmel=0.1122 Stop=0.0000\n",
            "[Epoch 8] Batch 260/405 Loss=0.2722 Mel=0.1445 Rmel=0.1276 Stop=0.0000\n",
            "[Epoch 8] Batch 270/405 Loss=0.2295 Mel=0.1222 Rmel=0.1073 Stop=0.0000\n",
            "[Epoch 8] Batch 280/405 Loss=0.2611 Mel=0.1389 Rmel=0.1222 Stop=0.0000\n",
            "[Epoch 8] Batch 290/405 Loss=0.2489 Mel=0.1329 Rmel=0.1159 Stop=0.0000\n",
            "[Epoch 8] Batch 300/405 Loss=0.2555 Mel=0.1350 Rmel=0.1205 Stop=0.0000\n",
            "[Epoch 8] Batch 310/405 Loss=0.2423 Mel=0.1288 Rmel=0.1135 Stop=0.0000\n",
            "[Epoch 8] Batch 320/405 Loss=0.2640 Mel=0.1398 Rmel=0.1241 Stop=0.0000\n",
            "[Epoch 8] Batch 330/405 Loss=0.2516 Mel=0.1340 Rmel=0.1176 Stop=0.0000\n",
            "[Epoch 8] Batch 340/405 Loss=0.2450 Mel=0.1310 Rmel=0.1139 Stop=0.0000\n",
            "[Epoch 8] Batch 350/405 Loss=0.2504 Mel=0.1332 Rmel=0.1172 Stop=0.0000\n",
            "[Epoch 8] Batch 360/405 Loss=0.2452 Mel=0.1309 Rmel=0.1143 Stop=0.0000\n",
            "[Epoch 8] Batch 370/405 Loss=0.2400 Mel=0.1275 Rmel=0.1125 Stop=0.0000\n",
            "[Epoch 8] Batch 380/405 Loss=0.2579 Mel=0.1377 Rmel=0.1202 Stop=0.0000\n",
            "[Epoch 8] Batch 390/405 Loss=0.2703 Mel=0.1442 Rmel=0.1261 Stop=0.0000\n",
            "[Epoch 8] Batch 400/405 Loss=0.2698 Mel=0.1436 Rmel=0.1261 Stop=0.0000\n",
            "Validation Loss: 0.2111\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_8\n",
            "\n",
            "Epoch 9\n",
            "[Epoch 9] Batch 0/405 Loss=0.2539 Mel=0.1354 Rmel=0.1184 Stop=0.0000\n",
            "[Epoch 9] Batch 10/405 Loss=0.2421 Mel=0.1290 Rmel=0.1130 Stop=0.0000\n",
            "[Epoch 9] Batch 20/405 Loss=0.2684 Mel=0.1431 Rmel=0.1253 Stop=0.0000\n",
            "[Epoch 9] Batch 30/405 Loss=0.2490 Mel=0.1332 Rmel=0.1158 Stop=0.0000\n",
            "[Epoch 9] Batch 40/405 Loss=0.2547 Mel=0.1361 Rmel=0.1186 Stop=0.0000\n",
            "[Epoch 9] Batch 50/405 Loss=0.2161 Mel=0.1150 Rmel=0.1011 Stop=0.0000\n",
            "[Epoch 9] Batch 60/405 Loss=0.2385 Mel=0.1272 Rmel=0.1113 Stop=0.0000\n",
            "[Epoch 9] Batch 70/405 Loss=0.2513 Mel=0.1338 Rmel=0.1174 Stop=0.0000\n",
            "[Epoch 9] Batch 80/405 Loss=0.2572 Mel=0.1366 Rmel=0.1206 Stop=0.0000\n",
            "[Epoch 9] Batch 90/405 Loss=0.2502 Mel=0.1334 Rmel=0.1167 Stop=0.0000\n",
            "[Epoch 9] Batch 100/405 Loss=0.2556 Mel=0.1361 Rmel=0.1195 Stop=0.0000\n",
            "[Epoch 9] Batch 110/405 Loss=0.2758 Mel=0.1465 Rmel=0.1293 Stop=0.0000\n",
            "[Epoch 9] Batch 120/405 Loss=0.2655 Mel=0.1414 Rmel=0.1241 Stop=0.0000\n",
            "[Epoch 9] Batch 130/405 Loss=0.2495 Mel=0.1335 Rmel=0.1160 Stop=0.0000\n",
            "[Epoch 9] Batch 140/405 Loss=0.2579 Mel=0.1375 Rmel=0.1203 Stop=0.0000\n",
            "[Epoch 9] Batch 150/405 Loss=0.2787 Mel=0.1483 Rmel=0.1304 Stop=0.0000\n",
            "[Epoch 9] Batch 160/405 Loss=0.2334 Mel=0.1251 Rmel=0.1082 Stop=0.0000\n",
            "[Epoch 9] Batch 170/405 Loss=0.2502 Mel=0.1335 Rmel=0.1167 Stop=0.0000\n",
            "[Epoch 9] Batch 180/405 Loss=0.2312 Mel=0.1235 Rmel=0.1077 Stop=0.0000\n",
            "[Epoch 9] Batch 190/405 Loss=0.2534 Mel=0.1350 Rmel=0.1184 Stop=0.0000\n",
            "[Epoch 9] Batch 200/405 Loss=0.2394 Mel=0.1275 Rmel=0.1119 Stop=0.0000\n",
            "[Epoch 9] Batch 210/405 Loss=0.2558 Mel=0.1364 Rmel=0.1194 Stop=0.0000\n",
            "[Epoch 9] Batch 220/405 Loss=0.2431 Mel=0.1298 Rmel=0.1133 Stop=0.0000\n",
            "[Epoch 9] Batch 230/405 Loss=0.2579 Mel=0.1377 Rmel=0.1202 Stop=0.0000\n",
            "[Epoch 9] Batch 240/405 Loss=0.2491 Mel=0.1325 Rmel=0.1166 Stop=0.0000\n",
            "[Epoch 9] Batch 250/405 Loss=0.2316 Mel=0.1240 Rmel=0.1076 Stop=0.0000\n",
            "[Epoch 9] Batch 260/405 Loss=0.2513 Mel=0.1343 Rmel=0.1171 Stop=0.0000\n",
            "[Epoch 9] Batch 270/405 Loss=0.2317 Mel=0.1242 Rmel=0.1075 Stop=0.0000\n",
            "[Epoch 9] Batch 280/405 Loss=0.2458 Mel=0.1314 Rmel=0.1144 Stop=0.0000\n",
            "[Epoch 9] Batch 290/405 Loss=0.2610 Mel=0.1392 Rmel=0.1217 Stop=0.0000\n",
            "[Epoch 9] Batch 300/405 Loss=0.2513 Mel=0.1340 Rmel=0.1173 Stop=0.0000\n",
            "[Epoch 9] Batch 310/405 Loss=0.2528 Mel=0.1351 Rmel=0.1177 Stop=0.0000\n",
            "[Epoch 9] Batch 320/405 Loss=0.2581 Mel=0.1381 Rmel=0.1200 Stop=0.0000\n",
            "[Epoch 9] Batch 330/405 Loss=0.2458 Mel=0.1312 Rmel=0.1146 Stop=0.0000\n",
            "[Epoch 9] Batch 340/405 Loss=0.2578 Mel=0.1371 Rmel=0.1207 Stop=0.0000\n",
            "[Epoch 9] Batch 350/405 Loss=0.2431 Mel=0.1305 Rmel=0.1126 Stop=0.0000\n",
            "[Epoch 9] Batch 360/405 Loss=0.2495 Mel=0.1333 Rmel=0.1162 Stop=0.0000\n",
            "[Epoch 9] Batch 370/405 Loss=0.2417 Mel=0.1293 Rmel=0.1124 Stop=0.0000\n",
            "[Epoch 9] Batch 380/405 Loss=0.2139 Mel=0.1146 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 9] Batch 390/405 Loss=0.2344 Mel=0.1253 Rmel=0.1090 Stop=0.0000\n",
            "[Epoch 9] Batch 400/405 Loss=0.2613 Mel=0.1397 Rmel=0.1216 Stop=0.0000\n",
            "Validation Loss: 0.2049\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_9\n",
            "\n",
            "Epoch 10\n",
            "[Epoch 10] Batch 0/405 Loss=0.2455 Mel=0.1314 Rmel=0.1141 Stop=0.0000\n",
            "[Epoch 10] Batch 10/405 Loss=0.2240 Mel=0.1200 Rmel=0.1040 Stop=0.0000\n",
            "[Epoch 10] Batch 20/405 Loss=0.2546 Mel=0.1363 Rmel=0.1183 Stop=0.0000\n",
            "[Epoch 10] Batch 30/405 Loss=0.2379 Mel=0.1276 Rmel=0.1103 Stop=0.0000\n",
            "[Epoch 10] Batch 40/405 Loss=0.2432 Mel=0.1303 Rmel=0.1129 Stop=0.0000\n",
            "[Epoch 10] Batch 50/405 Loss=0.2616 Mel=0.1403 Rmel=0.1213 Stop=0.0000\n",
            "[Epoch 10] Batch 60/405 Loss=0.2733 Mel=0.1452 Rmel=0.1280 Stop=0.0000\n",
            "[Epoch 10] Batch 70/405 Loss=0.2417 Mel=0.1284 Rmel=0.1132 Stop=0.0000\n",
            "[Epoch 10] Batch 80/405 Loss=0.2523 Mel=0.1342 Rmel=0.1181 Stop=0.0000\n",
            "[Epoch 10] Batch 90/405 Loss=0.2501 Mel=0.1341 Rmel=0.1161 Stop=0.0000\n",
            "[Epoch 10] Batch 100/405 Loss=0.2189 Mel=0.1173 Rmel=0.1016 Stop=0.0000\n",
            "[Epoch 10] Batch 110/405 Loss=0.2497 Mel=0.1336 Rmel=0.1160 Stop=0.0000\n",
            "[Epoch 10] Batch 120/405 Loss=0.2512 Mel=0.1346 Rmel=0.1166 Stop=0.0000\n",
            "[Epoch 10] Batch 130/405 Loss=0.2204 Mel=0.1181 Rmel=0.1023 Stop=0.0000\n",
            "[Epoch 10] Batch 140/405 Loss=0.2285 Mel=0.1228 Rmel=0.1057 Stop=0.0000\n",
            "[Epoch 10] Batch 150/405 Loss=0.2408 Mel=0.1289 Rmel=0.1118 Stop=0.0000\n",
            "[Epoch 10] Batch 160/405 Loss=0.2554 Mel=0.1367 Rmel=0.1187 Stop=0.0000\n",
            "[Epoch 10] Batch 170/405 Loss=0.2274 Mel=0.1224 Rmel=0.1050 Stop=0.0000\n",
            "[Epoch 10] Batch 180/405 Loss=0.2509 Mel=0.1348 Rmel=0.1161 Stop=0.0000\n",
            "[Epoch 10] Batch 190/405 Loss=0.2244 Mel=0.1202 Rmel=0.1042 Stop=0.0000\n",
            "[Epoch 10] Batch 200/405 Loss=0.2389 Mel=0.1281 Rmel=0.1108 Stop=0.0000\n",
            "[Epoch 10] Batch 210/405 Loss=0.2230 Mel=0.1194 Rmel=0.1036 Stop=0.0000\n",
            "[Epoch 10] Batch 220/405 Loss=0.2523 Mel=0.1356 Rmel=0.1166 Stop=0.0000\n",
            "[Epoch 10] Batch 230/405 Loss=0.2639 Mel=0.1386 Rmel=0.1252 Stop=0.0000\n",
            "[Epoch 10] Batch 240/405 Loss=0.2220 Mel=0.1187 Rmel=0.1032 Stop=0.0000\n",
            "[Epoch 10] Batch 250/405 Loss=0.2207 Mel=0.1185 Rmel=0.1022 Stop=0.0000\n",
            "[Epoch 10] Batch 260/405 Loss=0.2416 Mel=0.1298 Rmel=0.1118 Stop=0.0000\n",
            "[Epoch 10] Batch 270/405 Loss=0.2280 Mel=0.1215 Rmel=0.1065 Stop=0.0000\n",
            "[Epoch 10] Batch 280/405 Loss=0.2207 Mel=0.1167 Rmel=0.1041 Stop=0.0000\n",
            "[Epoch 10] Batch 290/405 Loss=0.2421 Mel=0.1294 Rmel=0.1127 Stop=0.0000\n",
            "[Epoch 10] Batch 300/405 Loss=0.2452 Mel=0.1308 Rmel=0.1144 Stop=0.0000\n",
            "[Epoch 10] Batch 310/405 Loss=0.2221 Mel=0.1188 Rmel=0.1033 Stop=0.0000\n",
            "[Epoch 10] Batch 320/405 Loss=0.2267 Mel=0.1213 Rmel=0.1053 Stop=0.0000\n",
            "[Epoch 10] Batch 330/405 Loss=0.2345 Mel=0.1254 Rmel=0.1090 Stop=0.0000\n",
            "[Epoch 10] Batch 340/405 Loss=0.2330 Mel=0.1250 Rmel=0.1079 Stop=0.0000\n",
            "[Epoch 10] Batch 350/405 Loss=0.2289 Mel=0.1228 Rmel=0.1060 Stop=0.0000\n",
            "[Epoch 10] Batch 360/405 Loss=0.2266 Mel=0.1215 Rmel=0.1051 Stop=0.0000\n",
            "[Epoch 10] Batch 370/405 Loss=0.2298 Mel=0.1229 Rmel=0.1069 Stop=0.0000\n",
            "[Epoch 10] Batch 380/405 Loss=0.2167 Mel=0.1163 Rmel=0.1004 Stop=0.0000\n",
            "[Epoch 10] Batch 390/405 Loss=0.2323 Mel=0.1248 Rmel=0.1075 Stop=0.0000\n",
            "[Epoch 10] Batch 400/405 Loss=0.2015 Mel=0.1080 Rmel=0.0935 Stop=0.0000\n",
            "Validation Loss: 0.2002\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_10\n",
            "\n",
            "Epoch 11\n",
            "[Epoch 11] Batch 0/405 Loss=0.2276 Mel=0.1223 Rmel=0.1053 Stop=0.0000\n",
            "[Epoch 11] Batch 10/405 Loss=0.2414 Mel=0.1297 Rmel=0.1116 Stop=0.0000\n",
            "[Epoch 11] Batch 20/405 Loss=0.2550 Mel=0.1367 Rmel=0.1183 Stop=0.0000\n",
            "[Epoch 11] Batch 30/405 Loss=0.2234 Mel=0.1200 Rmel=0.1034 Stop=0.0000\n",
            "[Epoch 11] Batch 40/405 Loss=0.2303 Mel=0.1235 Rmel=0.1067 Stop=0.0000\n",
            "[Epoch 11] Batch 50/405 Loss=0.2170 Mel=0.1164 Rmel=0.1006 Stop=0.0000\n",
            "[Epoch 11] Batch 60/405 Loss=0.2450 Mel=0.1310 Rmel=0.1140 Stop=0.0000\n",
            "[Epoch 11] Batch 70/405 Loss=0.2224 Mel=0.1185 Rmel=0.1038 Stop=0.0000\n",
            "[Epoch 11] Batch 80/405 Loss=0.2665 Mel=0.1425 Rmel=0.1239 Stop=0.0000\n",
            "[Epoch 11] Batch 90/405 Loss=0.2336 Mel=0.1250 Rmel=0.1085 Stop=0.0000\n",
            "[Epoch 11] Batch 100/405 Loss=0.2556 Mel=0.1374 Rmel=0.1181 Stop=0.0000\n",
            "[Epoch 11] Batch 110/405 Loss=0.2299 Mel=0.1236 Rmel=0.1063 Stop=0.0000\n",
            "[Epoch 11] Batch 120/405 Loss=0.2268 Mel=0.1217 Rmel=0.1051 Stop=0.0000\n",
            "[Epoch 11] Batch 130/405 Loss=0.2410 Mel=0.1299 Rmel=0.1111 Stop=0.0000\n",
            "[Epoch 11] Batch 140/405 Loss=0.2412 Mel=0.1299 Rmel=0.1113 Stop=0.0000\n",
            "[Epoch 11] Batch 150/405 Loss=0.2505 Mel=0.1345 Rmel=0.1159 Stop=0.0000\n",
            "[Epoch 11] Batch 160/405 Loss=0.2186 Mel=0.1176 Rmel=0.1010 Stop=0.0000\n",
            "[Epoch 11] Batch 170/405 Loss=0.2134 Mel=0.1149 Rmel=0.0985 Stop=0.0000\n",
            "[Epoch 11] Batch 180/405 Loss=0.2367 Mel=0.1275 Rmel=0.1092 Stop=0.0000\n",
            "[Epoch 11] Batch 190/405 Loss=0.2286 Mel=0.1230 Rmel=0.1057 Stop=0.0000\n",
            "[Epoch 11] Batch 200/405 Loss=0.2386 Mel=0.1276 Rmel=0.1111 Stop=0.0000\n",
            "[Epoch 11] Batch 210/405 Loss=0.2411 Mel=0.1291 Rmel=0.1120 Stop=0.0000\n",
            "[Epoch 11] Batch 220/405 Loss=0.2443 Mel=0.1309 Rmel=0.1134 Stop=0.0000\n",
            "[Epoch 11] Batch 230/405 Loss=0.2141 Mel=0.1150 Rmel=0.0991 Stop=0.0000\n",
            "[Epoch 11] Batch 240/405 Loss=0.2594 Mel=0.1385 Rmel=0.1209 Stop=0.0000\n",
            "[Epoch 11] Batch 250/405 Loss=0.2509 Mel=0.1351 Rmel=0.1158 Stop=0.0000\n",
            "[Epoch 11] Batch 260/405 Loss=0.2245 Mel=0.1206 Rmel=0.1039 Stop=0.0000\n",
            "[Epoch 11] Batch 270/405 Loss=0.2406 Mel=0.1293 Rmel=0.1113 Stop=0.0000\n",
            "[Epoch 11] Batch 280/405 Loss=0.2199 Mel=0.1182 Rmel=0.1017 Stop=0.0000\n",
            "[Epoch 11] Batch 290/405 Loss=0.2478 Mel=0.1335 Rmel=0.1143 Stop=0.0000\n",
            "[Epoch 11] Batch 300/405 Loss=0.2434 Mel=0.1310 Rmel=0.1124 Stop=0.0000\n",
            "[Epoch 11] Batch 310/405 Loss=0.2380 Mel=0.1273 Rmel=0.1107 Stop=0.0000\n",
            "[Epoch 11] Batch 320/405 Loss=0.2285 Mel=0.1227 Rmel=0.1058 Stop=0.0000\n",
            "[Epoch 11] Batch 330/405 Loss=0.2244 Mel=0.1204 Rmel=0.1041 Stop=0.0000\n",
            "[Epoch 11] Batch 340/405 Loss=0.2317 Mel=0.1248 Rmel=0.1070 Stop=0.0000\n",
            "[Epoch 11] Batch 350/405 Loss=0.2323 Mel=0.1248 Rmel=0.1075 Stop=0.0000\n",
            "[Epoch 11] Batch 360/405 Loss=0.2197 Mel=0.1179 Rmel=0.1017 Stop=0.0000\n",
            "[Epoch 11] Batch 370/405 Loss=0.2176 Mel=0.1173 Rmel=0.1003 Stop=0.0000\n",
            "[Epoch 11] Batch 380/405 Loss=0.2372 Mel=0.1277 Rmel=0.1094 Stop=0.0000\n",
            "[Epoch 11] Batch 390/405 Loss=0.2164 Mel=0.1169 Rmel=0.0996 Stop=0.0000\n",
            "[Epoch 11] Batch 400/405 Loss=0.2450 Mel=0.1319 Rmel=0.1131 Stop=0.0000\n",
            "Validation Loss: 0.1948\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_11\n",
            "\n",
            "Epoch 12\n",
            "[Epoch 12] Batch 0/405 Loss=0.2209 Mel=0.1192 Rmel=0.1017 Stop=0.0000\n",
            "[Epoch 12] Batch 10/405 Loss=0.2318 Mel=0.1247 Rmel=0.1071 Stop=0.0000\n",
            "[Epoch 12] Batch 20/405 Loss=0.2670 Mel=0.1426 Rmel=0.1244 Stop=0.0000\n",
            "[Epoch 12] Batch 30/405 Loss=0.2533 Mel=0.1346 Rmel=0.1186 Stop=0.0000\n",
            "[Epoch 12] Batch 40/405 Loss=0.2226 Mel=0.1183 Rmel=0.1043 Stop=0.0000\n",
            "[Epoch 12] Batch 50/405 Loss=0.2515 Mel=0.1348 Rmel=0.1167 Stop=0.0000\n",
            "[Epoch 12] Batch 60/405 Loss=0.2382 Mel=0.1280 Rmel=0.1101 Stop=0.0000\n",
            "[Epoch 12] Batch 70/405 Loss=0.2327 Mel=0.1252 Rmel=0.1074 Stop=0.0000\n",
            "[Epoch 12] Batch 80/405 Loss=0.2500 Mel=0.1344 Rmel=0.1156 Stop=0.0000\n",
            "[Epoch 12] Batch 90/405 Loss=0.2458 Mel=0.1321 Rmel=0.1136 Stop=0.0000\n",
            "[Epoch 12] Batch 100/405 Loss=0.2587 Mel=0.1365 Rmel=0.1222 Stop=0.0000\n",
            "[Epoch 12] Batch 110/405 Loss=0.2406 Mel=0.1291 Rmel=0.1115 Stop=0.0000\n",
            "[Epoch 12] Batch 120/405 Loss=0.2032 Mel=0.1091 Rmel=0.0941 Stop=0.0000\n",
            "[Epoch 12] Batch 130/405 Loss=0.2322 Mel=0.1251 Rmel=0.1070 Stop=0.0000\n",
            "[Epoch 12] Batch 140/405 Loss=0.2330 Mel=0.1256 Rmel=0.1074 Stop=0.0000\n",
            "[Epoch 12] Batch 150/405 Loss=0.2580 Mel=0.1381 Rmel=0.1199 Stop=0.0000\n",
            "[Epoch 12] Batch 160/405 Loss=0.2406 Mel=0.1295 Rmel=0.1111 Stop=0.0000\n",
            "[Epoch 12] Batch 170/405 Loss=0.2259 Mel=0.1219 Rmel=0.1039 Stop=0.0000\n",
            "[Epoch 12] Batch 180/405 Loss=0.2407 Mel=0.1297 Rmel=0.1110 Stop=0.0000\n",
            "[Epoch 12] Batch 190/405 Loss=0.2111 Mel=0.1133 Rmel=0.0978 Stop=0.0000\n",
            "[Epoch 12] Batch 200/405 Loss=0.2275 Mel=0.1226 Rmel=0.1049 Stop=0.0000\n",
            "[Epoch 12] Batch 210/405 Loss=0.2155 Mel=0.1158 Rmel=0.0997 Stop=0.0000\n",
            "[Epoch 12] Batch 220/405 Loss=0.2305 Mel=0.1242 Rmel=0.1063 Stop=0.0000\n",
            "[Epoch 12] Batch 230/405 Loss=0.2303 Mel=0.1240 Rmel=0.1062 Stop=0.0000\n",
            "[Epoch 12] Batch 240/405 Loss=0.2416 Mel=0.1299 Rmel=0.1117 Stop=0.0000\n",
            "[Epoch 12] Batch 250/405 Loss=0.2213 Mel=0.1193 Rmel=0.1020 Stop=0.0000\n",
            "[Epoch 12] Batch 260/405 Loss=0.2288 Mel=0.1228 Rmel=0.1060 Stop=0.0000\n",
            "[Epoch 12] Batch 270/405 Loss=0.2219 Mel=0.1193 Rmel=0.1025 Stop=0.0000\n",
            "[Epoch 12] Batch 280/405 Loss=0.2194 Mel=0.1183 Rmel=0.1010 Stop=0.0000\n",
            "[Epoch 12] Batch 290/405 Loss=0.2178 Mel=0.1174 Rmel=0.1004 Stop=0.0000\n",
            "[Epoch 12] Batch 300/405 Loss=0.2364 Mel=0.1276 Rmel=0.1088 Stop=0.0000\n",
            "[Epoch 12] Batch 310/405 Loss=0.2282 Mel=0.1229 Rmel=0.1053 Stop=0.0000\n",
            "[Epoch 12] Batch 320/405 Loss=0.2386 Mel=0.1289 Rmel=0.1097 Stop=0.0000\n",
            "[Epoch 12] Batch 330/405 Loss=0.2221 Mel=0.1198 Rmel=0.1022 Stop=0.0000\n",
            "[Epoch 12] Batch 340/405 Loss=0.2322 Mel=0.1250 Rmel=0.1072 Stop=0.0000\n",
            "[Epoch 12] Batch 350/405 Loss=0.2134 Mel=0.1146 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 12] Batch 360/405 Loss=0.2337 Mel=0.1260 Rmel=0.1076 Stop=0.0000\n",
            "[Epoch 12] Batch 370/405 Loss=0.2406 Mel=0.1299 Rmel=0.1107 Stop=0.0000\n",
            "[Epoch 12] Batch 380/405 Loss=0.2218 Mel=0.1196 Rmel=0.1022 Stop=0.0000\n",
            "[Epoch 12] Batch 390/405 Loss=0.2502 Mel=0.1351 Rmel=0.1152 Stop=0.0000\n",
            "[Epoch 12] Batch 400/405 Loss=0.2319 Mel=0.1249 Rmel=0.1070 Stop=0.0000\n",
            "Validation Loss: 0.1909\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_12\n",
            "\n",
            "Epoch 13\n",
            "[Epoch 13] Batch 0/405 Loss=0.2211 Mel=0.1189 Rmel=0.1022 Stop=0.0000\n",
            "[Epoch 13] Batch 10/405 Loss=0.2340 Mel=0.1264 Rmel=0.1076 Stop=0.0000\n",
            "[Epoch 13] Batch 20/405 Loss=0.2095 Mel=0.1128 Rmel=0.0967 Stop=0.0000\n",
            "[Epoch 13] Batch 30/405 Loss=0.2215 Mel=0.1194 Rmel=0.1021 Stop=0.0000\n",
            "[Epoch 13] Batch 40/405 Loss=0.2275 Mel=0.1224 Rmel=0.1051 Stop=0.0000\n",
            "[Epoch 13] Batch 50/405 Loss=0.2061 Mel=0.1109 Rmel=0.0952 Stop=0.0000\n",
            "[Epoch 13] Batch 60/405 Loss=0.2225 Mel=0.1204 Rmel=0.1021 Stop=0.0000\n",
            "[Epoch 13] Batch 70/405 Loss=0.2276 Mel=0.1230 Rmel=0.1046 Stop=0.0000\n",
            "[Epoch 13] Batch 80/405 Loss=0.2130 Mel=0.1151 Rmel=0.0979 Stop=0.0000\n",
            "[Epoch 13] Batch 90/405 Loss=0.2190 Mel=0.1181 Rmel=0.1009 Stop=0.0000\n",
            "[Epoch 13] Batch 100/405 Loss=0.2412 Mel=0.1299 Rmel=0.1114 Stop=0.0000\n",
            "[Epoch 13] Batch 110/405 Loss=0.2314 Mel=0.1249 Rmel=0.1066 Stop=0.0000\n",
            "[Epoch 13] Batch 120/405 Loss=0.2430 Mel=0.1313 Rmel=0.1117 Stop=0.0000\n",
            "[Epoch 13] Batch 130/405 Loss=0.2239 Mel=0.1210 Rmel=0.1029 Stop=0.0000\n",
            "[Epoch 13] Batch 140/405 Loss=0.2224 Mel=0.1202 Rmel=0.1022 Stop=0.0000\n",
            "[Epoch 13] Batch 150/405 Loss=0.2247 Mel=0.1211 Rmel=0.1036 Stop=0.0000\n",
            "[Epoch 13] Batch 160/405 Loss=0.2287 Mel=0.1232 Rmel=0.1055 Stop=0.0000\n",
            "[Epoch 13] Batch 170/405 Loss=0.2114 Mel=0.1143 Rmel=0.0971 Stop=0.0000\n",
            "[Epoch 13] Batch 180/405 Loss=0.2218 Mel=0.1200 Rmel=0.1018 Stop=0.0000\n",
            "[Epoch 13] Batch 190/405 Loss=0.2400 Mel=0.1297 Rmel=0.1103 Stop=0.0000\n",
            "[Epoch 13] Batch 200/405 Loss=0.2508 Mel=0.1354 Rmel=0.1154 Stop=0.0000\n",
            "[Epoch 13] Batch 210/405 Loss=0.2181 Mel=0.1175 Rmel=0.1006 Stop=0.0000\n",
            "[Epoch 13] Batch 220/405 Loss=0.2369 Mel=0.1282 Rmel=0.1087 Stop=0.0000\n",
            "[Epoch 13] Batch 230/405 Loss=0.2083 Mel=0.1125 Rmel=0.0957 Stop=0.0000\n",
            "[Epoch 13] Batch 240/405 Loss=0.2062 Mel=0.1110 Rmel=0.0953 Stop=0.0000\n",
            "[Epoch 13] Batch 250/405 Loss=0.2285 Mel=0.1235 Rmel=0.1050 Stop=0.0000\n",
            "[Epoch 13] Batch 260/405 Loss=0.2378 Mel=0.1283 Rmel=0.1095 Stop=0.0000\n",
            "[Epoch 13] Batch 270/405 Loss=0.2108 Mel=0.1139 Rmel=0.0969 Stop=0.0000\n",
            "[Epoch 13] Batch 280/405 Loss=0.2141 Mel=0.1155 Rmel=0.0986 Stop=0.0000\n",
            "[Epoch 13] Batch 290/405 Loss=0.2244 Mel=0.1214 Rmel=0.1030 Stop=0.0000\n",
            "[Epoch 13] Batch 300/405 Loss=0.2532 Mel=0.1366 Rmel=0.1166 Stop=0.0000\n",
            "[Epoch 13] Batch 310/405 Loss=0.2287 Mel=0.1230 Rmel=0.1057 Stop=0.0000\n",
            "[Epoch 13] Batch 320/405 Loss=0.2230 Mel=0.1203 Rmel=0.1027 Stop=0.0000\n",
            "[Epoch 13] Batch 330/405 Loss=0.2136 Mel=0.1156 Rmel=0.0979 Stop=0.0000\n",
            "[Epoch 13] Batch 340/405 Loss=0.2262 Mel=0.1225 Rmel=0.1037 Stop=0.0000\n",
            "[Epoch 13] Batch 350/405 Loss=0.2396 Mel=0.1293 Rmel=0.1103 Stop=0.0000\n",
            "[Epoch 13] Batch 360/405 Loss=0.2069 Mel=0.1118 Rmel=0.0951 Stop=0.0000\n",
            "[Epoch 13] Batch 370/405 Loss=0.2285 Mel=0.1232 Rmel=0.1052 Stop=0.0000\n",
            "[Epoch 13] Batch 380/405 Loss=0.2296 Mel=0.1238 Rmel=0.1058 Stop=0.0000\n",
            "[Epoch 13] Batch 390/405 Loss=0.2376 Mel=0.1283 Rmel=0.1093 Stop=0.0000\n",
            "[Epoch 13] Batch 400/405 Loss=0.2117 Mel=0.1141 Rmel=0.0975 Stop=0.0000\n",
            "Validation Loss: 0.1844\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_13\n",
            "\n",
            "Epoch 14\n",
            "[Epoch 14] Batch 0/405 Loss=0.2226 Mel=0.1200 Rmel=0.1025 Stop=0.0000\n",
            "[Epoch 14] Batch 10/405 Loss=0.2390 Mel=0.1289 Rmel=0.1101 Stop=0.0000\n",
            "[Epoch 14] Batch 20/405 Loss=0.2388 Mel=0.1292 Rmel=0.1096 Stop=0.0000\n",
            "[Epoch 14] Batch 30/405 Loss=0.2157 Mel=0.1164 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 14] Batch 40/405 Loss=0.2230 Mel=0.1206 Rmel=0.1025 Stop=0.0000\n",
            "[Epoch 14] Batch 50/405 Loss=0.2189 Mel=0.1180 Rmel=0.1008 Stop=0.0000\n",
            "[Epoch 14] Batch 60/405 Loss=0.2058 Mel=0.1110 Rmel=0.0947 Stop=0.0000\n",
            "[Epoch 14] Batch 70/405 Loss=0.2108 Mel=0.1134 Rmel=0.0973 Stop=0.0000\n",
            "[Epoch 14] Batch 80/405 Loss=0.1979 Mel=0.1069 Rmel=0.0909 Stop=0.0000\n",
            "[Epoch 14] Batch 90/405 Loss=0.2330 Mel=0.1257 Rmel=0.1073 Stop=0.0000\n",
            "[Epoch 14] Batch 100/405 Loss=0.2207 Mel=0.1190 Rmel=0.1016 Stop=0.0000\n",
            "[Epoch 14] Batch 110/405 Loss=0.2567 Mel=0.1389 Rmel=0.1179 Stop=0.0000\n",
            "[Epoch 14] Batch 120/405 Loss=0.2009 Mel=0.1087 Rmel=0.0921 Stop=0.0000\n",
            "[Epoch 14] Batch 130/405 Loss=0.2088 Mel=0.1127 Rmel=0.0961 Stop=0.0000\n",
            "[Epoch 14] Batch 140/405 Loss=0.2145 Mel=0.1160 Rmel=0.0984 Stop=0.0000\n",
            "[Epoch 14] Batch 150/405 Loss=0.2208 Mel=0.1192 Rmel=0.1016 Stop=0.0000\n",
            "[Epoch 14] Batch 160/405 Loss=0.2243 Mel=0.1212 Rmel=0.1031 Stop=0.0000\n",
            "[Epoch 14] Batch 170/405 Loss=0.2210 Mel=0.1195 Rmel=0.1015 Stop=0.0000\n",
            "[Epoch 14] Batch 180/405 Loss=0.2257 Mel=0.1215 Rmel=0.1041 Stop=0.0000\n",
            "[Epoch 14] Batch 190/405 Loss=0.2169 Mel=0.1176 Rmel=0.0993 Stop=0.0000\n",
            "[Epoch 14] Batch 200/405 Loss=0.2209 Mel=0.1194 Rmel=0.1015 Stop=0.0000\n",
            "[Epoch 14] Batch 210/405 Loss=0.2163 Mel=0.1169 Rmel=0.0994 Stop=0.0000\n",
            "[Epoch 14] Batch 220/405 Loss=0.2195 Mel=0.1187 Rmel=0.1008 Stop=0.0000\n",
            "[Epoch 14] Batch 230/405 Loss=0.2171 Mel=0.1173 Rmel=0.0998 Stop=0.0000\n",
            "[Epoch 14] Batch 240/405 Loss=0.2246 Mel=0.1214 Rmel=0.1031 Stop=0.0000\n",
            "[Epoch 14] Batch 250/405 Loss=0.2127 Mel=0.1149 Rmel=0.0978 Stop=0.0000\n",
            "[Epoch 14] Batch 260/405 Loss=0.2236 Mel=0.1204 Rmel=0.1032 Stop=0.0000\n",
            "[Epoch 14] Batch 270/405 Loss=0.2019 Mel=0.1090 Rmel=0.0929 Stop=0.0000\n",
            "[Epoch 14] Batch 280/405 Loss=0.2099 Mel=0.1136 Rmel=0.0963 Stop=0.0000\n",
            "[Epoch 14] Batch 290/405 Loss=0.2157 Mel=0.1167 Rmel=0.0989 Stop=0.0000\n",
            "[Epoch 14] Batch 300/405 Loss=0.1966 Mel=0.1066 Rmel=0.0900 Stop=0.0000\n",
            "[Epoch 14] Batch 310/405 Loss=0.2013 Mel=0.1091 Rmel=0.0922 Stop=0.0000\n",
            "[Epoch 14] Batch 320/405 Loss=0.2151 Mel=0.1164 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 14] Batch 330/405 Loss=0.2162 Mel=0.1165 Rmel=0.0996 Stop=0.0000\n",
            "[Epoch 14] Batch 340/405 Loss=0.2127 Mel=0.1146 Rmel=0.0981 Stop=0.0000\n",
            "[Epoch 14] Batch 350/405 Loss=0.1964 Mel=0.1058 Rmel=0.0906 Stop=0.0000\n",
            "[Epoch 14] Batch 360/405 Loss=0.1889 Mel=0.1017 Rmel=0.0873 Stop=0.0000\n",
            "[Epoch 14] Batch 370/405 Loss=0.1899 Mel=0.1023 Rmel=0.0876 Stop=0.0000\n",
            "[Epoch 14] Batch 380/405 Loss=0.2167 Mel=0.1167 Rmel=0.0999 Stop=0.0000\n",
            "[Epoch 14] Batch 390/405 Loss=0.2203 Mel=0.1187 Rmel=0.1015 Stop=0.0000\n",
            "[Epoch 14] Batch 400/405 Loss=0.2030 Mel=0.1095 Rmel=0.0935 Stop=0.0000\n",
            "Validation Loss: 0.1773\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_14\n",
            "\n",
            "Epoch 15\n",
            "[Epoch 15] Batch 0/405 Loss=0.2085 Mel=0.1123 Rmel=0.0962 Stop=0.0000\n",
            "[Epoch 15] Batch 10/405 Loss=0.2128 Mel=0.1147 Rmel=0.0981 Stop=0.0000\n",
            "[Epoch 15] Batch 20/405 Loss=0.2063 Mel=0.1117 Rmel=0.0945 Stop=0.0000\n",
            "[Epoch 15] Batch 30/405 Loss=0.2043 Mel=0.1111 Rmel=0.0933 Stop=0.0000\n",
            "[Epoch 15] Batch 40/405 Loss=0.2079 Mel=0.1127 Rmel=0.0951 Stop=0.0000\n",
            "[Epoch 15] Batch 50/405 Loss=0.2055 Mel=0.1117 Rmel=0.0938 Stop=0.0000\n",
            "[Epoch 15] Batch 60/405 Loss=0.2201 Mel=0.1196 Rmel=0.1005 Stop=0.0000\n",
            "[Epoch 15] Batch 70/405 Loss=0.2227 Mel=0.1209 Rmel=0.1018 Stop=0.0000\n",
            "[Epoch 15] Batch 80/405 Loss=0.2102 Mel=0.1141 Rmel=0.0961 Stop=0.0000\n",
            "[Epoch 15] Batch 90/405 Loss=0.2229 Mel=0.1209 Rmel=0.1020 Stop=0.0000\n",
            "[Epoch 15] Batch 100/405 Loss=0.1900 Mel=0.1032 Rmel=0.0868 Stop=0.0000\n",
            "[Epoch 15] Batch 110/405 Loss=0.2226 Mel=0.1210 Rmel=0.1016 Stop=0.0000\n",
            "[Epoch 15] Batch 120/405 Loss=0.2136 Mel=0.1160 Rmel=0.0976 Stop=0.0000\n",
            "[Epoch 15] Batch 130/405 Loss=0.2194 Mel=0.1192 Rmel=0.1002 Stop=0.0000\n",
            "[Epoch 15] Batch 140/405 Loss=0.2151 Mel=0.1168 Rmel=0.0982 Stop=0.0000\n",
            "[Epoch 15] Batch 150/405 Loss=0.2365 Mel=0.1284 Rmel=0.1081 Stop=0.0000\n",
            "[Epoch 15] Batch 160/405 Loss=0.2131 Mel=0.1157 Rmel=0.0974 Stop=0.0000\n",
            "[Epoch 15] Batch 170/405 Loss=0.2297 Mel=0.1246 Rmel=0.1052 Stop=0.0000\n",
            "[Epoch 15] Batch 180/405 Loss=0.2343 Mel=0.1270 Rmel=0.1073 Stop=0.0000\n",
            "[Epoch 15] Batch 190/405 Loss=0.1971 Mel=0.1070 Rmel=0.0901 Stop=0.0000\n",
            "[Epoch 15] Batch 200/405 Loss=0.2392 Mel=0.1299 Rmel=0.1093 Stop=0.0000\n",
            "[Epoch 15] Batch 210/405 Loss=0.2138 Mel=0.1163 Rmel=0.0976 Stop=0.0000\n",
            "[Epoch 15] Batch 220/405 Loss=0.2031 Mel=0.1102 Rmel=0.0929 Stop=0.0000\n",
            "[Epoch 15] Batch 230/405 Loss=0.2017 Mel=0.1092 Rmel=0.0925 Stop=0.0000\n",
            "[Epoch 15] Batch 240/405 Loss=0.2115 Mel=0.1146 Rmel=0.0969 Stop=0.0000\n",
            "[Epoch 15] Batch 250/405 Loss=0.2071 Mel=0.1121 Rmel=0.0949 Stop=0.0000\n",
            "[Epoch 15] Batch 260/405 Loss=0.2152 Mel=0.1166 Rmel=0.0986 Stop=0.0000\n",
            "[Epoch 15] Batch 270/405 Loss=0.2083 Mel=0.1127 Rmel=0.0956 Stop=0.0000\n",
            "[Epoch 15] Batch 280/405 Loss=0.2175 Mel=0.1177 Rmel=0.0998 Stop=0.0000\n",
            "[Epoch 15] Batch 290/405 Loss=0.2095 Mel=0.1131 Rmel=0.0964 Stop=0.0000\n",
            "[Epoch 15] Batch 300/405 Loss=0.2052 Mel=0.1108 Rmel=0.0944 Stop=0.0000\n",
            "[Epoch 15] Batch 310/405 Loss=0.1975 Mel=0.1066 Rmel=0.0909 Stop=0.0000\n",
            "[Epoch 15] Batch 320/405 Loss=0.2032 Mel=0.1099 Rmel=0.0934 Stop=0.0000\n",
            "[Epoch 15] Batch 330/405 Loss=0.2060 Mel=0.1112 Rmel=0.0948 Stop=0.0000\n",
            "[Epoch 15] Batch 340/405 Loss=0.2264 Mel=0.1227 Rmel=0.1037 Stop=0.0000\n",
            "[Epoch 15] Batch 350/405 Loss=0.2069 Mel=0.1115 Rmel=0.0953 Stop=0.0000\n",
            "[Epoch 15] Batch 360/405 Loss=0.2310 Mel=0.1249 Rmel=0.1061 Stop=0.0000\n",
            "[Epoch 15] Batch 370/405 Loss=0.2125 Mel=0.1136 Rmel=0.0988 Stop=0.0000\n",
            "[Epoch 15] Batch 380/405 Loss=0.2303 Mel=0.1235 Rmel=0.1069 Stop=0.0000\n",
            "[Epoch 15] Batch 390/405 Loss=0.2041 Mel=0.1102 Rmel=0.0939 Stop=0.0000\n",
            "[Epoch 15] Batch 400/405 Loss=0.1989 Mel=0.1076 Rmel=0.0913 Stop=0.0000\n",
            "Validation Loss: 0.1731\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_15\n",
            "\n",
            "Epoch 16\n",
            "[Epoch 16] Batch 0/405 Loss=0.1992 Mel=0.1075 Rmel=0.0916 Stop=0.0000\n",
            "[Epoch 16] Batch 10/405 Loss=0.2151 Mel=0.1164 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 16] Batch 20/405 Loss=0.1976 Mel=0.1069 Rmel=0.0907 Stop=0.0000\n",
            "[Epoch 16] Batch 30/405 Loss=0.2354 Mel=0.1268 Rmel=0.1086 Stop=0.0000\n",
            "[Epoch 16] Batch 40/405 Loss=0.2292 Mel=0.1234 Rmel=0.1058 Stop=0.0000\n",
            "[Epoch 16] Batch 50/405 Loss=0.2040 Mel=0.1101 Rmel=0.0939 Stop=0.0000\n",
            "[Epoch 16] Batch 60/405 Loss=0.2035 Mel=0.1099 Rmel=0.0936 Stop=0.0000\n",
            "[Epoch 16] Batch 70/405 Loss=0.2157 Mel=0.1170 Rmel=0.0987 Stop=0.0000\n",
            "[Epoch 16] Batch 80/405 Loss=0.1994 Mel=0.1078 Rmel=0.0916 Stop=0.0000\n",
            "[Epoch 16] Batch 90/405 Loss=0.2090 Mel=0.1131 Rmel=0.0959 Stop=0.0000\n",
            "[Epoch 16] Batch 100/405 Loss=0.2173 Mel=0.1173 Rmel=0.0999 Stop=0.0000\n",
            "[Epoch 16] Batch 110/405 Loss=0.1827 Mel=0.0987 Rmel=0.0840 Stop=0.0000\n",
            "[Epoch 16] Batch 120/405 Loss=0.2074 Mel=0.1127 Rmel=0.0946 Stop=0.0000\n",
            "[Epoch 16] Batch 130/405 Loss=0.2278 Mel=0.1229 Rmel=0.1049 Stop=0.0000\n",
            "[Epoch 16] Batch 140/405 Loss=0.2312 Mel=0.1251 Rmel=0.1061 Stop=0.0000\n",
            "[Epoch 16] Batch 150/405 Loss=0.2148 Mel=0.1171 Rmel=0.0977 Stop=0.0000\n",
            "[Epoch 16] Batch 160/405 Loss=0.2268 Mel=0.1238 Rmel=0.1030 Stop=0.0000\n",
            "[Epoch 16] Batch 170/405 Loss=0.1999 Mel=0.1089 Rmel=0.0911 Stop=0.0000\n",
            "[Epoch 16] Batch 180/405 Loss=0.2402 Mel=0.1309 Rmel=0.1094 Stop=0.0000\n",
            "[Epoch 16] Batch 190/405 Loss=0.2373 Mel=0.1296 Rmel=0.1076 Stop=0.0000\n",
            "[Epoch 16] Batch 200/405 Loss=0.2347 Mel=0.1278 Rmel=0.1069 Stop=0.0000\n",
            "[Epoch 16] Batch 210/405 Loss=0.2285 Mel=0.1246 Rmel=0.1039 Stop=0.0000\n",
            "[Epoch 16] Batch 220/405 Loss=0.2229 Mel=0.1216 Rmel=0.1013 Stop=0.0000\n",
            "[Epoch 16] Batch 230/405 Loss=0.2352 Mel=0.1284 Rmel=0.1068 Stop=0.0000\n",
            "[Epoch 16] Batch 240/405 Loss=0.1966 Mel=0.1071 Rmel=0.0895 Stop=0.0000\n",
            "[Epoch 16] Batch 250/405 Loss=0.2382 Mel=0.1299 Rmel=0.1083 Stop=0.0000\n",
            "[Epoch 16] Batch 260/405 Loss=0.1958 Mel=0.1068 Rmel=0.0890 Stop=0.0000\n",
            "[Epoch 16] Batch 270/405 Loss=0.2164 Mel=0.1180 Rmel=0.0984 Stop=0.0000\n",
            "[Epoch 16] Batch 280/405 Loss=0.2144 Mel=0.1170 Rmel=0.0973 Stop=0.0000\n",
            "[Epoch 16] Batch 290/405 Loss=0.1882 Mel=0.1025 Rmel=0.0857 Stop=0.0000\n",
            "[Epoch 16] Batch 300/405 Loss=0.1941 Mel=0.1054 Rmel=0.0887 Stop=0.0000\n",
            "[Epoch 16] Batch 310/405 Loss=0.2101 Mel=0.1143 Rmel=0.0958 Stop=0.0000\n",
            "[Epoch 16] Batch 320/405 Loss=0.2046 Mel=0.1111 Rmel=0.0935 Stop=0.0000\n",
            "[Epoch 16] Batch 330/405 Loss=0.2172 Mel=0.1178 Rmel=0.0994 Stop=0.0000\n",
            "[Epoch 16] Batch 340/405 Loss=0.2118 Mel=0.1149 Rmel=0.0968 Stop=0.0000\n",
            "[Epoch 16] Batch 350/405 Loss=0.2026 Mel=0.1096 Rmel=0.0930 Stop=0.0000\n",
            "[Epoch 16] Batch 360/405 Loss=0.2044 Mel=0.1109 Rmel=0.0935 Stop=0.0000\n",
            "[Epoch 16] Batch 370/405 Loss=0.1936 Mel=0.1051 Rmel=0.0885 Stop=0.0000\n",
            "[Epoch 16] Batch 380/405 Loss=0.1995 Mel=0.1081 Rmel=0.0914 Stop=0.0000\n",
            "[Epoch 16] Batch 390/405 Loss=0.2049 Mel=0.1113 Rmel=0.0936 Stop=0.0000\n",
            "[Epoch 16] Batch 400/405 Loss=0.2066 Mel=0.1122 Rmel=0.0944 Stop=0.0000\n",
            "Validation Loss: 0.1704\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_16\n",
            "\n",
            "Epoch 17\n",
            "[Epoch 17] Batch 0/405 Loss=0.1943 Mel=0.1051 Rmel=0.0892 Stop=0.0000\n",
            "[Epoch 17] Batch 10/405 Loss=0.1796 Mel=0.0973 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 17] Batch 20/405 Loss=0.1965 Mel=0.1060 Rmel=0.0904 Stop=0.0000\n",
            "[Epoch 17] Batch 30/405 Loss=0.1857 Mel=0.1004 Rmel=0.0853 Stop=0.0000\n",
            "[Epoch 17] Batch 40/405 Loss=0.2016 Mel=0.1086 Rmel=0.0930 Stop=0.0000\n",
            "[Epoch 17] Batch 50/405 Loss=0.1999 Mel=0.1082 Rmel=0.0917 Stop=0.0000\n",
            "[Epoch 17] Batch 60/405 Loss=0.1955 Mel=0.1058 Rmel=0.0897 Stop=0.0000\n",
            "[Epoch 17] Batch 70/405 Loss=0.1986 Mel=0.1076 Rmel=0.0909 Stop=0.0000\n",
            "[Epoch 17] Batch 80/405 Loss=0.1952 Mel=0.1055 Rmel=0.0896 Stop=0.0000\n",
            "[Epoch 17] Batch 90/405 Loss=0.2030 Mel=0.1098 Rmel=0.0932 Stop=0.0000\n",
            "[Epoch 17] Batch 100/405 Loss=0.1962 Mel=0.1064 Rmel=0.0899 Stop=0.0000\n",
            "[Epoch 17] Batch 110/405 Loss=0.1565 Mel=0.0842 Rmel=0.0723 Stop=0.0000\n",
            "[Epoch 17] Batch 120/405 Loss=0.1805 Mel=0.0977 Rmel=0.0829 Stop=0.0000\n",
            "[Epoch 17] Batch 130/405 Loss=0.1963 Mel=0.1061 Rmel=0.0902 Stop=0.0000\n",
            "[Epoch 17] Batch 140/405 Loss=0.2026 Mel=0.1094 Rmel=0.0932 Stop=0.0000\n",
            "[Epoch 17] Batch 150/405 Loss=0.2034 Mel=0.1100 Rmel=0.0934 Stop=0.0000\n",
            "[Epoch 17] Batch 160/405 Loss=0.2040 Mel=0.1103 Rmel=0.0937 Stop=0.0000\n",
            "[Epoch 17] Batch 170/405 Loss=0.1916 Mel=0.1040 Rmel=0.0877 Stop=0.0000\n",
            "[Epoch 17] Batch 180/405 Loss=0.1855 Mel=0.1001 Rmel=0.0853 Stop=0.0000\n",
            "[Epoch 17] Batch 190/405 Loss=0.2065 Mel=0.1116 Rmel=0.0949 Stop=0.0000\n",
            "[Epoch 17] Batch 200/405 Loss=0.2030 Mel=0.1095 Rmel=0.0935 Stop=0.0000\n",
            "[Epoch 17] Batch 210/405 Loss=0.2087 Mel=0.1130 Rmel=0.0957 Stop=0.0000\n",
            "[Epoch 17] Batch 220/405 Loss=0.1940 Mel=0.1049 Rmel=0.0891 Stop=0.0000\n",
            "[Epoch 17] Batch 230/405 Loss=0.2134 Mel=0.1155 Rmel=0.0979 Stop=0.0000\n",
            "[Epoch 17] Batch 240/405 Loss=0.2096 Mel=0.1135 Rmel=0.0961 Stop=0.0000\n",
            "[Epoch 17] Batch 250/405 Loss=0.2036 Mel=0.1102 Rmel=0.0934 Stop=0.0000\n",
            "[Epoch 17] Batch 260/405 Loss=0.2087 Mel=0.1131 Rmel=0.0956 Stop=0.0000\n",
            "[Epoch 17] Batch 270/405 Loss=0.1851 Mel=0.1002 Rmel=0.0849 Stop=0.0000\n",
            "[Epoch 17] Batch 280/405 Loss=0.2142 Mel=0.1160 Rmel=0.0982 Stop=0.0000\n",
            "[Epoch 17] Batch 290/405 Loss=0.2018 Mel=0.1089 Rmel=0.0929 Stop=0.0000\n",
            "[Epoch 17] Batch 300/405 Loss=0.1997 Mel=0.1082 Rmel=0.0916 Stop=0.0000\n",
            "[Epoch 17] Batch 310/405 Loss=0.1961 Mel=0.1062 Rmel=0.0899 Stop=0.0000\n",
            "[Epoch 17] Batch 320/405 Loss=0.1894 Mel=0.1025 Rmel=0.0868 Stop=0.0000\n",
            "[Epoch 17] Batch 330/405 Loss=0.1806 Mel=0.0980 Rmel=0.0826 Stop=0.0000\n",
            "[Epoch 17] Batch 340/405 Loss=0.1911 Mel=0.1037 Rmel=0.0874 Stop=0.0000\n",
            "[Epoch 17] Batch 350/405 Loss=0.1945 Mel=0.1052 Rmel=0.0894 Stop=0.0000\n",
            "[Epoch 17] Batch 360/405 Loss=0.1852 Mel=0.1001 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 17] Batch 370/405 Loss=0.2136 Mel=0.1159 Rmel=0.0977 Stop=0.0000\n",
            "[Epoch 17] Batch 380/405 Loss=0.1974 Mel=0.1071 Rmel=0.0903 Stop=0.0000\n",
            "[Epoch 17] Batch 390/405 Loss=0.1815 Mel=0.0983 Rmel=0.0832 Stop=0.0000\n",
            "[Epoch 17] Batch 400/405 Loss=0.1972 Mel=0.1063 Rmel=0.0908 Stop=0.0000\n",
            "Validation Loss: 0.1660\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_17\n",
            "\n",
            "Epoch 18\n",
            "[Epoch 18] Batch 0/405 Loss=0.2061 Mel=0.1116 Rmel=0.0945 Stop=0.0000\n",
            "[Epoch 18] Batch 10/405 Loss=0.1895 Mel=0.1025 Rmel=0.0870 Stop=0.0000\n",
            "[Epoch 18] Batch 20/405 Loss=0.1874 Mel=0.1010 Rmel=0.0864 Stop=0.0000\n",
            "[Epoch 18] Batch 30/405 Loss=0.1781 Mel=0.0966 Rmel=0.0815 Stop=0.0000\n",
            "[Epoch 18] Batch 40/405 Loss=0.1746 Mel=0.0945 Rmel=0.0801 Stop=0.0000\n",
            "[Epoch 18] Batch 50/405 Loss=0.1876 Mel=0.1013 Rmel=0.0863 Stop=0.0000\n",
            "[Epoch 18] Batch 60/405 Loss=0.1971 Mel=0.1064 Rmel=0.0906 Stop=0.0000\n",
            "[Epoch 18] Batch 70/405 Loss=0.1907 Mel=0.1035 Rmel=0.0872 Stop=0.0000\n",
            "[Epoch 18] Batch 80/405 Loss=0.1895 Mel=0.1024 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 18] Batch 90/405 Loss=0.1871 Mel=0.1015 Rmel=0.0856 Stop=0.0000\n",
            "[Epoch 18] Batch 100/405 Loss=0.1933 Mel=0.1046 Rmel=0.0888 Stop=0.0000\n",
            "[Epoch 18] Batch 110/405 Loss=0.1811 Mel=0.0981 Rmel=0.0830 Stop=0.0000\n",
            "[Epoch 18] Batch 120/405 Loss=0.2098 Mel=0.1134 Rmel=0.0963 Stop=0.0000\n",
            "[Epoch 18] Batch 130/405 Loss=0.1984 Mel=0.1072 Rmel=0.0911 Stop=0.0000\n",
            "[Epoch 18] Batch 140/405 Loss=0.1866 Mel=0.1010 Rmel=0.0856 Stop=0.0000\n",
            "[Epoch 18] Batch 150/405 Loss=0.2109 Mel=0.1144 Rmel=0.0964 Stop=0.0000\n",
            "[Epoch 18] Batch 160/405 Loss=0.2006 Mel=0.1087 Rmel=0.0919 Stop=0.0000\n",
            "[Epoch 18] Batch 170/405 Loss=0.1814 Mel=0.0979 Rmel=0.0835 Stop=0.0000\n",
            "[Epoch 18] Batch 180/405 Loss=0.1802 Mel=0.0977 Rmel=0.0826 Stop=0.0000\n",
            "[Epoch 18] Batch 190/405 Loss=0.1921 Mel=0.1039 Rmel=0.0881 Stop=0.0000\n",
            "[Epoch 18] Batch 200/405 Loss=0.1910 Mel=0.1033 Rmel=0.0877 Stop=0.0000\n",
            "[Epoch 18] Batch 210/405 Loss=0.1827 Mel=0.0989 Rmel=0.0839 Stop=0.0000\n",
            "[Epoch 18] Batch 220/405 Loss=0.1907 Mel=0.1033 Rmel=0.0873 Stop=0.0000\n",
            "[Epoch 18] Batch 230/405 Loss=0.2044 Mel=0.1104 Rmel=0.0940 Stop=0.0000\n",
            "[Epoch 18] Batch 240/405 Loss=0.1834 Mel=0.0989 Rmel=0.0845 Stop=0.0000\n",
            "[Epoch 18] Batch 250/405 Loss=0.1876 Mel=0.1013 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 18] Batch 260/405 Loss=0.1838 Mel=0.0993 Rmel=0.0844 Stop=0.0000\n",
            "[Epoch 18] Batch 270/405 Loss=0.1890 Mel=0.1022 Rmel=0.0868 Stop=0.0000\n",
            "[Epoch 18] Batch 280/405 Loss=0.1955 Mel=0.1060 Rmel=0.0895 Stop=0.0000\n",
            "[Epoch 18] Batch 290/405 Loss=0.2012 Mel=0.1086 Rmel=0.0925 Stop=0.0000\n",
            "[Epoch 18] Batch 300/405 Loss=0.2158 Mel=0.1164 Rmel=0.0994 Stop=0.0000\n",
            "[Epoch 18] Batch 310/405 Loss=0.1879 Mel=0.1014 Rmel=0.0865 Stop=0.0000\n",
            "[Epoch 18] Batch 320/405 Loss=0.2046 Mel=0.1108 Rmel=0.0938 Stop=0.0000\n",
            "[Epoch 18] Batch 330/405 Loss=0.2007 Mel=0.1083 Rmel=0.0924 Stop=0.0000\n",
            "[Epoch 18] Batch 340/405 Loss=0.1933 Mel=0.1044 Rmel=0.0889 Stop=0.0000\n",
            "[Epoch 18] Batch 350/405 Loss=0.1706 Mel=0.0921 Rmel=0.0785 Stop=0.0000\n",
            "[Epoch 18] Batch 360/405 Loss=0.2045 Mel=0.1107 Rmel=0.0938 Stop=0.0000\n",
            "[Epoch 18] Batch 370/405 Loss=0.2073 Mel=0.1125 Rmel=0.0948 Stop=0.0000\n",
            "[Epoch 18] Batch 380/405 Loss=0.1794 Mel=0.0970 Rmel=0.0825 Stop=0.0000\n",
            "[Epoch 18] Batch 390/405 Loss=0.1848 Mel=0.1003 Rmel=0.0845 Stop=0.0000\n",
            "[Epoch 18] Batch 400/405 Loss=0.1601 Mel=0.0865 Rmel=0.0736 Stop=0.0000\n",
            "Validation Loss: 0.1601\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_18\n",
            "\n",
            "Epoch 19\n",
            "[Epoch 19] Batch 0/405 Loss=0.1830 Mel=0.0989 Rmel=0.0841 Stop=0.0000\n",
            "[Epoch 19] Batch 10/405 Loss=0.1865 Mel=0.1008 Rmel=0.0856 Stop=0.0000\n",
            "[Epoch 19] Batch 20/405 Loss=0.2019 Mel=0.1094 Rmel=0.0925 Stop=0.0000\n",
            "[Epoch 19] Batch 30/405 Loss=0.1777 Mel=0.0960 Rmel=0.0817 Stop=0.0000\n",
            "[Epoch 19] Batch 40/405 Loss=0.1984 Mel=0.1076 Rmel=0.0908 Stop=0.0000\n",
            "[Epoch 19] Batch 50/405 Loss=0.2072 Mel=0.1121 Rmel=0.0951 Stop=0.0000\n",
            "[Epoch 19] Batch 60/405 Loss=0.1818 Mel=0.0983 Rmel=0.0835 Stop=0.0000\n",
            "[Epoch 19] Batch 70/405 Loss=0.1792 Mel=0.0968 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 19] Batch 80/405 Loss=0.2034 Mel=0.1100 Rmel=0.0934 Stop=0.0000\n",
            "[Epoch 19] Batch 90/405 Loss=0.1926 Mel=0.1041 Rmel=0.0885 Stop=0.0000\n",
            "[Epoch 19] Batch 100/405 Loss=0.1911 Mel=0.1032 Rmel=0.0879 Stop=0.0000\n",
            "[Epoch 19] Batch 110/405 Loss=0.1925 Mel=0.1044 Rmel=0.0881 Stop=0.0000\n",
            "[Epoch 19] Batch 120/405 Loss=0.2100 Mel=0.1134 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 19] Batch 130/405 Loss=0.1858 Mel=0.1005 Rmel=0.0853 Stop=0.0000\n",
            "[Epoch 19] Batch 140/405 Loss=0.1905 Mel=0.1030 Rmel=0.0875 Stop=0.0000\n",
            "[Epoch 19] Batch 150/405 Loss=0.1956 Mel=0.1060 Rmel=0.0896 Stop=0.0000\n",
            "[Epoch 19] Batch 160/405 Loss=0.2070 Mel=0.1120 Rmel=0.0950 Stop=0.0000\n",
            "[Epoch 19] Batch 170/405 Loss=0.1922 Mel=0.1042 Rmel=0.0880 Stop=0.0000\n",
            "[Epoch 19] Batch 180/405 Loss=0.1933 Mel=0.1044 Rmel=0.0889 Stop=0.0000\n",
            "[Epoch 19] Batch 190/405 Loss=0.1877 Mel=0.1016 Rmel=0.0861 Stop=0.0000\n",
            "[Epoch 19] Batch 200/405 Loss=0.1786 Mel=0.0968 Rmel=0.0818 Stop=0.0000\n",
            "[Epoch 19] Batch 210/405 Loss=0.1793 Mel=0.0972 Rmel=0.0821 Stop=0.0000\n",
            "[Epoch 19] Batch 220/405 Loss=0.2009 Mel=0.1089 Rmel=0.0920 Stop=0.0000\n",
            "[Epoch 19] Batch 230/405 Loss=0.1815 Mel=0.0982 Rmel=0.0833 Stop=0.0000\n",
            "[Epoch 19] Batch 240/405 Loss=0.1825 Mel=0.0985 Rmel=0.0840 Stop=0.0000\n",
            "[Epoch 19] Batch 250/405 Loss=0.1883 Mel=0.1020 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 19] Batch 260/405 Loss=0.1801 Mel=0.0973 Rmel=0.0828 Stop=0.0000\n",
            "[Epoch 19] Batch 270/405 Loss=0.1901 Mel=0.1030 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 19] Batch 280/405 Loss=0.1967 Mel=0.1063 Rmel=0.0904 Stop=0.0000\n",
            "[Epoch 19] Batch 290/405 Loss=0.1801 Mel=0.0972 Rmel=0.0829 Stop=0.0000\n",
            "[Epoch 19] Batch 300/405 Loss=0.1810 Mel=0.0979 Rmel=0.0831 Stop=0.0000\n",
            "[Epoch 19] Batch 310/405 Loss=0.2012 Mel=0.1089 Rmel=0.0923 Stop=0.0000\n",
            "[Epoch 19] Batch 320/405 Loss=0.1897 Mel=0.1026 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 19] Batch 330/405 Loss=0.1892 Mel=0.1026 Rmel=0.0867 Stop=0.0000\n",
            "[Epoch 19] Batch 340/405 Loss=0.1825 Mel=0.0987 Rmel=0.0838 Stop=0.0000\n",
            "[Epoch 19] Batch 350/405 Loss=0.1956 Mel=0.1059 Rmel=0.0897 Stop=0.0000\n",
            "[Epoch 19] Batch 360/405 Loss=0.1794 Mel=0.0970 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 19] Batch 370/405 Loss=0.1909 Mel=0.1034 Rmel=0.0875 Stop=0.0000\n",
            "[Epoch 19] Batch 380/405 Loss=0.1981 Mel=0.1071 Rmel=0.0909 Stop=0.0000\n",
            "[Epoch 19] Batch 390/405 Loss=0.1735 Mel=0.0937 Rmel=0.0798 Stop=0.0000\n",
            "[Epoch 19] Batch 400/405 Loss=0.1783 Mel=0.0967 Rmel=0.0816 Stop=0.0000\n",
            "Validation Loss: 0.1574\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_19\n",
            "\n",
            "Epoch 20\n",
            "[Epoch 20] Batch 0/405 Loss=0.1794 Mel=0.0971 Rmel=0.0823 Stop=0.0000\n",
            "[Epoch 20] Batch 10/405 Loss=0.1797 Mel=0.0974 Rmel=0.0823 Stop=0.0000\n",
            "[Epoch 20] Batch 20/405 Loss=0.1888 Mel=0.1023 Rmel=0.0864 Stop=0.0000\n",
            "[Epoch 20] Batch 30/405 Loss=0.1937 Mel=0.1049 Rmel=0.0889 Stop=0.0000\n",
            "[Epoch 20] Batch 40/405 Loss=0.1833 Mel=0.0991 Rmel=0.0842 Stop=0.0000\n",
            "[Epoch 20] Batch 50/405 Loss=0.1855 Mel=0.1004 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 20] Batch 60/405 Loss=0.2095 Mel=0.1132 Rmel=0.0963 Stop=0.0000\n",
            "[Epoch 20] Batch 70/405 Loss=0.1798 Mel=0.0972 Rmel=0.0825 Stop=0.0000\n",
            "[Epoch 20] Batch 80/405 Loss=0.2011 Mel=0.1088 Rmel=0.0923 Stop=0.0000\n",
            "[Epoch 20] Batch 90/405 Loss=0.1813 Mel=0.0981 Rmel=0.0832 Stop=0.0000\n",
            "[Epoch 20] Batch 100/405 Loss=0.1840 Mel=0.0995 Rmel=0.0846 Stop=0.0000\n",
            "[Epoch 20] Batch 110/405 Loss=0.1821 Mel=0.0982 Rmel=0.0839 Stop=0.0000\n",
            "[Epoch 20] Batch 120/405 Loss=0.1882 Mel=0.1017 Rmel=0.0864 Stop=0.0000\n",
            "[Epoch 20] Batch 130/405 Loss=0.2039 Mel=0.1105 Rmel=0.0934 Stop=0.0000\n",
            "[Epoch 20] Batch 140/405 Loss=0.1970 Mel=0.1067 Rmel=0.0903 Stop=0.0000\n",
            "[Epoch 20] Batch 150/405 Loss=0.1750 Mel=0.0944 Rmel=0.0805 Stop=0.0000\n",
            "[Epoch 20] Batch 160/405 Loss=0.1774 Mel=0.0959 Rmel=0.0815 Stop=0.0000\n",
            "[Epoch 20] Batch 170/405 Loss=0.1730 Mel=0.0933 Rmel=0.0796 Stop=0.0000\n",
            "[Epoch 20] Batch 180/405 Loss=0.1697 Mel=0.0915 Rmel=0.0782 Stop=0.0000\n",
            "[Epoch 20] Batch 190/405 Loss=0.1847 Mel=0.0999 Rmel=0.0848 Stop=0.0000\n",
            "[Epoch 20] Batch 200/405 Loss=0.1978 Mel=0.1069 Rmel=0.0909 Stop=0.0000\n",
            "[Epoch 20] Batch 210/405 Loss=0.1898 Mel=0.1026 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 20] Batch 220/405 Loss=0.1796 Mel=0.0973 Rmel=0.0823 Stop=0.0000\n",
            "[Epoch 20] Batch 230/405 Loss=0.1931 Mel=0.1046 Rmel=0.0884 Stop=0.0000\n",
            "[Epoch 20] Batch 240/405 Loss=0.1953 Mel=0.1059 Rmel=0.0894 Stop=0.0000\n",
            "[Epoch 20] Batch 250/405 Loss=0.1812 Mel=0.0985 Rmel=0.0828 Stop=0.0000\n",
            "[Epoch 20] Batch 260/405 Loss=0.2123 Mel=0.1152 Rmel=0.0971 Stop=0.0000\n",
            "[Epoch 20] Batch 270/405 Loss=0.2403 Mel=0.1314 Rmel=0.1089 Stop=0.0000\n",
            "[Epoch 20] Batch 280/405 Loss=0.2334 Mel=0.1276 Rmel=0.1059 Stop=0.0000\n",
            "[Epoch 20] Batch 290/405 Loss=0.2277 Mel=0.1243 Rmel=0.1034 Stop=0.0000\n",
            "[Epoch 20] Batch 300/405 Loss=0.2293 Mel=0.1251 Rmel=0.1042 Stop=0.0000\n",
            "[Epoch 20] Batch 310/405 Loss=0.2066 Mel=0.1124 Rmel=0.0942 Stop=0.0000\n",
            "[Epoch 20] Batch 320/405 Loss=0.2050 Mel=0.1120 Rmel=0.0931 Stop=0.0000\n",
            "[Epoch 20] Batch 330/405 Loss=0.2010 Mel=0.1092 Rmel=0.0918 Stop=0.0000\n",
            "[Epoch 20] Batch 340/405 Loss=0.2316 Mel=0.1265 Rmel=0.1052 Stop=0.0000\n",
            "[Epoch 20] Batch 350/405 Loss=0.2189 Mel=0.1193 Rmel=0.0995 Stop=0.0000\n",
            "[Epoch 20] Batch 360/405 Loss=0.1942 Mel=0.1058 Rmel=0.0884 Stop=0.0000\n",
            "[Epoch 20] Batch 370/405 Loss=0.2044 Mel=0.1115 Rmel=0.0929 Stop=0.0000\n",
            "[Epoch 20] Batch 380/405 Loss=0.1907 Mel=0.1038 Rmel=0.0869 Stop=0.0000\n",
            "[Epoch 20] Batch 390/405 Loss=0.2150 Mel=0.1170 Rmel=0.0980 Stop=0.0000\n",
            "[Epoch 20] Batch 400/405 Loss=0.2143 Mel=0.1168 Rmel=0.0975 Stop=0.0000\n",
            "Validation Loss: 0.1627\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_20\n",
            "\n",
            "Epoch 21\n",
            "[Epoch 21] Batch 0/405 Loss=0.1880 Mel=0.1020 Rmel=0.0860 Stop=0.0000\n",
            "[Epoch 21] Batch 10/405 Loss=0.1985 Mel=0.1082 Rmel=0.0903 Stop=0.0000\n",
            "[Epoch 21] Batch 20/405 Loss=0.2102 Mel=0.1143 Rmel=0.0959 Stop=0.0000\n",
            "[Epoch 21] Batch 30/405 Loss=0.1907 Mel=0.1037 Rmel=0.0870 Stop=0.0000\n",
            "[Epoch 21] Batch 40/405 Loss=0.1950 Mel=0.1060 Rmel=0.0889 Stop=0.0000\n",
            "[Epoch 21] Batch 50/405 Loss=0.1974 Mel=0.1074 Rmel=0.0900 Stop=0.0000\n",
            "[Epoch 21] Batch 60/405 Loss=0.1905 Mel=0.1033 Rmel=0.0871 Stop=0.0000\n",
            "[Epoch 21] Batch 70/405 Loss=0.1869 Mel=0.1017 Rmel=0.0852 Stop=0.0000\n",
            "[Epoch 21] Batch 80/405 Loss=0.1840 Mel=0.0997 Rmel=0.0843 Stop=0.0000\n",
            "[Epoch 21] Batch 90/405 Loss=0.1869 Mel=0.1013 Rmel=0.0855 Stop=0.0000\n",
            "[Epoch 21] Batch 100/405 Loss=0.1884 Mel=0.1025 Rmel=0.0860 Stop=0.0000\n",
            "[Epoch 21] Batch 110/405 Loss=0.1943 Mel=0.1056 Rmel=0.0887 Stop=0.0000\n",
            "[Epoch 21] Batch 120/405 Loss=0.1979 Mel=0.1074 Rmel=0.0905 Stop=0.0000\n",
            "[Epoch 21] Batch 130/405 Loss=0.1906 Mel=0.1032 Rmel=0.0874 Stop=0.0000\n",
            "[Epoch 21] Batch 140/405 Loss=0.2016 Mel=0.1096 Rmel=0.0921 Stop=0.0000\n",
            "[Epoch 21] Batch 150/405 Loss=0.1984 Mel=0.1076 Rmel=0.0907 Stop=0.0000\n",
            "[Epoch 21] Batch 160/405 Loss=0.1925 Mel=0.1046 Rmel=0.0878 Stop=0.0000\n",
            "[Epoch 21] Batch 170/405 Loss=0.1819 Mel=0.0986 Rmel=0.0833 Stop=0.0000\n",
            "[Epoch 21] Batch 180/405 Loss=0.1894 Mel=0.1025 Rmel=0.0868 Stop=0.0000\n",
            "[Epoch 21] Batch 190/405 Loss=0.1839 Mel=0.0999 Rmel=0.0840 Stop=0.0000\n",
            "[Epoch 21] Batch 200/405 Loss=0.1849 Mel=0.1002 Rmel=0.0847 Stop=0.0000\n",
            "[Epoch 21] Batch 210/405 Loss=0.1861 Mel=0.1010 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 21] Batch 220/405 Loss=0.2042 Mel=0.1108 Rmel=0.0933 Stop=0.0000\n",
            "[Epoch 21] Batch 230/405 Loss=0.1861 Mel=0.1009 Rmel=0.0852 Stop=0.0000\n",
            "[Epoch 21] Batch 240/405 Loss=0.1782 Mel=0.0966 Rmel=0.0815 Stop=0.0000\n",
            "[Epoch 21] Batch 250/405 Loss=0.1810 Mel=0.0982 Rmel=0.0828 Stop=0.0000\n",
            "[Epoch 21] Batch 260/405 Loss=0.1798 Mel=0.0976 Rmel=0.0822 Stop=0.0000\n",
            "[Epoch 21] Batch 270/405 Loss=0.2017 Mel=0.1094 Rmel=0.0922 Stop=0.0000\n",
            "[Epoch 21] Batch 280/405 Loss=0.1744 Mel=0.0945 Rmel=0.0799 Stop=0.0000\n",
            "[Epoch 21] Batch 290/405 Loss=0.2116 Mel=0.1150 Rmel=0.0966 Stop=0.0000\n",
            "[Epoch 21] Batch 300/405 Loss=0.1936 Mel=0.1051 Rmel=0.0885 Stop=0.0000\n",
            "[Epoch 21] Batch 310/405 Loss=0.1812 Mel=0.0984 Rmel=0.0829 Stop=0.0000\n",
            "[Epoch 21] Batch 320/405 Loss=0.2021 Mel=0.1096 Rmel=0.0924 Stop=0.0000\n",
            "[Epoch 21] Batch 330/405 Loss=0.1843 Mel=0.1002 Rmel=0.0841 Stop=0.0000\n",
            "[Epoch 21] Batch 340/405 Loss=0.1955 Mel=0.1063 Rmel=0.0892 Stop=0.0000\n",
            "[Epoch 21] Batch 350/405 Loss=0.1895 Mel=0.1029 Rmel=0.0866 Stop=0.0000\n",
            "[Epoch 21] Batch 360/405 Loss=0.1843 Mel=0.0998 Rmel=0.0845 Stop=0.0000\n",
            "[Epoch 21] Batch 370/405 Loss=0.1889 Mel=0.1021 Rmel=0.0867 Stop=0.0000\n",
            "[Epoch 21] Batch 380/405 Loss=0.1957 Mel=0.1062 Rmel=0.0895 Stop=0.0000\n",
            "[Epoch 21] Batch 390/405 Loss=0.1891 Mel=0.1026 Rmel=0.0865 Stop=0.0000\n",
            "[Epoch 21] Batch 400/405 Loss=0.1803 Mel=0.0977 Rmel=0.0827 Stop=0.0000\n",
            "Validation Loss: 0.1560\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_21\n",
            "\n",
            "Epoch 22\n",
            "[Epoch 22] Batch 0/405 Loss=0.1717 Mel=0.0932 Rmel=0.0786 Stop=0.0000\n",
            "[Epoch 22] Batch 10/405 Loss=0.1837 Mel=0.0995 Rmel=0.0842 Stop=0.0000\n",
            "[Epoch 22] Batch 20/405 Loss=0.1726 Mel=0.0936 Rmel=0.0790 Stop=0.0000\n",
            "[Epoch 22] Batch 30/405 Loss=0.1959 Mel=0.1062 Rmel=0.0896 Stop=0.0000\n",
            "[Epoch 22] Batch 40/405 Loss=0.1912 Mel=0.1037 Rmel=0.0875 Stop=0.0000\n",
            "[Epoch 22] Batch 50/405 Loss=0.1919 Mel=0.1043 Rmel=0.0876 Stop=0.0000\n",
            "[Epoch 22] Batch 60/405 Loss=0.1797 Mel=0.0974 Rmel=0.0822 Stop=0.0000\n",
            "[Epoch 22] Batch 70/405 Loss=0.1901 Mel=0.1030 Rmel=0.0870 Stop=0.0000\n",
            "[Epoch 22] Batch 80/405 Loss=0.1826 Mel=0.0990 Rmel=0.0836 Stop=0.0000\n",
            "[Epoch 22] Batch 90/405 Loss=0.1855 Mel=0.1004 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 22] Batch 100/405 Loss=0.1832 Mel=0.0989 Rmel=0.0844 Stop=0.0000\n",
            "[Epoch 22] Batch 110/405 Loss=0.1912 Mel=0.1036 Rmel=0.0876 Stop=0.0000\n",
            "[Epoch 22] Batch 120/405 Loss=0.1867 Mel=0.1011 Rmel=0.0856 Stop=0.0000\n",
            "[Epoch 22] Batch 130/405 Loss=0.1856 Mel=0.1005 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 22] Batch 140/405 Loss=0.1941 Mel=0.1052 Rmel=0.0890 Stop=0.0000\n",
            "[Epoch 22] Batch 150/405 Loss=0.1964 Mel=0.1065 Rmel=0.0899 Stop=0.0000\n",
            "[Epoch 22] Batch 160/405 Loss=0.1847 Mel=0.1005 Rmel=0.0843 Stop=0.0000\n",
            "[Epoch 22] Batch 170/405 Loss=0.1891 Mel=0.1023 Rmel=0.0868 Stop=0.0000\n",
            "[Epoch 22] Batch 180/405 Loss=0.1944 Mel=0.1053 Rmel=0.0891 Stop=0.0000\n",
            "[Epoch 22] Batch 190/405 Loss=0.1821 Mel=0.0988 Rmel=0.0833 Stop=0.0000\n",
            "[Epoch 22] Batch 200/405 Loss=0.1789 Mel=0.0970 Rmel=0.0819 Stop=0.0000\n",
            "[Epoch 22] Batch 210/405 Loss=0.1799 Mel=0.0975 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 22] Batch 220/405 Loss=0.1769 Mel=0.0959 Rmel=0.0810 Stop=0.0000\n",
            "[Epoch 22] Batch 230/405 Loss=0.1636 Mel=0.0885 Rmel=0.0752 Stop=0.0000\n",
            "[Epoch 22] Batch 240/405 Loss=0.1709 Mel=0.0925 Rmel=0.0783 Stop=0.0000\n",
            "[Epoch 22] Batch 250/405 Loss=0.1736 Mel=0.0941 Rmel=0.0796 Stop=0.0000\n",
            "[Epoch 22] Batch 260/405 Loss=0.1860 Mel=0.1009 Rmel=0.0851 Stop=0.0000\n",
            "[Epoch 22] Batch 270/405 Loss=0.1952 Mel=0.1057 Rmel=0.0895 Stop=0.0000\n",
            "[Epoch 22] Batch 280/405 Loss=0.1743 Mel=0.0947 Rmel=0.0797 Stop=0.0000\n",
            "[Epoch 22] Batch 290/405 Loss=0.1773 Mel=0.0961 Rmel=0.0812 Stop=0.0000\n",
            "[Epoch 22] Batch 300/405 Loss=0.1737 Mel=0.0940 Rmel=0.0797 Stop=0.0000\n",
            "[Epoch 22] Batch 310/405 Loss=0.1734 Mel=0.0939 Rmel=0.0794 Stop=0.0000\n",
            "[Epoch 22] Batch 320/405 Loss=0.1724 Mel=0.0933 Rmel=0.0792 Stop=0.0000\n",
            "[Epoch 22] Batch 330/405 Loss=0.1745 Mel=0.0945 Rmel=0.0800 Stop=0.0000\n",
            "[Epoch 22] Batch 340/405 Loss=0.1661 Mel=0.0898 Rmel=0.0762 Stop=0.0000\n",
            "[Epoch 22] Batch 350/405 Loss=0.1882 Mel=0.1020 Rmel=0.0862 Stop=0.0000\n",
            "[Epoch 22] Batch 360/405 Loss=0.1750 Mel=0.0947 Rmel=0.0802 Stop=0.0000\n",
            "[Epoch 22] Batch 370/405 Loss=0.1754 Mel=0.0950 Rmel=0.0804 Stop=0.0000\n",
            "[Epoch 22] Batch 380/405 Loss=0.1966 Mel=0.1068 Rmel=0.0899 Stop=0.0000\n",
            "[Epoch 22] Batch 390/405 Loss=0.1764 Mel=0.0955 Rmel=0.0809 Stop=0.0000\n",
            "[Epoch 22] Batch 400/405 Loss=0.1813 Mel=0.0982 Rmel=0.0831 Stop=0.0000\n",
            "Validation Loss: 0.1533\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_22\n",
            "\n",
            "Epoch 23\n",
            "[Epoch 23] Batch 0/405 Loss=0.1844 Mel=0.0997 Rmel=0.0847 Stop=0.0000\n",
            "[Epoch 23] Batch 10/405 Loss=0.1653 Mel=0.0896 Rmel=0.0757 Stop=0.0000\n",
            "[Epoch 23] Batch 20/405 Loss=0.1843 Mel=0.0997 Rmel=0.0846 Stop=0.0000\n",
            "[Epoch 23] Batch 30/405 Loss=0.1743 Mel=0.0946 Rmel=0.0797 Stop=0.0000\n",
            "[Epoch 23] Batch 40/405 Loss=0.1856 Mel=0.1004 Rmel=0.0852 Stop=0.0000\n",
            "[Epoch 23] Batch 50/405 Loss=0.1638 Mel=0.0888 Rmel=0.0750 Stop=0.0000\n",
            "[Epoch 23] Batch 60/405 Loss=0.1975 Mel=0.1070 Rmel=0.0905 Stop=0.0000\n",
            "[Epoch 23] Batch 70/405 Loss=0.1709 Mel=0.0924 Rmel=0.0785 Stop=0.0000\n",
            "[Epoch 23] Batch 80/405 Loss=0.1731 Mel=0.0938 Rmel=0.0793 Stop=0.0000\n",
            "[Epoch 23] Batch 90/405 Loss=0.1729 Mel=0.0936 Rmel=0.0793 Stop=0.0000\n",
            "[Epoch 23] Batch 100/405 Loss=0.1747 Mel=0.0948 Rmel=0.0800 Stop=0.0000\n",
            "[Epoch 23] Batch 110/405 Loss=0.1658 Mel=0.0897 Rmel=0.0761 Stop=0.0000\n",
            "[Epoch 23] Batch 120/405 Loss=0.1797 Mel=0.0974 Rmel=0.0823 Stop=0.0000\n",
            "[Epoch 23] Batch 130/405 Loss=0.1943 Mel=0.1053 Rmel=0.0889 Stop=0.0000\n",
            "[Epoch 23] Batch 140/405 Loss=0.1818 Mel=0.0983 Rmel=0.0835 Stop=0.0000\n",
            "[Epoch 23] Batch 150/405 Loss=0.1540 Mel=0.0835 Rmel=0.0704 Stop=0.0000\n",
            "[Epoch 23] Batch 160/405 Loss=0.1714 Mel=0.0928 Rmel=0.0785 Stop=0.0000\n",
            "[Epoch 23] Batch 170/405 Loss=0.1591 Mel=0.0861 Rmel=0.0730 Stop=0.0000\n",
            "[Epoch 23] Batch 180/405 Loss=0.1940 Mel=0.1052 Rmel=0.0887 Stop=0.0000\n",
            "[Epoch 23] Batch 190/405 Loss=0.1683 Mel=0.0914 Rmel=0.0769 Stop=0.0000\n",
            "[Epoch 23] Batch 200/405 Loss=0.1918 Mel=0.1040 Rmel=0.0878 Stop=0.0000\n",
            "[Epoch 23] Batch 210/405 Loss=0.1706 Mel=0.0925 Rmel=0.0781 Stop=0.0000\n",
            "[Epoch 23] Batch 220/405 Loss=0.1683 Mel=0.0910 Rmel=0.0773 Stop=0.0000\n",
            "[Epoch 23] Batch 230/405 Loss=0.1655 Mel=0.0897 Rmel=0.0758 Stop=0.0000\n",
            "[Epoch 23] Batch 240/405 Loss=0.1728 Mel=0.0937 Rmel=0.0791 Stop=0.0000\n",
            "[Epoch 23] Batch 250/405 Loss=0.1727 Mel=0.0936 Rmel=0.0791 Stop=0.0000\n",
            "[Epoch 23] Batch 260/405 Loss=0.1955 Mel=0.1061 Rmel=0.0893 Stop=0.0000\n",
            "[Epoch 23] Batch 270/405 Loss=0.1802 Mel=0.0978 Rmel=0.0824 Stop=0.0000\n",
            "[Epoch 23] Batch 280/405 Loss=0.1691 Mel=0.0915 Rmel=0.0776 Stop=0.0000\n",
            "[Epoch 23] Batch 290/405 Loss=0.1664 Mel=0.0902 Rmel=0.0762 Stop=0.0000\n",
            "[Epoch 23] Batch 300/405 Loss=0.1802 Mel=0.0977 Rmel=0.0825 Stop=0.0000\n",
            "[Epoch 23] Batch 310/405 Loss=0.2061 Mel=0.1117 Rmel=0.0944 Stop=0.0000\n",
            "[Epoch 23] Batch 320/405 Loss=0.1851 Mel=0.1003 Rmel=0.0848 Stop=0.0000\n",
            "[Epoch 23] Batch 330/405 Loss=0.1744 Mel=0.0945 Rmel=0.0800 Stop=0.0000\n",
            "[Epoch 23] Batch 340/405 Loss=0.1926 Mel=0.1044 Rmel=0.0882 Stop=0.0000\n",
            "[Epoch 23] Batch 350/405 Loss=0.1672 Mel=0.0905 Rmel=0.0767 Stop=0.0000\n",
            "[Epoch 23] Batch 360/405 Loss=0.1660 Mel=0.0898 Rmel=0.0762 Stop=0.0000\n",
            "[Epoch 23] Batch 370/405 Loss=0.1826 Mel=0.0990 Rmel=0.0835 Stop=0.0000\n",
            "[Epoch 23] Batch 380/405 Loss=0.1885 Mel=0.1022 Rmel=0.0863 Stop=0.0000\n",
            "[Epoch 23] Batch 390/405 Loss=0.1852 Mel=0.1004 Rmel=0.0848 Stop=0.0000\n",
            "[Epoch 23] Batch 400/405 Loss=0.1963 Mel=0.1063 Rmel=0.0900 Stop=0.0000\n",
            "Validation Loss: 0.1515\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_23\n",
            "\n",
            "Epoch 24\n",
            "[Epoch 24] Batch 0/405 Loss=0.1835 Mel=0.0993 Rmel=0.0842 Stop=0.0000\n",
            "[Epoch 24] Batch 10/405 Loss=0.1794 Mel=0.0974 Rmel=0.0820 Stop=0.0000\n",
            "[Epoch 24] Batch 20/405 Loss=0.1875 Mel=0.1016 Rmel=0.0859 Stop=0.0000\n",
            "[Epoch 24] Batch 30/405 Loss=0.1716 Mel=0.0930 Rmel=0.0787 Stop=0.0000\n",
            "[Epoch 24] Batch 40/405 Loss=0.2002 Mel=0.1087 Rmel=0.0915 Stop=0.0000\n",
            "[Epoch 24] Batch 50/405 Loss=0.1814 Mel=0.0982 Rmel=0.0832 Stop=0.0000\n",
            "[Epoch 24] Batch 60/405 Loss=0.1701 Mel=0.0920 Rmel=0.0781 Stop=0.0000\n",
            "[Epoch 24] Batch 70/405 Loss=0.1774 Mel=0.0961 Rmel=0.0812 Stop=0.0000\n",
            "[Epoch 24] Batch 80/405 Loss=0.1674 Mel=0.0907 Rmel=0.0767 Stop=0.0000\n",
            "[Epoch 24] Batch 90/405 Loss=0.1781 Mel=0.0963 Rmel=0.0817 Stop=0.0000\n",
            "[Epoch 24] Batch 100/405 Loss=0.1758 Mel=0.0952 Rmel=0.0806 Stop=0.0000\n",
            "[Epoch 24] Batch 110/405 Loss=0.1883 Mel=0.1018 Rmel=0.0864 Stop=0.0000\n",
            "[Epoch 24] Batch 120/405 Loss=0.1691 Mel=0.0915 Rmel=0.0776 Stop=0.0000\n",
            "[Epoch 24] Batch 130/405 Loss=0.1609 Mel=0.0871 Rmel=0.0738 Stop=0.0000\n",
            "[Epoch 24] Batch 140/405 Loss=0.1871 Mel=0.1013 Rmel=0.0858 Stop=0.0000\n",
            "[Epoch 24] Batch 150/405 Loss=0.1770 Mel=0.0960 Rmel=0.0810 Stop=0.0000\n",
            "[Epoch 24] Batch 160/405 Loss=0.1977 Mel=0.1071 Rmel=0.0906 Stop=0.0000\n",
            "[Epoch 24] Batch 170/405 Loss=0.1880 Mel=0.1021 Rmel=0.0859 Stop=0.0000\n",
            "[Epoch 24] Batch 180/405 Loss=0.1816 Mel=0.0986 Rmel=0.0830 Stop=0.0000\n",
            "[Epoch 24] Batch 190/405 Loss=0.1761 Mel=0.0955 Rmel=0.0806 Stop=0.0000\n",
            "[Epoch 24] Batch 200/405 Loss=0.1629 Mel=0.0882 Rmel=0.0747 Stop=0.0000\n",
            "[Epoch 24] Batch 210/405 Loss=0.1853 Mel=0.1004 Rmel=0.0849 Stop=0.0000\n",
            "[Epoch 24] Batch 220/405 Loss=0.1866 Mel=0.1012 Rmel=0.0854 Stop=0.0000\n",
            "[Epoch 24] Batch 230/405 Loss=0.2060 Mel=0.1117 Rmel=0.0943 Stop=0.0000\n",
            "[Epoch 24] Batch 240/405 Loss=0.1770 Mel=0.0960 Rmel=0.0810 Stop=0.0000\n",
            "[Epoch 24] Batch 250/405 Loss=0.1784 Mel=0.0966 Rmel=0.0818 Stop=0.0000\n",
            "[Epoch 24] Batch 260/405 Loss=0.1898 Mel=0.1027 Rmel=0.0870 Stop=0.0000\n",
            "[Epoch 24] Batch 270/405 Loss=0.1749 Mel=0.0947 Rmel=0.0802 Stop=0.0000\n",
            "[Epoch 24] Batch 280/405 Loss=0.1820 Mel=0.0988 Rmel=0.0832 Stop=0.0000\n",
            "[Epoch 24] Batch 290/405 Loss=0.1954 Mel=0.1059 Rmel=0.0895 Stop=0.0000\n",
            "[Epoch 24] Batch 300/405 Loss=0.1924 Mel=0.1044 Rmel=0.0880 Stop=0.0000\n",
            "[Epoch 24] Batch 310/405 Loss=0.1775 Mel=0.0962 Rmel=0.0813 Stop=0.0000\n",
            "[Epoch 24] Batch 320/405 Loss=0.1769 Mel=0.0959 Rmel=0.0811 Stop=0.0000\n",
            "[Epoch 24] Batch 330/405 Loss=0.1569 Mel=0.0850 Rmel=0.0719 Stop=0.0000\n",
            "[Epoch 24] Batch 340/405 Loss=0.1759 Mel=0.0953 Rmel=0.0807 Stop=0.0000\n",
            "[Epoch 24] Batch 350/405 Loss=0.1792 Mel=0.0974 Rmel=0.0819 Stop=0.0000\n",
            "[Epoch 24] Batch 360/405 Loss=0.1752 Mel=0.0948 Rmel=0.0804 Stop=0.0000\n",
            "[Epoch 24] Batch 370/405 Loss=0.1854 Mel=0.1006 Rmel=0.0848 Stop=0.0000\n",
            "[Epoch 24] Batch 380/405 Loss=0.1852 Mel=0.1004 Rmel=0.0848 Stop=0.0000\n",
            "[Epoch 24] Batch 390/405 Loss=0.1568 Mel=0.0849 Rmel=0.0719 Stop=0.0000\n",
            "[Epoch 24] Batch 400/405 Loss=0.1821 Mel=0.0987 Rmel=0.0834 Stop=0.0000\n",
            "Validation Loss: 0.1504\n",
            "Removed shared tensor {'encoder.lstm.bias_hh_l0_reverse', 'encoder.lstm.weight_hh_l0', 'encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.weight_hh_l0_reverse', 'encoder.lstm.weight_ih_l0_reverse', 'encoder.lstm.bias_hh_l0', 'encoder.lstm.bias_ih_l0'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
            "Saved checkpoint: new_runs/checkpoint_24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KsOsvn0j2fOu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
